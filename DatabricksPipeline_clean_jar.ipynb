{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0e234960-42e9-49db-91d8-4a9464a35466",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-schema-registry-client in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
      "Requirement already satisfied: httpx>=0.19.0 in /usr/local/lib/python3.9/dist-packages (from python-schema-registry-client) (0.22.0)\n",
      "Requirement already satisfied: fastavro>=1.4.4 in /usr/local/lib/python3.9/dist-packages (from python-schema-registry-client) (1.4.9)\n",
      "Requirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from python-schema-registry-client) (4.4.0)\n",
      "Requirement already satisfied: aiofiles>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from python-schema-registry-client) (0.8.0)\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.9/dist-packages (from httpx>=0.19.0->python-schema-registry-client) (2.0.12)\n",
      "Requirement already satisfied: httpcore<0.15.0,>=0.14.5 in /usr/local/lib/python3.9/dist-packages (from httpx>=0.19.0->python-schema-registry-client) (0.14.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx>=0.19.0->python-schema-registry-client) (1.2.0)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.9/dist-packages (from httpx>=0.19.0->python-schema-registry-client) (1.5.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx>=0.19.0->python-schema-registry-client) (2021.10.8)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.2.0->python-schema-registry-client) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.2.0->python-schema-registry-client) (21.4.0)\n",
      "Requirement already satisfied: h11<0.13,>=0.11 in /usr/local/lib/python3.9/dist-packages (from httpcore<0.15.0,>=0.14.5->httpx>=0.19.0->python-schema-registry-client) (0.12.0)\n",
      "Requirement already satisfied: anyio==3.* in /usr/local/lib/python3.9/dist-packages (from httpcore<0.15.0,>=0.14.5->httpx>=0.19.0->python-schema-registry-client) (3.5.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.9/dist-packages (from anyio==3.*->httpcore<0.15.0,>=0.14.5->httpx>=0.19.0->python-schema-registry-client) (3.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install python-schema-registry-client\n",
    "\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import to_date\n",
    "import base64\n",
    "import gzip\n",
    "import _datetime\n",
    "from schema_registry.client import SchemaRegistryClient\n",
    "import json\n",
    "import sys\n",
    "import datetime\n",
    "from pyspark.sql.functions import max as sparkMax\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# import org.apache.spark.sql.streaming.ProcessingTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-streaming_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-0c3ffae7-2c3c-4833-a234-a66d606b0dc4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.974 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.9-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-core_2.12;0.8.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-10_2.12;3.0.0 in central\n",
      ":: resolution report :: resolve 743ms :: artifacts dl 29ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.974 from central in [default]\n",
      "\tcom.github.luben#zstd-jni;1.4.9-1 from central in [default]\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;0.8.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 by [org.apache.kafka#kafka-clients;2.8.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   0   |   0   |   1   ||   20  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-0c3ffae7-2c3c-4833-a234-a66d606b0dc4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 20 already retrieved (0kB/19ms)\n",
      "22/03/07 12:19:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/07 12:19:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.\\\n",
    "#         builder.\\\n",
    "#         appName(\"pyspark-notebook\").\\\n",
    "#         master(\"spark://spark-master:7077\").\\\n",
    "#         config(\"spark.executor.memory\", \"512m\").\\\n",
    "#         getOrCreate()\n",
    "\n",
    "\n",
    "# org.apache.spark:spark-avro:3.0.0\n",
    "\n",
    "# SparkConf conf = new SparkConf()\n",
    "#             .setAppName(\"Test spark\")\n",
    "#             .setMaster(\"spark://ip of your master node:port of your master node\")\n",
    "#             .set(\"spark.blockManager.port\", \"10025\")\n",
    "#             .set(\"spark.driver.blockManager.port\", \"10026\")\n",
    "#             .set(\"spark.driver.port\", \"10027\") //make all communication ports static (not necessary if you disabled firewalls, or if your nodes located in local network, otherwise you must open this ports in firewall settings)\n",
    "#             .set(\"spark.cores.max\", \"12\") \n",
    "#             .set(\"spark.executor.memory\", \"2g\")\n",
    "#             .set(\"spark.driver.host\", \"ip of your driver (PC)\");\n",
    "        \n",
    "    \n",
    "import os\n",
    "\n",
    "# setup arguments\n",
    "os.environ['kafka1'] = '34.220.52.64'\n",
    "os.environ['kafka2'] = '52.35.125.165'\n",
    "os.environ['schema-registry'] = '52.35.125.165'\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = \"/opt/workspace/env3/bin/python3.9\"\n",
    "\n",
    "spark = SparkSession.builder.appName('pyspark-notebook')\\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .config('spark.jars.packages', 'com.amazonaws:aws-java-sdk-bundle:1.11.974,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql_2.12:3.0.0,org.apache.kafka:kafka-clients:2.8.0,org.apache.spark:spark-avro_2.12:3.0.0,io.delta:delta-core_2.12:0.8.0,org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-streaming_2.12:3.0.0')\\\n",
    "    .config(\"spark.executor.memory\", \"512m\")\\\n",
    "    .config(\"spark.jars\", \"scalaSpark-assembly-1.0.jar\") \\\n",
    "    .config(\"spark.cores.max\", \"12\")\\\n",
    "    .config(\"spark.driver.port\", \"4040\")\\\n",
    "    .config(\"spark.archives\",\"venv1.zip\")\\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.addPyFile(\"scalaSpark-assembly-1.0.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/07 12:21:08 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "22/03/07 12:21:08 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:716)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:152)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:258)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:168)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:203)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/03/07 12:21:08 ERROR TaskSchedulerImpl: Lost executor 0 on 172.22.0.5: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/07 12:21:08 ERROR TaskSchedulerImpl: Lost executor 1 on 172.22.0.4: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import *\n",
    "import pyspark.sql.functions as fn\n",
    "data = spark.range(1,5)\n",
    "data.write.format(\"delta\").mode(\"overwrite\").save(\"delta_sample2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datalake-qa-91"
     ]
    }
   ],
   "source": [
    "# df4 = spark.sparkContext._jvm.com.example.Hello.add\n",
    "\n",
    "# print(spark._jsparkSession)\n",
    "df5 = spark.sparkContext._jvm.com.example.Hello.add(spark._jsparkSession, \"34.220.52.64:9092,52.35.125.165:9092\", \"datalake-qa-91\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "394c8281-4dd2-46ed-b46a-06cfad847105",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This method get is used to extract columns present in schema and generate sql statements for create , update and insert table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "290ab3f3-5a52-40c5-93be-6cda135695a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'updatesTable.id = targetTable.id'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_partition_query(partition_column: str, primary_key: str):\n",
    "\n",
    "        if not primary_key or not partition_column:\n",
    "            return \"updatesTable.id = targetTable.id\"\n",
    "        \n",
    "#         query = \"updatesTable.{0} = targetTable.{0}\".format(primary_key)\n",
    "        \n",
    "        query = \"\"\n",
    "        \n",
    "        primary_keyA = []\n",
    "    \n",
    "        pkey_array = primary_key.split(\",\")\n",
    "        for partitions in pkey_array:\n",
    "            partitions = partitions.strip()\n",
    "            if query == \"\":\n",
    "              query += \"updatesTable.{0} = targetTable.{0}\".format(partitions)\n",
    "            else:\n",
    "              query += \" and updatesTable.{0} = targetTable.{0}\".format(partitions)\n",
    "              \n",
    "            primary_keyA.append(partitions)\n",
    "        \n",
    "#         primary_key = primary_key.strip()\n",
    "        partitions_array = partition_column.split(\",\")\n",
    "        for partitions in partitions_array:\n",
    "            partitions = partitions.strip()\n",
    "            if partitions not in primary_keyA:\n",
    "                query += \" and updatesTable.{0} = targetTable.{0}\".format(partitions)\n",
    "        return query\n",
    "\n",
    "    \n",
    "get_partition_query(\"\", \"company_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "feb33619-e63e-43af-b365-8ec5e98de83d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, StringType, FloatType\n",
    "from delta.tables import *\n",
    "from pyspark.sql import DataFrame\n",
    "  \n",
    "def getTableColumn(my_schema):\n",
    "    tableList = []\n",
    "    col = []\n",
    "    inputTable = \"updatesTable.\"\n",
    "    targetTable = \"targetTable.\"\n",
    "\n",
    "    ignoreColumns = [\"table_key_ts\", \"table_delete\", \"table_key\"]\n",
    "\n",
    "    for field in my_schema.fields:\n",
    "        if field.name == \"after\":\n",
    "            for f3 in field.dataType:\n",
    "                te = f3.name + \" \" + f3.dataType.typeName()\n",
    "                if f3.name not in ignoreColumns:\n",
    "                    tableList.append(te)\n",
    "                    col.append(f3.name)\n",
    "        else:\n",
    "            te = field.name + \" \" + field.dataType.typeName()\n",
    "            if field.name not in ignoreColumns:\n",
    "                tableList.append(te)\n",
    "                col.append(field.name)\n",
    "    res = \"\"\n",
    "    insertResInp = \"\"\n",
    "    insertResOut = \"\"\n",
    "    merge = \"\"\n",
    "    for i in range(len(tableList)):\n",
    "        if i == (len(tableList) - 1):\n",
    "            res += tableList[i]\n",
    "            insertResInp += col[i]\n",
    "            insertResOut += inputTable + col[i]\n",
    "            merge += targetTable + col[i] + \" = \" + inputTable + col[i]\n",
    "        else:\n",
    "            res += tableList[i] + \",\"\n",
    "            insertResInp += col[i] + \",\"\n",
    "            insertResOut += inputTable + col[i] + \",\"\n",
    "            merge += targetTable + col[i] + \" = \" + inputTable + col[i] + \",\"\n",
    "    ans = [res, insertResInp, insertResOut, merge]\n",
    "    \n",
    "    \n",
    "    print(ans)\n",
    "    return ans\n",
    "\n",
    "\n",
    "class KafkaJob:\n",
    "\n",
    "    def __init__(self, record, global_variables):\n",
    "        self.test_data = global_variables.test_data\n",
    "        self.test_schema = global_variables.test_schema\n",
    "        self.sc = global_variables.sc\n",
    "        self.spark = global_variables.spark\n",
    "        self.schemaRegistryAddress = global_variables.schemaRegistryUrl\n",
    "        self.database = record[\"database\"]\n",
    "#         self.batchOutputPath = global_variables.batchOutputPath\n",
    "        \n",
    "        #change 1\n",
    "        self.batchOutputPath = record[\"batchOutputPath\"]\n",
    "        \n",
    "#         self.kafkaGrp = global_variables.kafka_grp\n",
    "        \n",
    "#         #change 2\n",
    "#         self.kafkaGrp = record[\"kafka_grp\"]\n",
    "        \n",
    "        \n",
    "#         self.OutputPathForMaterializedView = global_variables.OutputPathForMaterializedView\n",
    "        \n",
    "        # change 3\n",
    "        self.OutputPathForMaterializedView = record[\"OutputPathForMaterializedView\"]\n",
    "        \n",
    "        self.schemasByIdDict = {}\n",
    "        self.bootstrapServers = global_variables.bootstrapServers\n",
    "        \n",
    "        # change 4\n",
    "#         self.kafkaConsumerGroupForLag = record[\"kafkaConsumerGroupForLag\"]\n",
    "        \n",
    "        self.kafkaConsumerGroupForLag = global_variables.kafkaConsumerGroupForLag\n",
    "        \n",
    "        \n",
    "        self.startDateOfPipeline = global_variables.startDateOfPipeline\n",
    "        self.timestampToStartStreaming = global_variables.timestampToStartStreaming\n",
    "        self.table = record[\"table_name\"]\n",
    "        \n",
    "        self.secondary_key = record[\"secondary_key\"]\n",
    "        self.topic_prefix = record[\"topic_prefix\"]\n",
    "        self.partition_column = record[\"partition_column\"]\n",
    "        self.zorder_column = record[\"zorder_column\"]\n",
    "        self.partition_column_derive_from_column = record[\"partition_column_derive_from_column\"]\n",
    "        self.primary_key = record[\"primary_key\"]\n",
    "        self.reference_index = record[\"reference_index\"]\n",
    "        self.topic_pattern_to_subscribe = record[\"topic_pattern\"]\n",
    "        self.is_pattern = record[\"is_pattern\"]\n",
    "        self.batchOutputPathForTopic = \"{}{}/{}\".format(self.batchOutputPath, self.database, self.table)\n",
    "        self.checkpoint = self.batchOutputPathForTopic + \"_AvroCheckpoint\"\n",
    "        self.OutputPathForMaterializedViewPath = \"{}{}/{}\".format(self.OutputPathForMaterializedView, self.database, self.table)\n",
    "        self.table_exists = False\n",
    "        self.subriction_option = \"subscribePattern\"\n",
    "        self.current_primary_key = self.sc.broadcast(self.primary_key)\n",
    "        self.current_secondary_key = self.sc.broadcast(self.secondary_key)\n",
    "        \n",
    "\n",
    "        self.spark.sql(\"create database  IF NOT EXISTS  {} LOCATION  '{}'\".format(self.database, self.OutputPathForMaterializedView))\n",
    "        self.spark.sql(\"use {}\".format(self.database))\n",
    "        print(self.table)\n",
    "        self.new_partition_key = None\n",
    "        self.new_partition_value = None\n",
    "        self.byteArrayToLong = fn.udf(lambda x: int.from_bytes(x, byteorder='big', signed=False), LongType())\n",
    "        self.schemasByIdDict = {}\n",
    "#         val = sc._jvm.example13.Hello.add(self.bootstrapServers, self.kafkaConsumerGroupForLag)\n",
    "\n",
    "\n",
    "        scalaDfLag = self.sc._jvm.com.example.Hello.add(self.spark._jsparkSession, dbutils.widgets.get(\"bootstrapServers\"), dbutils.widgets.get(\"kafkaConsumerGroupForLag\"))\n",
    "\n",
    "        print(scalaDfLag)\n",
    "#         print(\"query\")\n",
    "#         print(val)\n",
    "        \n",
    "        if self.partition_column_derive_from_column:\n",
    "            partition_column_derive_from_column_split = self.partition_column_derive_from_column.split(\",\")\n",
    "            self.new_partition_key = self.sc.broadcast(partition_column_derive_from_column_split[0])\n",
    "            self.new_partition_value = self.sc.broadcast(partition_column_derive_from_column_split[1])\n",
    "\n",
    "        if not self.is_pattern:\n",
    "            self.subriction_option = \"subscribe\"\n",
    "            \n",
    "            \n",
    "        self.spark.sql(\"SET spark.databricks.delta.schema.autoMerge.enabled = true\")\n",
    "        \n",
    "        \n",
    "        # adding option of lowshuffle merge but will work with DBR9.0+ so uncomment below as needed\n",
    "#         self.spark.sql(\"SET spark.databricks.delta.merge.enableLowShuffle = true\")\n",
    "        \n",
    "        self.decode_process_data = fn.udf(KafkaJob.decode_process_data_udf, StringType())\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_process_data_udf(coded_string):\n",
    "        result = \"\"\n",
    "        try:\n",
    "            temp = base64.b64decode(coded_string)\n",
    "            result = gzip.decompress(temp).decode('utf-8')\n",
    "        except:\n",
    "            result = coded_string\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def get_multiple_partition_query(partition_column: str, primary_key: str):\n",
    "\n",
    "        if not primary_key or not partition_column:\n",
    "            return \"updatesTable.id = targetTable.id\"\n",
    "        \n",
    "#         query = \"updatesTable.{0} = targetTable.{0}\".format(primary_key)\n",
    "        \n",
    "        query = \"\"\n",
    "        \n",
    "        primary_keyA = []\n",
    "    \n",
    "        pkey_array = primary_key.split(\",\")\n",
    "        for partitions in pkey_array:\n",
    "            partitions = partitions.strip()\n",
    "            if query == \"\":\n",
    "              query += \"updatesTable.{0} = targetTable.{0}\".format(partitions)\n",
    "            else:\n",
    "              query += \" and updatesTable.{0} = targetTable.{0}\".format(partitions)\n",
    "              \n",
    "            primary_keyA.append(partitions)\n",
    "        \n",
    "#         primary_key = primary_key.strip()\n",
    "        partitions_array = partition_column.split(\",\")\n",
    "        for partitions in partitions_array:\n",
    "            partitions = partitions.strip()\n",
    "            if partitions not in primary_keyA:\n",
    "                query += \" and updatesTable.{0} = targetTable.{0}\".format(partitions)\n",
    "        return query\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_secondary_partition_query(partition_column: str, primary_key: str, secondary_key: str):\n",
    "        \n",
    "        query = \"updatesTable.{0} = targetTable.{0}\".format(primary_key)\n",
    "        \n",
    "        query += \" and updatesTable.{0} = targetTable.{0}\".format(secondary_key)\n",
    "        \n",
    "        primary_key = primary_key.strip()\n",
    "        partitions_array = partition_column.split(\",\")\n",
    "        for partitions in partitions_array:\n",
    "            partitions = partitions.strip()\n",
    "            if partitions != primary_key and partitions != secondary_key:\n",
    "                query += \" and updatesTable.{0} = targetTable.{0}\".format(partitions)\n",
    "        return query\n",
    "\n",
    "      \n",
    "      \n",
    "    @staticmethod\n",
    "    def get_partition_query(partition_column: str, primary_key: str):\n",
    "\n",
    "        if not primary_key or not partition_column:\n",
    "            return \"updatesTable.id = targetTable.id\"\n",
    "        query = \"updatesTable.{0} = targetTable.{0}\".format(primary_key)\n",
    "        primary_key = primary_key.strip()\n",
    "        partitions_array = partition_column.split(\",\")\n",
    "        for partitions in partitions_array:\n",
    "            partitions = partitions.strip()\n",
    "            if partitions != primary_key:\n",
    "                query += \" and updatesTable.{0} = targetTable.{0}\".format(partitions)\n",
    "        return query\n",
    "\n",
    "    def getJsonSchema(self, schemaId, schemasById, registrySchemaUrl):\n",
    "        jsonSchema = schemasById.get(schemaId)\n",
    "        if jsonSchema is None:\n",
    "            src = SchemaRegistryClient(registrySchemaUrl)\n",
    "            schema = src.get_by_id(schemaId)\n",
    "            jsonSchema = json.dumps(schema.flat_schema)\n",
    "            schemasById[schemaId] = jsonSchema\n",
    "        return jsonSchema\n",
    "\n",
    "    @staticmethod\n",
    "    def generator(partition):\n",
    "      \"\"\"\n",
    "      Function yielding some result created by some function applied to each row of a partition (in this case lower-casing a string)\n",
    "\n",
    "      @partition: iterator-object of partition\n",
    "      \"\"\"\n",
    "\n",
    "      partCount = 1\n",
    "      for row in partition:\n",
    "          yield [row[\"offset\"].lower(), row, partCount]\n",
    "          partCount+=1\n",
    "        \n",
    "    @staticmethod\n",
    "    def groupk(partition):\n",
    "      from itertools import groupby\n",
    "      \n",
    "      partCount = 1\n",
    "      l = []\n",
    "      for key, group in groupby(sorted(partition, key = lambda x: x[\"table_key\"]), lambda x: x[\"table_key\"]):\n",
    "        a = []\n",
    "        k = None\n",
    "        temp = -1\n",
    "\n",
    "        for thing in group:\n",
    "          if temp < int(thing[\"offset\"]):\n",
    "            temp = thing[\"offset\"]\n",
    "            k = thing\n",
    "\n",
    "#         yield k\n",
    "        l.append(k)\n",
    "        partCount += 1\n",
    "    \n",
    "      return l\n",
    "    \n",
    "    def upsertToDelta(self,microBatchOutputDF, batchId):\n",
    "        microBatchOutputDF = microBatchOutputDF.drop(\"valuenew\")\n",
    "        \n",
    "        microBatchOutputDF = microBatchOutputDF.withColumn('valueConsumerKafka', microBatchOutputDF['value'])\n",
    "        microBatchOutputDF = microBatchOutputDF.drop(\"value\")\n",
    "        \n",
    "        \n",
    "        microBatchOutputDF = microBatchOutputDF.withColumn(\"table_delete\", F.when(\n",
    "            col(\"valueConsumerKafka.before\").isNotNull and col(\"valueConsumerKafka.op\").isNotNull and col(\"valueConsumerKafka.op\").contains(\"d\"),\n",
    "            True).otherwise(False))\n",
    "        \n",
    "        \n",
    "        if self.secondary_key:\n",
    "          microBatchOutputDF = microBatchOutputDF.withColumn(\"table_key\", F.when(col(\"table_delete\")==True, \n",
    "                                                                                 F.concat(microBatchOutputDF[\n",
    "                                                                                     \"valueConsumerKafka.before.{}\".format(self.current_primary_key.value)],\n",
    "                                                                                          F.lit('_'), \n",
    "                                                                                          microBatchOutputDF[\"valueConsumerKafka.before.{}\".format(\n",
    "                                                                                         self.current_secondary_key.value)])).otherwise(F.concat(microBatchOutputDF[\n",
    "                                                                                     \"valueConsumerKafka.after.{}\".format(self.current_primary_key.value)],F.lit('_'), \n",
    "                                                                                         microBatchOutputDF[\"valueConsumerKafka.after.{}\".format(\n",
    "                                                                                         self.current_secondary_key.value)])))\n",
    "        else:\n",
    "          microBatchOutputDF = microBatchOutputDF.withColumn(\"table_key\", F.when(col(\"table_delete\") == True,\n",
    "                                                                                 microBatchOutputDF[\n",
    "                                                                                     \"valueConsumerKafka.before.{}\".format(\n",
    "                                                                                         self.current_primary_key.value)]).otherwise(\n",
    "              microBatchOutputDF[\"valueConsumerKafka.after.{}\".format(self.current_primary_key.value)]))\n",
    "\n",
    "        \n",
    "          \n",
    "        \n",
    "        print(\"starting point\")\n",
    "        #logger.debug(\"starting point\")\n",
    "\n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "        #logger.debug(str(datetime.datetime.now()))\n",
    "        \n",
    "#         microBatchOutputDF.show()\n",
    "        # display(microBatchOutputDF)\n",
    "        print(\"total partitions are\")\n",
    "        print(microBatchOutputDF.rdd.getNumPartitions())\n",
    "      \n",
    "      \n",
    "#         dfByTopic = microBatchOutputDF.withColumn(\"rn\", F.row_number().over(\n",
    "#             Window.partitionBy(\"table_key\").orderBy(F.col(\"offset\").desc())))\n",
    "#         dfByTopic = dfByTopic.filter(F.col(\"rn\") == 1).drop(\"rn\")\n",
    "      \n",
    "      \n",
    "#         dfByTopic2 = microBatchOutputDF.withColumn(\"max_offset\", F.max(\"offset\").over(\n",
    "#             Window.partitionBy(\"table_key\")))\n",
    "        \n",
    "#         dfByTopic = dfByTopic2.where(F.col(\"offset\") == F.col(\"max_offset\")).drop(\"max_offset\")\n",
    "    \n",
    "        \n",
    "#         # option 1 [still it requires group by but broadcasts it to multiple so maybe less shuffles]\n",
    "#         dfMax = microBatchOutputDF.groupBy(col(\"table_key\").alias(\"max_table_key_con\")).agg(sparkMax(col(\"offset\")).alias(\"max_offset_value_con\"))\n",
    "\n",
    "#         dfTopByJoin = microBatchOutputDF.join(broadcast(dfMax),\n",
    "#         (col(\"table_key\") == col(\"max_table_key_con\")) & (col(\"offset\") == col(\"max_offset_value_con\"))).drop(\"max_table_key_con\").drop(\"max_offset_value_con\")\n",
    "        \n",
    "# #         for same offset can it have same key value??\n",
    "#         dfByTopic = dfTopByJoin.dropDuplicates([\"table_key\",\"offset\"])\n",
    "  \n",
    "# #         dfByTopic = spark.createDataFrame(dfByTopic2.rdd, schema = microBatchOutputDF.schema)\n",
    "\n",
    "# #         # option 1\n",
    "        \n",
    "        \n",
    "#         #option 2 [reducebyKey is also wide operation but it have less shuffles as can combine first and then shuffles will be less]\n",
    "\n",
    "#         dfrdd = microBatchOutputDF.rdd.map(lambda x: (x.table_key, x)).reduceByKey(lambda x, y: x if (x.offset > y.offset) else y)\n",
    "\n",
    "#         a = microBatchOutputDF.schema\n",
    "  \n",
    "#         schema_str = StructType([\n",
    "#           StructField('table_key', IntegerType(), True),\n",
    "#           StructField('others', a)])\n",
    "    \n",
    "#         dfByT = spark.createDataFrame(dfrdd, schema = schema_str)\n",
    "      \n",
    "#         dfByTopic = dfByT.select(\"others.*\")\n",
    "  \n",
    "#         # option 2\n",
    "\n",
    "\n",
    "        dfrdd = microBatchOutputDF.rdd.mapPartitions(KafkaJob.groupk)\n",
    "        dfByTopic = spark.createDataFrame(dfrdd, schema = microBatchOutputDF.schema)\n",
    "    \n",
    "#         dfByTopic = microBatchOutputDF\n",
    "\n",
    "#         dfByTopic = microBatchOutputDF\n",
    "        print(dfByTopic.rdd.getNumPartitions())\n",
    "        #logger.debug(\"ending point\")\n",
    "        \n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "        #logger.debug(str(datetime.datetime.now()))\n",
    "        dfByTopic.show()\n",
    "        display(dfByTopic)\n",
    "\n",
    "        dfByTopicDeleted = dfByTopic.filter(F.col(\"table_delete\") == True)\n",
    "        dfByTopicDeleted = dfByTopicDeleted.select(\"valueConsumerKafka.before.*\", \"table_delete\", \"table_key\", \"offset\")\n",
    "\n",
    "        dfByTopicDeleted = dfByTopicDeleted.drop(\"valueConsumerKafka\", \"kafka-key\", \"schemaId\")\n",
    "        dfByTopicDeleted = dfByTopicDeleted.drop(\"offset\")\n",
    "\n",
    "        dfByTopicInsert = dfByTopic.filter(F.col(\"table_delete\") == False)\n",
    "        dfByTopicInsert = dfByTopicInsert.select(\"valueConsumerKafka.after.*\", \"table_delete\", \"table_key\", \"offset\")\n",
    "        # display(df)\n",
    "        dfByTopicInsert = dfByTopicInsert.drop(\"valueConsumerKafka\", \"kafka-key\", \"schemaId\")\n",
    "        dfByTopicInsert = dfByTopicInsert.drop(\"offset\")\n",
    "        if self.partition_column_derive_from_column:\n",
    "            dfByTopicDeleted = dfByTopicDeleted.withColumn(self.new_partition_key.value,\n",
    "                                                           to_date(col(self.new_partition_value.value)))\n",
    "            dfByTopicInsert = dfByTopicInsert.withColumn(self.new_partition_key.value,\n",
    "                                                         to_date(col(self.new_partition_value.value)))\n",
    "        if self.table == 'process':\n",
    "            dfByTopicInsert = dfByTopicInsert.withColumn(\"process_data\", self.decode_process_data(\"process_data\"))\n",
    "\n",
    "        if not self.table_exists:\n",
    "            my_schema = dfByTopicInsert.schema\n",
    "            res = getTableColumn(my_schema)\n",
    "            dfByTopic._jdf.sparkSession().sql(\"use {}\".format(self.database))\n",
    "            if self.partition_column:\n",
    "                dfByTopic._jdf.sparkSession().sql(\n",
    "                    \"create table if not exists {} ( {} ) using delta PARTITIONED BY ({}) location '{}' \".format(self.table,\n",
    "                                                                                                                 res[0],\n",
    "                                                                                                                 self.partition_column,\n",
    "                                                                                                                 self.OutputPathForMaterializedViewPath))\n",
    "            else:\n",
    "                dfByTopic._jdf.sparkSession().sql(\n",
    "                    \"create table if not exists {} ( {} ) using delta location '{}' \".format(self.table, res[0],\n",
    "                                                                                             self.OutputPathForMaterializedViewPath))\n",
    "\n",
    "            self.table_exists = True\n",
    "\n",
    "        dfByTopicInsert = dfByTopicInsert.drop(\"table_delete\")\n",
    "        dfByTopicInsert = dfByTopicInsert.drop(\"table_key\")\n",
    "\n",
    "#         PYSPARK MERGE\n",
    "#         deltaTable=DeltaTable.forName(spark,self.table)\n",
    "        FullTableName = self.database + \".\" + self.table\n",
    "\n",
    "        print(FullTableName)\n",
    "\n",
    "#         deltaTable = DeltaTable.forName(spark, FullTableName)\n",
    "        \n",
    "        deltaTable=DeltaTable.forName(spark,FullTableName)\n",
    "    \n",
    "#         jab\n",
    "        \n",
    "        if self.secondary_key:\n",
    "            if not self.partition_column:\n",
    "              query = KafkaJob.get_secondary_partition_query(\"\", self.primary_key, self.secondary_key)\n",
    "            else:\n",
    "              query = KafkaJob.get_secondary_partition_query(self.partition_column, self.primary_key, self.secondary_key)\n",
    "\n",
    "            deltaTable.alias(\"targetTable\").merge(\n",
    "                dfByTopicDeleted.alias(\"updatesTable\"),\n",
    "                \"{0}\".format(query)) \\\n",
    "              .whenMatchedDelete() \\\n",
    "              .execute()\n",
    "\n",
    "\n",
    "            deltaTable.alias(\"targetTable\").merge(\n",
    "                dfByTopicInsert.alias(\"updatesTable\"),\n",
    "                \"{0}\".format(query)) \\\n",
    "              .whenMatchedUpdateAll() \\\n",
    "              .whenNotMatchedInsertAll().execute()\n",
    "\n",
    "        elif self.partition_column and self.partition_column != self.primary_key:\n",
    "            query = KafkaJob.get_partition_query(self.partition_column, self.primary_key)\n",
    "\n",
    "            deltaTable.alias(\"targetTable\").merge(\n",
    "                dfByTopicDeleted.alias(\"updatesTable\"),\n",
    "                \"{0}\".format(query)) \\\n",
    "              .whenMatchedDelete() \\\n",
    "              .execute()\n",
    "\n",
    "\n",
    "            deltaTable.alias(\"targetTable\").merge(\n",
    "                dfByTopicInsert.alias(\"updatesTable\"),\n",
    "                \"{0}\".format(query)) \\\n",
    "              .whenMatchedUpdateAll() \\\n",
    "              .whenNotMatchedInsertAll().execute()\n",
    "\n",
    "        else:\n",
    "            \n",
    "            deltaTable.alias(\"targetTable\").merge(\n",
    "                dfByTopicDeleted.alias(\"updatesD\"),\n",
    "                \"updatesD.table_key = targetTable.{0}\".format(self.primary_key)) \\\n",
    "              .whenMatchedDelete() \\\n",
    "              .execute()\n",
    "\n",
    "            deltaTable.alias(\"targetTable\").merge(\n",
    "                dfByTopicInsert.alias(\"updatesI\"),\n",
    "                \"updatesI.{0} = targetTable.{0}\".format(self.primary_key)) \\\n",
    "              .whenMatchedUpdateAll() \\\n",
    "              .whenNotMatchedInsertAll().execute()\n",
    "\n",
    "#         PYSPARK MERGE\n",
    "\n",
    "        print(\"after merge\")\n",
    "\n",
    "        #logger.debug(\"after merge\")\n",
    "        \n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "        #logger.debug(str(datetime.datetime.now()))\n",
    "\n",
    "    def get_avro_data_by_schema_for_each(self,df, batchId):\n",
    "      \n",
    "        self.reference_index = batchId % 50\n",
    "        print(\"optimize check\")\n",
    "        \n",
    "        ldf = spark.sql(\"select (int(current_timestamp()) - int(last_updated))/60 from pipeline_common.vacuumStatus where table = '\" + self.database + \".\" + self.table + \"'\")\n",
    "        \n",
    "        if ldf.first()[0] > 5:\n",
    "          if batchId % 50 == int(self.reference_index) and batchId >= 50:\n",
    "              try:\n",
    "                  print(\"optimizing {}.{}\".format(self.database,self.table))\n",
    "                  if self.is_pattern:\n",
    "                    print(\"using partition last 3 month\")\n",
    "\n",
    "                    timeCol = self.partition_column_derive_from_column.split(\",\")[0] \n",
    "\n",
    "                    if self.zorder_column and batchId % 150 == int(self.reference_index):\n",
    "                        self.spark.sql(\"optimize {0}.{1} WHERE {3} >= current_timestamp() - INTERVAL 100 day ZORDER BY ({2})\".format(self.database, self.table, self.zorder_column, timeCol))\n",
    "                    else:\n",
    "                        self.spark.sql(\"optimize {0}.{1} WHERE {2} >= current_timestamp() - INTERVAL 100 day \".format(self.database, self.table, timeCol))\n",
    "                  else:\n",
    "                    if self.zorder_column and batchId % 150 == int(self.reference_index):\n",
    "                        self.spark.sql(\"optimize {0}.{1} ZORDER BY ({2})\".format(self.database, self.table, self.zorder_column))\n",
    "                    else:\n",
    "                        self.spark.sql(\"optimize {0}.{1}\".format(self.database, self.table))\n",
    "\n",
    "                  print(\"vacuum {}.{}\".format(self.database,self.table))\n",
    "                  self.spark.sql(\"vacuum {0}.{1}\".format(self.database, self.table))\n",
    "                  spark.sql(\"update pipeline_common.vacuumStatus set last_updated = current_timestamp() where table = '\" + self.database + \".\" + self.table + \"'\")\n",
    "        \n",
    "              except Exception as e:\n",
    "                  print(e)\n",
    "        month_year = \"\"\n",
    "        avro_topic = self.topic_pattern_to_subscribe + \"-value\"\n",
    "        if self.is_pattern:\n",
    "            month_year = \"_\" + str(_datetime.date.today().month) + \"_\" + str(_datetime.date.today().year)\n",
    "            avro_topic = self.topic_prefix + self.table + month_year + \"-value\"\n",
    "        print(\"avro_topic: \" + avro_topic)\n",
    "        avro_topic_br = self.sc.broadcast(avro_topic)\n",
    "        try:\n",
    "            print(\"running fast: {}\", avro_topic)\n",
    "            df = df.withColumn('valuenew', df['value'])\n",
    "            df = df.withColumn('value', from_avro(\"value\", avro_topic_br.value, self.schemaRegistryAddress))\n",
    "            \n",
    "            self.upsertToDelta(df, batchId)\n",
    "        except Exception as fastException:\n",
    "#           print(\"checking aa\")  \n",
    "            try:\n",
    "                mem_limit = 2 ** 20\n",
    "                print(\"running slow: {}\", avro_topic)\n",
    "                print(\"fastException try slow\", fastException)\n",
    "                if sys.getsizeof(self.schemasByIdDict) > mem_limit:\n",
    "                    self.schemasByIdDict.clear()\n",
    "                df = df \\\n",
    "                    .withColumn(\"schemaId\", self.byteArrayToLong(fn.substring(\"valuenew\", 2, 4))) \\\n",
    "                    .withColumn(\"payload\", fn.expr(\"substring(valuenew, 6, length(valuenew)-5)\"))\n",
    "                dfAllSchemas = df.select(\"schemaId\").distinct()\n",
    "                schemaRowList = dfAllSchemas.collect()\n",
    "                print(schemaRowList)\n",
    "                for schemaRow in schemaRowList:\n",
    "                    jsonSchema = self.getJsonSchema(schemaRow.schemaId, self.schemasByIdDict,\n",
    "                                                    'http://schema-registry:8081')\n",
    "                    currentValueSchemaId = self.sc.broadcast(schemaRow.schemaId)\n",
    "                    currentValueSchema = self.sc.broadcast(jsonSchema)\n",
    "                    dfBySchemaId = df.where(df.schemaId == currentValueSchemaId.value)\n",
    "                    dfBySchemaId = dfBySchemaId.drop(\"value\")\n",
    "                    dfBySchemaId = dfBySchemaId.withColumn(\"value\", from_avro(\"payload\", currentValueSchema.value))\n",
    "                    dfBySchemaId = dfBySchemaId.drop(\"payload\")\n",
    "                    self.upsertToDelta(dfBySchemaId, batchId)\n",
    "            except Exception as slowException:\n",
    "                print(\"slowException {}\".format(avro_topic), fastException)\n",
    "                raise slowException\n",
    "\n",
    "    def start_read_stream(self):\n",
    "      \n",
    "        print(\"starting stream\")\n",
    "        #logger.debug(\"starting stream\")\n",
    "\n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "        #logger.debug(str(datetime.datetime.now()))\n",
    "        \n",
    "        print(\"topic_pattern_to_subscribe: \" + self.topic_pattern_to_subscribe)\n",
    "        \n",
    "        \n",
    "#         GrpId = \"fareye_\" + self.kafkaGrp + \"_\" + self.topic_pattern_to_subscribe\n",
    "        kafka = self.spark.readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", self.bootstrapServers) \\\n",
    "            .option(self.subriction_option, self.topic_pattern_to_subscribe) \\\n",
    "            .option(\"startingOffsets\", \"earliest\") \\\n",
    "            .option(\"maxOffsetsPerTrigger\", 1000000) \\\n",
    "            .option(\"failOnDataLoss\", False) \\\n",
    "            .load()\n",
    "\n",
    "        if self.timestampToStartStreaming is not None:\n",
    "            kafka = kafka.filter(kafka['timestamp'] >= self.timestampToStartStreaming)\n",
    "\n",
    "        kafka = kafka.filter(\"value is NOT NULL\")\n",
    "        return kafka\n",
    "\n",
    "    def start_write_stream(self,kafka,method_to_run_on_for_each):\n",
    "      \n",
    "        print(\"starting write\")\n",
    "        #logger.debug(\"starting write\")\n",
    "\n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "        #logger.debug(str(datetime.datetime.now()))\n",
    "        \n",
    "        ssc = kafka \\\n",
    "            .writeStream \\\n",
    "            .queryName(\"{0}_{1}\".format(self.database,self.table)) \\\n",
    "            .option(\"checkpointLocation\", self.checkpoint) \\\n",
    "            .trigger(processingTime='60 seconds') \\\n",
    "            .foreachBatch(method_to_run_on_for_each).start()\n",
    "        ssc.awaitTermination()\n",
    "\n",
    "\n",
    "    def start_pipeline_with_avro(self):\n",
    "        kafka = self.start_read_stream()\n",
    "        self.start_write_stream(kafka,self.get_avro_data_by_schema_for_each)\n",
    "\n",
    "    def start_pipeline_without_avro(self):\n",
    "        kafka = self.start_read_stream()\n",
    "        self.start_write_stream(kafka,self.upsertToDelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1a79368e-934f-4f12-bec5-c920053c19bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "        \n",
    "#         # option 1 [still it requires group by but broadcasts it to multiple so maybe less shuffles]\n",
    "#         dfMax = microBatchOutputDF.groupBy(col(\"table_key\").alias(\"max_table_key\")).agg(sparkMax(col(\"offset\")).alias(\"max_offset_value\"))\n",
    "\n",
    "#         dfByTopic = microBatchOutputDF.join(broadcast(dfMax),\n",
    "#         (col(\"table_key\") == col(\"max_table_key\")) & (col(\"offset\") == col(\"max_offset_value\"))).drop(\"max_table_key\").drop(\"max_offset_value\")\n",
    "        \n",
    "#         # for same offset can it have same key value??\n",
    "#         #  dfByTopic = dfTopByJoin.groupBy(\"table_key\").agg(\n",
    "#         #  first(\"category\").alias(\"category\"),\n",
    "#         #  first(\"TotalValue\").alias(\"TotalValue\"))\n",
    "\n",
    "#         # option 1\n",
    "        \n",
    "        \n",
    "#         #option 2 [reducebyKey is also wide operation but it have less shuffles as can combine first and then shuffles will be less]\n",
    "\n",
    "#         print(\"option 2 debug\")\n",
    "#         dfrdd = microBatchOutputDF.rdd.map(lambda x: (x.table_key, x)).reduceByKey(lambda x, y: x if (x.offset > y.offset) else y)\n",
    "\n",
    "#         a = microBatchOutputDF.schema\n",
    "  \n",
    "#         schema_str = StructType([\n",
    "#           StructField('table_key', IntegerType(), True),\n",
    "#           StructField('others', a)])\n",
    "    \n",
    "#         dfByT = spark.createDataFrame(dfrdd, schema = schema_str)\n",
    "      \n",
    "#         dfByTopic = dfByT.select(\"others.*\")\n",
    "  \n",
    "#         # option 2\n",
    "\n",
    "        \n",
    "  \n",
    "        # identified as best among others\n",
    "        # option 3 using foreach partition (which is based on condition that a particular table key which always be in same partition and not in any other which is taken care by debziumn and kafka)\n",
    "\n",
    "#         def f(a):\n",
    "#             print(a)\n",
    "        \n",
    "#         res = microBatchOutputDF.rdd.mapPartitions(KafkaJob.groupk).take(5) #toDF().select(\"_2.*\")\n",
    "        \n",
    "#         [f(e) for e in res]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "050fc845-4379-4352-b1b9-f5b60971928b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Global_variables():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "835797db-5314-45d1-9c16-9fa215ec957a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.avro.functions import from_avro, to_avro\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark import Row\n",
    "import random\n",
    "from pyspark.sql.types import LongType, StringType\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import to_json,from_json,schema_of_json\n",
    "import sys\n",
    "from schema_registry.client import SchemaRegistryClient, schema\n",
    "import json \n",
    "from pyspark.sql.functions import udf\n",
    "import base64\n",
    "import gzip \n",
    "import _datetime\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# from databricks_pipeline.Dbutils import get_dbutils\n",
    "# from databricks_pipeline.Global_variables import Global_variables\n",
    "# from schema_registry.client import SchemaRegistryClient\n",
    "# import json\n",
    "# import datetime\n",
    "# from databricks_pipeline import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "445d74e8-5b64-4561-beaf-d3856e50205f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark = SparkSession.builder.config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:0.8.0\") \\\n",
    "            .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "            .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3ec10631-0f0d-4d25-a23f-ef3f0a1417ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "# Logger= spark._jvm.org.apache.log4j.Logger\n",
    "# mylogger = Logger.getLogger(__name__)\n",
    "\n",
    "# print(mylogger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d2ab4ccf-ad41-4842-9c0f-7a8898338b2b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Adjusting log4j.properties here: \n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Adjusting log4j.properties here: \n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh\n",
    "#!/bin/bash\n",
    "echo \"Adjusting log4j.properties here: ${LOG4J_PATH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "66ec6f86-4f10-4bd8-a8eb-eda57f9cd651",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# mylogger.debug(\"debug message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad9e418b-e905-442b-931c-ba56d53e4b2c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%scala\n",
    "// sc.parallelize(Seq(\"\")).foreachPartition(x => {\n",
    "//   import org.apache.log4j.{LogManager, Level}\n",
    "//   import org.apache.commons.logging.LogFactory\n",
    "\n",
    "//   LogManager.getRootLogger().setLevel(Level.ERROR)\n",
    "//   val log = LogFactory.getLog(\"EXECUTOR-LOG:\")\n",
    "//   log.debug(\"START EXECUTOR ERROR LOG LEVEL\")\n",
    "// })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "511870df-1857-4d70-8f30-b75120d7b9ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">timestamp: 2021-11-05 00:00:00\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">timestamp: 2021-11-05 00:00:00\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "schemaRegistryAddress = None\n",
    "batchOutputPath = None\n",
    "kafkaGrp = None\n",
    "OutputPathForMaterializedView = None\n",
    "schemasByIdDict = {}\n",
    "bootstrapServers = None\n",
    "kafkaConsumerGroupForLag = None\n",
    "startDateOfPipeline = None\n",
    "schemaRegistryAddress = dbutils.widgets.get(\"schemaRegistryUrl\")\n",
    "# batchOutputPath = dbutils.widgets.get(\"batchOutputPath\")\n",
    "# kafkaGrp = dbutils.widgets.get(\"kafka-grp\")\n",
    "job_database = dbutils.widgets.get(\"database\")\n",
    "# OutputPathForMaterializedView = dbutils.widgets.get(\"OutputPathForMaterializedView\")\n",
    "schemasByIdDict = {}\n",
    "bootstrapServers = dbutils.widgets.get(\"bootstrapServers\")\n",
    "# kafkaConsumerGroupForLag = dbutils.widgets.get(\"kafkaConsumerGroupForLag\")\n",
    "startDateOfPipeline = dbutils.widgets.get(\"startDateOfPipeline\")\n",
    "# TO_EMAIL = dbutils.widgets.get(\"TO_EMAIL\")\n",
    "# EMAIL_SERVICE_URL = dbutils.widgets.get(\"EMAIL_SERVICE_URL\")\n",
    "# EMAIL_SERVICE_API_KEY = dbutils.widgets.get(\"EMAIL_SERVICE_API_KEY\")\n",
    "# SUBJECT_EMAIL = dbutils.widgets.get(\"SUBJECT_EMAIL\")\n",
    "job_database = dbutils.widgets.get(\"database\")\n",
    "\n",
    "# print(\"kafkaGrp: \" + kafkaGrp)\n",
    "# print(\"bootstrapServers: \" + bootstrapServers)\n",
    "# print(\"schemaRegistryUrl: \" + schemaRegistryAddress)\n",
    "# print(\"kafkaConsumerGroupForLag: \" + kafkaConsumerGroupForLag)\n",
    "# print(\"startDateOfPipeline : \" + startDateOfPipeline)\n",
    "timestampToStartStreaming = None\n",
    "\n",
    "if startDateOfPipeline != \"\":\n",
    "    timestampToStartStreaming = datetime.datetime.strptime(startDateOfPipeline, \"%d-%m-%Y\")\n",
    "    print(\"timestamp: \" + str(timestampToStartStreaming))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a641c3e6-8fcb-42bf-8fe3-e4bf8987a9ea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>database_name</th><th>table_name</th><th>partition_column</th><th>zorder_column</th><th>partition_column_derive_from_column</th><th>primary_key</th><th>reference_index</th><th>topic_pattern</th><th>table_exists</th><th>is_pattern</th><th>topic_prefix</th><th>kafkaConsumerGroupForLag</th><th>OutputPathForMaterializedView</th><th>kafka_grp</th><th>batchOutputPath</th><th>secondary_key</th></tr></thead><tbody><tr><td>dev_pipeline_test_optimize</td><td>runsheet</td><td>company_id,start_date_part</td><td>id</td><td>start_date_part,start_date</td><td>id</td><td>0</td><td>qatest12nonfes.public.runsheet_old</td><td>false</td><td>false</td><td></td><td>databricks-datalake</td><td>/mnt/fareye-datalake-ca-test/processed55/</td><td>datalake2</td><td>/mnt/fareye-datalake-ca-test/raw55/</td><td>null</td></tr><tr><td>dev_pipeline_test_optimize</td><td>user_summary_old</td><td>company_id,date</td><td>id</td><td>null</td><td>id</td><td>1</td><td>qatest12nonfes.public.user_summary_old</td><td>false</td><td>false</td><td></td><td>databricks-datalake</td><td>/mnt/fareye-datalake-ca-test/processed55/</td><td>datalake2</td><td>/mnt/fareye-datalake-ca-test/raw55/</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dev_pipeline_test_optimize",
         "runsheet",
         "company_id,start_date_part",
         "id",
         "start_date_part,start_date",
         "id",
         0,
         "qatest12nonfes.public.runsheet_old",
         false,
         false,
         "",
         "databricks-datalake",
         "/mnt/fareye-datalake-ca-test/processed55/",
         "datalake2",
         "/mnt/fareye-datalake-ca-test/raw55/",
         null
        ],
        [
         "dev_pipeline_test_optimize",
         "user_summary_old",
         "company_id,date",
         "id",
         null,
         "id",
         1,
         "qatest12nonfes.public.user_summary_old",
         false,
         false,
         "",
         "databricks-datalake",
         "/mnt/fareye-datalake-ca-test/processed55/",
         "datalake2",
         "/mnt/fareye-datalake-ca-test/raw55/",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "database_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "table_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "partition_column",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "zorder_column",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "partition_column_derive_from_column",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "primary_key",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "reference_index",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "topic_pattern",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "table_exists",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "is_pattern",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "topic_prefix",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "kafkaConsumerGroupForLag",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "OutputPathForMaterializedView",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "kafka_grp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "batchOutputPath",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "secondary_key",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tabledf = spark.sql(\"select * from pipeline_common.pipeline_tables where database_name in ({0})\".format(job_database))\n",
    "display(tabledf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "854d88e9-5272-4eb1-8394-17005cee17de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "def toMap(record):\n",
    "    data = {};\n",
    "    data[\"partition_column\"] = record[\"partition_column\"];\n",
    "    data[\"zorder_column\"] = record[\"zorder_column\"];\n",
    "    data[\"primary_key\"] = record[\"primary_key\"];\n",
    "    data[\"partition_column_derive_from_column\"] = record[\"partition_column_derive_from_column\"];\n",
    "    data[\"table_name\"] = record[\"table_name\"];\n",
    "    data[\"reference_index\"] = record[\"reference_index\"];\n",
    "    data[\"topic_pattern\"] = record[\"topic_pattern\"];\n",
    "    data[\"table_exists\"] = record[\"table_exists\"];\n",
    "    data[\"is_pattern\"] = record[\"is_pattern\"];\n",
    "    data[\"topic_prefix\"] = record[\"topic_prefix\"];\n",
    "    data[\"database\"] = record[\"database_name\"];\n",
    "    \n",
    "    data[\"OutputPathForMaterializedView\"] = record[\"OutputPathForMaterializedView\"];\n",
    "    data[\"kafkaConsumerGroupForLag\"] = record[\"kafkaConsumerGroupForLag\"];\n",
    "    data[\"kafka_grp\"] = record[\"kafka_grp\"];\n",
    "    data[\"batchOutputPath\"] = record[\"batchOutputPath\"];\n",
    "    \n",
    "    \n",
    "\n",
    "    return (record['table_name'], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eb979b00-8eeb-43ae-b5b6-53bcba08fa6b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "mapOfTopicAndData = tabledf.rdd.map(lambda record: toMap(record)).collectAsMap()\n",
    "mapForTableToPartition = spark.sparkContext.broadcast(mapOfTopicAndData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7fcdc66b-f888-455c-b04d-8470ab33b8e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getPrefixTopic(topicToProcess:str):\n",
    "  if '_old' in topicToProcess:\n",
    "    return topicToProcess.replace(\"_old\",\"\")\n",
    "  a = topicToProcess.split(\"_\")\n",
    "  try:\n",
    "    month = int(a[len(a)-2])\n",
    "    year = int(a[len(a)-1])\n",
    "    if month>=1 and month <=12 and year >=2019:\n",
    "      res = \"\"\n",
    "      for i in range(len(a)-2):\n",
    "         if i == (len(a)-3): \n",
    "             res+=a[i]\n",
    "         else:\n",
    "             res+=a[i]+\"_\"\n",
    "      return res    \n",
    "  except:        \n",
    "    pass\n",
    "\n",
    "  return topicToProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "607af1b6-a7bf-4963-810e-9fb24afba2a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "global_variables = Global_variables()\n",
    "global_variables.sc = spark.sparkContext\n",
    "global_variables.spark = spark\n",
    "global_variables.schemaRegistryUrl = schemaRegistryAddress\n",
    "# global_variables.batchOutputPath = batchOutputPath\n",
    "# global_variables.kafka_grp = kafkaGrp\n",
    "# global_variables.OutputPathForMaterializedView = OutputPathForMaterializedView\n",
    "global_variables.bootstrapServers = bootstrapServers\n",
    "# global_variables.kafkaConsumerGroupForLag = kafkaConsumerGroupForLag\n",
    "global_variables.startDateOfPipeline = startDateOfPipeline\n",
    "global_variables.timestampToStartStreaming = timestampToStartStreaming\n",
    "global_variables.test_data = None\n",
    "global_variables.test_schema = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "920c9271-59cb-4855-8d80-006265631df5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %python\n",
    "# from pyspark.sql import DataFrame\n",
    "\n",
    "# df4 = sc._jvm.example10.Hello.add(dbutils.widgets.get(\"bootstrapServers\"), dbutils.widgets.get(\"kafkaConsumerGroupForLag\"))\n",
    "\n",
    "# print(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3f2f33e1-4d12-4967-8424-60169fd50066",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " %scala\n",
    "//  import java.util\n",
    "//  import com.fasterxml.jackson.databind.{DeserializationFeature, ObjectMapper}\n",
    "//  import com.fasterxml.jackson.module.scala.DefaultScalaModule\n",
    "//  import org.apache.kafka.clients.consumer.{KafkaConsumer, OffsetAndMetadata}\n",
    "//  import org.apache.kafka.common.TopicPartition\n",
    "//  import org.apache.kafka.clients.KafkaClient\n",
    "//  import java.util.Properties\n",
    "//  import org.apache.spark.sql.streaming.StreamingQueryListener\n",
    "//  import org.apache.spark.sql.streaming.StreamingQueryListener._\n",
    " \n",
    "//  spark.streams.addListener(new StreamingQueryListener() {\n",
    " \n",
    "//      val props = new Properties()\n",
    "//      val bootstrapServers =  dbutils.widgets.get(\"bootstrapServers\")\n",
    "//      val kafkaConsumerGroupForLag  =  dbutils.widgets.get(\"kafkaConsumerGroupForLag\")\n",
    "//      props.put(\"bootstrap.servers\", bootstrapServers)\n",
    "//      props.put(\"group.id\", kafkaConsumerGroupForLag )\n",
    "//      props.put(\"key.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\")\n",
    "//      props.put(\"value.deserializer\", \"org.apache.kafka.common.serialization.StringDeserializer\")\n",
    "//      private val consumer = new KafkaConsumer[String, String](props)\n",
    "//      override def onQueryStarted(queryStarted: QueryStartedEvent): Unit = {\n",
    "//          println(\"Query started: \" + queryStarted.id)\n",
    "//      }\n",
    "//      override def onQueryTerminated(queryTerminated: QueryTerminatedEvent): Unit = {\n",
    "//          println(\"Query terminated: \" + queryTerminated.id)\n",
    "//      }\n",
    "//      override def onQueryProgress(event: QueryProgressEvent): Unit = {\n",
    " \n",
    "//        val om = new ObjectMapper().configure(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES, false)\n",
    "//        om.registerModule(DefaultScalaModule)\n",
    "//        event.progress.sources.foreach(source => {\n",
    "//          val jsonOffsets = om.readValue(source.endOffset, classOf[Map[String, Map[String, Int]]])\n",
    "//          jsonOffsets.keys\n",
    "//            .foreach(topic => {\n",
    "//              val topicPartitionMap = new util.HashMap[TopicPartition, OffsetAndMetadata]()\n",
    "//              val offsets = jsonOffsets.get(topic)\n",
    "//              offsets match {\n",
    "//                case Some(topicOffsetData) =>\n",
    "//                  topicOffsetData.keys.foreach(partition => {\n",
    "//                    val tp = new TopicPartition(topic, partition.toInt)\n",
    "//                    val oam = new OffsetAndMetadata(topicOffsetData(partition).toLong)\n",
    "//                    topicPartitionMap.put(tp, oam)\n",
    "//                  })\n",
    "//                case _ =>\n",
    " \n",
    "//              }\n",
    "//              consumer.commitSync(topicPartitionMap)\n",
    "//              println(\"Query made progress: \" + topicPartitionMap)\n",
    "//            })\n",
    "//        })\n",
    "//      }\n",
    "//  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2e1753f8-a951-4924-9913-f69cb94adf6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getJsonSchema(schemaId, schemasById, registrySchemaUrl):\n",
    "  jsonSchema = schemasById.get(schemaId)\n",
    "  if jsonSchema is None:\n",
    "    src = SchemaRegistryClient(registrySchemaUrl)\n",
    "    schema = src.get_by_id(schemaId)\n",
    "    jsonSchema = json.dumps(schema.flat_schema)\n",
    "    schemasById[schemaId] = jsonSchema\n",
    "  return jsonSchema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "afdc4391-7a25-4408-978e-1f5df7cf8187",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def getTableColumn(my_schema):\n",
    "  tableList = []\n",
    "  col = []\n",
    "  inputTable = \"updatesTable.\"\n",
    "  targetTable = \"targetTable.\"\n",
    "  \n",
    "  ignoreColumns = [\"table_key_ts\",\"table_delete\",\"table_key\"]\n",
    " \n",
    "  for field in my_schema.fields:\n",
    "    if field.name == \"after\":\n",
    "        for f3 in field.dataType:\n",
    "          te = f3.name+\" \"+f3.dataType.typeName()\n",
    "          if f3.name not in ignoreColumns:\n",
    "             tableList.append(te) \n",
    "             col.append(f3.name) \n",
    "    else:\n",
    "        te = field.name+\" \"+field.dataType.typeName()\n",
    "        if field.name not in ignoreColumns:\n",
    "          tableList.append(te)\n",
    "          col.append(field.name)\n",
    "  res = \"\"\n",
    "  insertResInp = \"\"\n",
    "  insertResOut = \"\"\n",
    "  merge = \"\"\n",
    "  for i in range(len(tableList)): \n",
    "      if i == (len(tableList)-1): \n",
    "         res+=tableList[i]\n",
    "         insertResInp+=col[i]\n",
    "         insertResOut+=inputTable+col[i]\n",
    "         merge+=targetTable+col[i] + \" = \" + inputTable+col[i]         \n",
    "      else:\n",
    "         res+=tableList[i]+\",\"\n",
    "         insertResInp+=col[i]+\",\"          \n",
    "         insertResOut+=inputTable+col[i]+\",\"\n",
    "         merge+=targetTable+col[i]+ \" = \" + inputTable+col[i] +\",\"\n",
    "  ans = []\n",
    "  ans.append(res)\n",
    "  ans.append(insertResInp)\n",
    "  ans.append(insertResOut)\n",
    "  ans.append(merge)\n",
    "#   print(ans)\n",
    "  return ans  \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "166ddd07-0879-4150-8909-8bbc17fdb667",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # deltaStream = spark.readStream.format(\"delta\").load(\"/tmp/fareye-datalake/prod_pipeline_xx_1/track_log\")\n",
    "# # display(deltaStream)\n",
    "\n",
    "# def send_alert_mail(topic):\n",
    "#   mailData = '{{\"fromEmail\":\"no-reply@getfareye.com\", \"toEmail\":\"saurabh.nigam@getfareye.com\", \"sub\":\"Alert in pipeline aws ca\",\"messageBody\":\"Pipeline stopped topic: {}\",\"serverName\":\"databricks-aws-ca\"}}'.format(topic)\n",
    "#   return\n",
    "#   headers = {'apiKey': 'auX6DIJk4ZAF2Q1505373210570ung72CmxZIsrnLM27K'}\n",
    "#   payload = {\n",
    "#       'mailData': (None, mailData,'application/json')\n",
    "#   }\n",
    "#   response = requests.post('https://email-service.fareye.co/app/rest/mail/sendMail', files=payload, headers=headers)\n",
    "#   print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fbe590db-520d-4d1b-addf-f02d62e9757b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# def kafka_job_wrapper(record):\n",
    "#     try:\n",
    "#         kafka_job(record)\n",
    "#     except Exception as e:\n",
    "#         print(\"Exception in wrapper.{0}\".format(record[\"table_name\"]), e)\n",
    "# #         send_alert_mail(record[\"table_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "67f02a71-9db2-4173-ad5f-08d920b44150",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# def parallelKafkaWorkers(dataMap, numInParallel):\n",
    "#     returnRes = []\n",
    "#     with ThreadPoolExecutor(max_workers=100) as ec:\n",
    "#         for index in dataMap:\n",
    "\n",
    "#             if dataMap[index]:\n",
    "#                 print(index)\n",
    "                \n",
    "#                 kj = KafkaJob(dataMap[index] , global_variables)\n",
    "#                 returnRes.append(ec.submit(kafka_job_wrapper,kj))\n",
    "#     return returnRes               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8d094ebb-685c-4e9d-9968-e7dcca949de7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(len(mapOfTopicAndData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b4b85913-86bf-40ed-beec-41b9ac413fff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import  concurrent.futures\n",
    "# res = parallelKafkaWorkers(mapOfTopicAndData, len(mapOfTopicAndData))\n",
    "# for future in concurrent.futures.as_completed(res):\n",
    "#         try:\n",
    "#             print(future.result())\n",
    "#         except:\n",
    "#             print(\"Exception.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8af3c201-55ca-456c-9311-bec4e697c63a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# result = [i.result() for i in res] # This is a blocking call.\n",
    "# print(result)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "afcaabfa-3bd2-4948-ad97-2e1e2ff16ced",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">dev_pipeline_test_optimize.user_summary_old\n",
       "after merge\n",
       "2022-03-01 12:08:12.974273\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">dev_pipeline_test_optimize.user_summary_old\nafter merge\n2022-03-01 12:08:12.974273\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "schemaRegistryAddress = None\n",
    "\n",
    "# batchOutputPath = None\n",
    "\n",
    "kafkaGrp = None\n",
    "\n",
    "# OutputPathForMaterializedView = None\n",
    "\n",
    "schemasByIdDict = {}\n",
    "bootstrapServers = None\n",
    "kafkaConsumerGroupForLag = None\n",
    "startDateOfPipeline = None\n",
    "schemaRegistryAddress = dbutils.widgets.get(\"schemaRegistryUrl\")\n",
    "\n",
    "# batchOutputPath = dbutils.widgets.get(\"batchOutputPath\")\n",
    "\n",
    "# kafkaGrp = dbutils.widgets.get(\"kafka-grp\")\n",
    "\n",
    "# OutputPathForMaterializedView = dbutils.widgets.get(\"OutputPathForMaterializedView\")\n",
    "\n",
    "schemasByIdDict = {}\n",
    "bootstrapServers = dbutils.widgets.get(\"bootstrapServers\")\n",
    "kafkaConsumerGroupForLag = dbutils.widgets.get(\"kafkaConsumerGroupForLag\")\n",
    "startDateOfPipeline = dbutils.widgets.get(\"startDateOfPipeline\")\n",
    "TO_EMAIL = dbutils.widgets.get(\"TO_EMAIL\")\n",
    "EMAIL_SERVICE_URL = dbutils.widgets.get(\"EMAIL_SERVICE_URL\")\n",
    "EMAIL_SERVICE_API_KEY = dbutils.widgets.get(\"EMAIL_SERVICE_API_KEY\")\n",
    "SUBJECT_EMAIL = dbutils.widgets.get(\"SUBJECT_EMAIL\")\n",
    "job_database = dbutils.widgets.get(\"database\")\n",
    "\n",
    "# print(\"kafkaGrp: \" + kafkaGrp)\n",
    "print(\"bootstrapServers: \" + bootstrapServers)\n",
    "print(\"schemaRegistryUrl: \" + schemaRegistryAddress)\n",
    "print(\"kafkaConsumerGroupForLag: \" + kafkaConsumerGroupForLag)\n",
    "print(\"startDateOfPipeline : \" + startDateOfPipeline)\n",
    "timestampToStartStreaming = None\n",
    "\n",
    "print(\"db is\")\n",
    "print(job_database)\n",
    "\n",
    "if startDateOfPipeline != \"\":\n",
    "    timestampToStartStreaming = datetime.datetime.strptime(startDateOfPipeline, \"%d-%m-%Y\")\n",
    "    print(\"timestamp: \" + str(timestampToStartStreaming))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "tabledf = spark.sql(\"select * from pipeline_common.pipeline_tables where database_name in ({0})\".format(job_database))\n",
    "# tabledf = spark.sql(\"select * from pipeline_common.pipeline_tables where database_name in ({0}) and table_name = 'field_data' \".format(job_database))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "display(tabledf)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def toMap(record):\n",
    "    data = {};\n",
    "    data[\"partition_column\"] = record[\"partition_column\"];\n",
    "    data[\"zorder_column\"] = record[\"zorder_column\"];\n",
    "    data[\"primary_key\"] = record[\"primary_key\"];\n",
    "    data[\"partition_column_derive_from_column\"] = record[\"partition_column_derive_from_column\"];\n",
    "    data[\"table_name\"] = record[\"table_name\"];\n",
    "    data[\"reference_index\"] = record[\"reference_index\"];\n",
    "    data[\"topic_pattern\"] = record[\"topic_pattern\"];\n",
    "    data[\"table_exists\"] = record[\"table_exists\"];\n",
    "    data[\"is_pattern\"] = record[\"is_pattern\"];\n",
    "    data[\"topic_prefix\"] = record[\"topic_prefix\"];\n",
    "    data[\"database\"] = record[\"database_name\"];\n",
    "    \n",
    "    data[\"secondary_key\"] = record[\"secondary_key\"];\n",
    "    \n",
    "    \n",
    "#     data[\"OutputPathForMaterializedView\"] = \"/mnt/fareye-datalake-ca-test/processed54/\"\n",
    "#     data[\"kafkaConsumerGroupForLag\"] = \"databricks-datalake\"\n",
    "#     data[\"kafka_grp\"] = \"datalake2\"\n",
    "#     data[\"batchOutputPath\"] = \"/mnt/fareye-datalake-ca-test/raw54/\"\n",
    "    \n",
    "#     data[\"OutputPathForMaterializedView\"] = record[\"OutputPathForMaterializedView\"];\n",
    "#     data[\"batchOutputPath\"] = record[\"batchOutputPath\"];\n",
    "    \n",
    "    data[\"OutputPathForMaterializedView\"] = \"/mnt/fareye-datalake-ca-test/processed717/\";\n",
    "    data[\"batchOutputPath\"] = \"/mnt/fareye-datalake-ca-test/raw717/\";\n",
    "    \n",
    "    \n",
    "    rec_comb = record['table_name'] + \"_\" + record['database_name']\n",
    "\n",
    "    \n",
    "\n",
    "    return (rec_comb, data)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "mapOfTopicAndData = tabledf.rdd.map(lambda record: toMap(record)).collectAsMap()\n",
    "mapForTableToPartition = spark.sparkContext.broadcast(mapOfTopicAndData)\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "global_variables = Global_variables()\n",
    "global_variables.sc = spark.sparkContext\n",
    "global_variables.spark = spark\n",
    "global_variables.schemaRegistryUrl = schemaRegistryAddress\n",
    "\n",
    "# global_variables.batchOutputPath = batchOutputPath\n",
    "\n",
    "# global_variables.kafka_grp = kafkaGrp\n",
    "\n",
    "# global_variables.OutputPathForMaterializedView = OutputPathForMaterializedView\n",
    "\n",
    "global_variables.bootstrapServers = bootstrapServers\n",
    "global_variables.kafkaConsumerGroupForLag = kafkaConsumerGroupForLag\n",
    "global_variables.startDateOfPipeline = startDateOfPipeline\n",
    "global_variables.timestampToStartStreaming = timestampToStartStreaming\n",
    "global_variables.test_data = None\n",
    "global_variables.test_schema = None\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "def send_alert_mail(topic):\n",
    "    mailData = '{{\"fromEmail\":\"no-reply@getfareye.com\", \"toEmail\":\"{}\", \"sub\":\"{}\",\"messageBody\":\"Pipeline stopped topic: {}\",\"serverName\":\"databricks\"}}'.format(\n",
    "        TO_EMAIL,SUBJECT_EMAIL,topic)\n",
    "\n",
    "    headers = {'apiKey': EMAIL_SERVICE_API_KEY}\n",
    "    payload = {\n",
    "        'mailData': (None, mailData, 'application/json')\n",
    "    }\n",
    "\n",
    "    response = requests.post(EMAIL_SERVICE_URL, files=payload, headers=headers)\n",
    "    print(response.json())\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "\n",
    "def getJsonSchema(schemaId, schemasById, registrySchemaUrl):\n",
    "    jsonSchema = schemasById.get(schemaId)\n",
    "    if jsonSchema is None:\n",
    "        src = SchemaRegistryClient(registrySchemaUrl)\n",
    "        schema = src.get_by_id(schemaId)\n",
    "        jsonSchema = json.dumps(schema.flat_schema)\n",
    "        schemasById[schemaId] = jsonSchema\n",
    "    return jsonSchema\n",
    "\n",
    "\n",
    "# # COMMAND ----------\n",
    "def kafka_job_wrapper(kj):\n",
    "    try:\n",
    "        kj.start_pipeline_with_avro()\n",
    "    except Exception as e:\n",
    "        print(\"Exception in wrapper.{0}\".format(kj.table), e)\n",
    "        send_alert_mail(kj.table)\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def parallelKafkaWorkers(dataMap, numInParallel):\n",
    "    returnRes = []\n",
    "    with ThreadPoolExecutor(max_workers=100) as ec:\n",
    "        for index in dataMap:\n",
    "\n",
    "            if dataMap[index]:\n",
    "                print(index)\n",
    "                kj = KafkaJob(dataMap[index] , global_variables)\n",
    "                returnRes.append(ec.submit(kafka_job_wrapper,kj))\n",
    "    return returnRes\n",
    "\n",
    "\n",
    "# COMMAND ----------\n",
    "print(\"lenght is\")\n",
    "print(len(mapOfTopicAndData))\n",
    "\n",
    "print(mapOfTopicAndData)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "res = parallelKafkaWorkers(mapOfTopicAndData, len(mapOfTopicAndData))\n",
    "for future in concurrent.futures.as_completed(res):\n",
    "    try:\n",
    "        print(future.result())\n",
    "    except:\n",
    "        print(\"Exception.\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# try:\n",
    "#    for index in mapOfTopicAndData:\n",
    "#         if mapOfTopicAndData[index]:\n",
    "#             print(\"index is\")\n",
    "#             print(index)\n",
    "#             kj = KafkaJob(mapOfTopicAndData[index] , global_variables)\n",
    "#             kj.start_pipeline_with_avro()\n",
    "# except Exception as e:\n",
    "#     print(\"Exception in wrapper.{0}\".format(kj.table), e)\n",
    "#     send_alert_mail(kj.table)\n",
    "\n",
    "result = [i.result() for i in res]  # This is a blocking call.\n",
    "print(result)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "723edab6-88f7-4aa3-a7bf-abe60d0cd83d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# select * from pipeline_common.pipeline_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "90cc9f44-fc2a-470c-863d-cedd64638425",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select * from dev_pipeline_test_optimize.runsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1c0a2c1d-ca6d-4e2c-b9df-c18d996febb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# truncate table dev_pipeline_test_optimize.user_summary_old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c2b423e0-38e4-47b0-a486-a0693be0e49a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "UNIT TESTING CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9652ee55-cdd4-424d-be84-b3ce831f9d8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "from abc import ABC, abstractmethod\n",
    "from argparse import ArgumentParser\n",
    "from logging import Logger\n",
    "from typing import Dict, Any\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# abstract class for jobs\n",
    "class Job(ABC):\n",
    "    @abstractmethod\n",
    "    def init_adapter(self):\n",
    "        \"\"\"\n",
    "        Init adapter is an abstract method to perform some particular settings in the Job subclass.\n",
    "        Method is called after creation of the SparkSession.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def __init__(self, spark=None, init_conf=None):\n",
    "        self.spark = self._prepare_spark(spark)\n",
    "        self.logger = self._prepare_logger()\n",
    "        self.dbutils = self.get_dbutils()\n",
    "        if init_conf:\n",
    "            self.conf = init_conf\n",
    "        else:\n",
    "            self.conf = self._provide_config()\n",
    "        self.init_adapter()\n",
    "        self._log_conf()\n",
    "\n",
    "    @staticmethod\n",
    "    def _prepare_spark(spark) -> SparkSession:\n",
    "        if not spark:\n",
    "            return SparkSession.builder.getOrCreate()\n",
    "        else:\n",
    "            return spark\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_dbutils(spark: SparkSession):\n",
    "        try:\n",
    "            from pyspark.dbutils import DBUtils # noqa\n",
    "            if \"dbutils\" not in locals():\n",
    "                utils = DBUtils(spark)\n",
    "                return utils\n",
    "            else:\n",
    "                return locals().get(\"dbutils\")\n",
    "        except ImportError:\n",
    "            return None\n",
    "\n",
    "    def get_dbutils(self):\n",
    "        utils = self._get_dbutils(self.spark)\n",
    "\n",
    "        if not utils:\n",
    "            self.logger.warn(\"No DBUtils defined in the runtime\")\n",
    "        else:\n",
    "            self.logger.info(\"DBUtils class initialized\")\n",
    "\n",
    "        return utils\n",
    "\n",
    "    def _provide_config(self):\n",
    "        self.logger.info(\"Reading configuration from --conf-file job option\")\n",
    "        conf_file = self._get_conf_file()\n",
    "        if not conf_file:\n",
    "            self.logger.info(\n",
    "                \"No conf file was provided, setting configuration to empty dict.\"\n",
    "                \"Please override configuration in subclass init method\"\n",
    "            )\n",
    "            return {}\n",
    "        else:\n",
    "            self.logger.info(\n",
    "                f\"Conf file was provided, reading configuration from {conf_file}\"\n",
    "            )\n",
    "            return self._read_config(conf_file)\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_conf_file():\n",
    "        p = ArgumentParser()\n",
    "        p.add_argument(\"--conf-file\", required=False, type=str)\n",
    "        namespace = p.parse_known_args()[0]\n",
    "        return namespace.conf_file\n",
    "\n",
    "    def _read_config(self, conf_file) -> Dict[str, Any]:\n",
    "        raw_content = \"\".join(\n",
    "            self.spark.read.format(\"text\").load(conf_file).toPandas()[\"value\"].tolist()\n",
    "        )\n",
    "        config = json.loads(raw_content)\n",
    "        return config\n",
    "\n",
    "    def _prepare_logger(self) -> Logger:\n",
    "        log4j_logger = self.spark._jvm.org.apache.log4j # noqa\n",
    "        return log4j_logger.LogManager.getLogger(self.__class__.__name__)\n",
    "\n",
    "    def _log_conf(self):\n",
    "        # log parameters\n",
    "        self.logger.info(\"Launching job with configuration parameters:\")\n",
    "        for key, item in self.conf.items():\n",
    "            self.logger.info(\"\\t Parameter: %-30s with value => %-30s\" % (key, item))\n",
    "\n",
    "    @abstractmethod\n",
    "    def launch(self):\n",
    "        \"\"\"\n",
    "        Main method of the job.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fad8dc88-0ffc-4f6c-80ef-4cfd058e39fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e4cf5f8a-6ffd-46e9-9ea9-bb2902b7f2d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "\n",
    "\n",
    "class TestJob(KafkaJob):\n",
    "\n",
    "    def start_read_stream(self):\n",
    "        rddjson = self.sc.parallelize([self.test_data])\n",
    "        kafka = self.spark.read.json(rddjson)\n",
    "        kafka = kafka.filter(\"value is NOT NULL\");\n",
    "        kafka= kafka.withColumn('value', from_json(col('value'), self.test_schema))\n",
    "        return kafka\n",
    "\n",
    "    def start_write_stream(self, kafka, method_to_run_on_for_each):\n",
    "        self.spark.sql(\"create database  IF NOT EXISTS  {} LOCATION  '{}'\".format(self.database, self.OutputPathForMaterializedView))\n",
    "        self.spark.sql(\"use {}\".format(self.database))\n",
    "        method_to_run_on_for_each(kafka,1)\n",
    "\n",
    "    @staticmethod\n",
    "    def write_dataframe(kafka):\n",
    "        kafka.write \\\n",
    "            .format(\"delta\").mode(\"overwrite\") \\\n",
    "            .save(\"/tmp/qwerty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1853c088-0d28-4e86-b50c-68f9bd2e0c9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from concurrent.futures.thread import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "class PipelineJob(Job):\n",
    "\n",
    "    def init_adapter(self):\n",
    "        if not self.conf:\n",
    "            self.logger.info(\"Init configuration was not provided, using configuration from default_init method\")\n",
    "            self.conf = {\n",
    "                \"output_format\": \"delta\",\n",
    "                \"output_path\": \"dbfs:/dbx/tmp/test/interactive/cicd_sample_project\"\n",
    "            }\n",
    "        else:\n",
    "            self.logger.info(\"Init configuration is already provided\")\n",
    "\n",
    "    def launch(self):\n",
    "        self.logger.info(\"Launching bootstrap job\")\n",
    "\n",
    "        # df = self.spark.range(0, 1000)\n",
    "        # df.write.format(self.conf[\"output_format\"]).mode(\"overwrite\").save(self.conf[\"output_path\"])\n",
    "\n",
    "\n",
    "        self.logger.info(\"Bootstrap job finished!\")\n",
    "\n",
    "class TestPipelineJob(Job):\n",
    "\n",
    "    def init_adapter(self):\n",
    "        if not self.conf:\n",
    "            self.logger.info(\"Init configuration was not provided, using configuration from default_init method\")\n",
    "            self.conf = {\n",
    "                \"output_format\": \"delta\",\n",
    "                \"output_path\": \"dbfs:/dbx/tmp/test/interactive/cicd_sample_project\"\n",
    "            }\n",
    "        else:\n",
    "            self.logger.info(\"Init configuration is already provided\")\n",
    "\n",
    "    @staticmethod\n",
    "    def parallelKafkaWorkers(conf, numInParallel):\n",
    "        returnRes = []\n",
    "        dataMap = conf.get(\"mapOfTopicAndData\")\n",
    "        global_variables = conf.get(\"global_variables\")\n",
    "        with ThreadPoolExecutor(max_workers=100) as ec:\n",
    "            for index in dataMap:\n",
    "\n",
    "                if dataMap[index]:\n",
    "                    print(index)\n",
    "                    kj = TestJob(dataMap[index], global_variables)\n",
    "                    returnRes.append(ec.submit(kj.start_pipeline_without_avro))\n",
    "        return returnRes\n",
    "\n",
    "    def launch(self):\n",
    "        self.logger.info(\"Launching bootstrap job\")\n",
    "\n",
    "        # df = self.spark.range(0, 1000)\n",
    "        # df.write.format(self.conf[\"output_format\"]).mode(\"overwrite\").save(self.conf[\"output_path\"])\n",
    "\n",
    "        print(len(self.conf.get(\"mapOfTopicAndData\")))\n",
    "        res = self.parallelKafkaWorkers(self.conf, len(self.conf.get(\"mapOfTopicAndData\")))\n",
    "        for future in concurrent.futures.as_completed(res):\n",
    "            try:\n",
    "                print(future.result())\n",
    "            except Exception as ex:\n",
    "                print(ex)\n",
    "\n",
    "        result = [i.result() for i in res]  # This is a blocking call.\n",
    "        print(result)\n",
    "\n",
    "        self.logger.info(\"Bootstrap job finished!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    job = PipelineJob()\n",
    "    job.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "50f3f87a-d751-4418-81bd-d217cee5600c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "import time\n",
    "import unittest\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType\n",
    "import json\n",
    "\n",
    "\n",
    "  \n",
    "def get_dbutils(spark):\n",
    "    try:\n",
    "        from pyspark.dbutils import DBUtils\n",
    "        dbutils = DBUtils(spark)\n",
    "    except ImportError:\n",
    "        import IPython\n",
    "        dbutils = IPython.get_ipython().user_ns[\"dbutils\"]\n",
    "    return dbutils\n",
    "  \n",
    "TESTDATA_INSERT = \"\"\"[{\"key\":\"AAAABoMIMzAwMg==\",\"value\":\"{\\\\\"before\\\\\":null,\\\\\"after\\\\\":{\\\\\"id\\\\\":\\\\\"3002\\\\\",\\\\\"username\\\\\":\\\\\"3002\\\\\",\\\\\"created_on\\\\\":\\\\\"2021-03-02T19:50:29.363+0000\\\\\",\\\\\"test\\\\\":\\\\\"1\\\\\",\\\\\"test3\\\\\":\\\\\"2\\\\\",\\\\\"t\\\\\":\\\\\"3\\\\\",\\\\\"pink\\\\\":null,\\\\\"pink2\\\\\":null,\\\\\"test10\\\\\":null,\\\\\"test11\\\\\":null,\\\\\"test12\\\\\":null},\\\\\"source\\\\\":{\\\\\"version\\\\\":\\\\\"0.10.0.Final\\\\\",\\\\\"connector\\\\\":\\\\\"postgresql\\\\\",\\\\\"name\\\\\":\\\\\"temporal\\\\\",\\\\\"ts_ms\\\\\":1614714629363,\\\\\"snapshot\\\\\":\\\\\"false\\\\\",\\\\\"db\\\\\":\\\\\"test\\\\\",\\\\\"schema\\\\\":\\\\\"public\\\\\",\\\\\"table\\\\\":\\\\\"jb\\\\\",\\\\\"txId\\\\\":30408688,\\\\\"lsn\\\\\":484660223888,\\\\\"xmin\\\\\":null},\\\\\"op\\\\\":\\\\\"c\\\\\",\\\\\"ts_ms\\\\\":1614714629363}\",\"topic\":\"temporal.public.jb\",\"partition\":\"8\",\"offset\":\"1\",\"timestamp\":\"2021-03-02T19:50:29.745+0000\",\"timestampType\":\"0\"},\n",
    "{\"key\":\"AAAABoMIMzAwMQ==\",\"value\":\"{\\\\\"before\\\\\":null,\\\\\"after\\\\\":{\\\\\"id\\\\\":\\\\\"3001\\\\\",\\\\\"username\\\\\":\\\\\"3001\\\\\",\\\\\"created_on\\\\\":\\\\\"2021-03-02T19:49:56.888+0000\\\\\",\\\\\"test\\\\\":\\\\\"1\\\\\",\\\\\"test3\\\\\":\\\\\"2\\\\\",\\\\\"t\\\\\":\\\\\"3\\\\\",\\\\\"pink\\\\\":null,\\\\\"pink2\\\\\":null,\\\\\"test10\\\\\":null,\\\\\"test11\\\\\":null,\\\\\"test12\\\\\":null},\\\\\"source\\\\\":{\\\\\"version\\\\\":\\\\\"0.10.0.Final\\\\\",\\\\\"connector\\\\\":\\\\\"postgresql\\\\\",\\\\\"name\\\\\":\\\\\"temporal\\\\\",\\\\\"ts_ms\\\\\":1614714596888,\\\\\"snapshot\\\\\":\\\\\"false\\\\\",\\\\\"db\\\\\":\\\\\"test\\\\\",\\\\\"schema\\\\\":\\\\\"public\\\\\",\\\\\"table\\\\\":\\\\\"jb\\\\\",\\\\\"txId\\\\\":30408687,\\\\\"lsn\\\\\":484660223568,\\\\\"xmin\\\\\":null},\\\\\"op\\\\\":\\\\\"c\\\\\",\\\\\"ts_ms\\\\\":1614714596888}\",\"topic\":\"temporal.public.jb\",\"partition\":\"9\",\"offset\":\"2\",\"timestamp\":\"2021-03-02T19:49:57.245+0000\",\"timestampType\":\"0\"}]\n",
    "\"\"\"\n",
    "\n",
    "TESTDATA_UPDATE = \"\"\"[{\"key\":\"AAAABoMIMzAwMQ==\",\"value\":\"{\\\\\"before\\\\\":{\\\\\"id\\\\\":\\\\\"3002\\\\\",\\\\\"username\\\\\":\\\\\"__debezium_unavailable_value\\\\\",\\\\\"created_on\\\\\":\\\\\"1970-01-01T00:00:00.000+0000\\\\\",\\\\\"test\\\\\":\\\\\"__debezium_unavailable_value\\\\\",\\\\\"test3\\\\\":\\\\\"__debezium_unavailable_value\\\\\",\\\\\"t\\\\\":\\\\\"__debezium_unavailable_value\\\\\",\\\\\"pink\\\\\":null,\\\\\"pink2\\\\\":null,\\\\\"test10\\\\\":null,\\\\\"test11\\\\\":null,\\\\\"test12\\\\\":null},\\\\\"after\\\\\":null,\\\\\"source\\\\\":{\\\\\"version\\\\\":\\\\\"0.10.0.Final\\\\\",\\\\\"connector\\\\\":\\\\\"postgresql\\\\\",\\\\\"name\\\\\":\\\\\"temporal\\\\\",\\\\\"ts_ms\\\\\":1614714660395,\\\\\"snapshot\\\\\":\\\\\"false\\\\\",\\\\\"db\\\\\":\\\\\"test\\\\\",\\\\\"schema\\\\\":\\\\\"public\\\\\",\\\\\"table\\\\\":\\\\\"jb\\\\\",\\\\\"txId\\\\\":30408689,\\\\\"lsn\\\\\":484660224088,\\\\\"xmin\\\\\":null},\\\\\"op\\\\\":\\\\\"d\\\\\",\\\\\"ts_ms\\\\\":1614714660395}\",\"topic\":\"temporal.public.jb\",\"partition\":\"9\",\"offset\":\"3\",\"timestamp\":\"2021-03-02T19:51:00.745+0000\",\"timestampType\":\"0\"},\n",
    "{\"key\":\"AAAABoMIMzAwMQ==\",\"value\":\"{\\\\\"before\\\\\":{\\\\\"id\\\\\":\\\\\"3001\\\\\",\\\\\"username\\\\\":\\\\\"__debezium_unavailable_value\\\\\",\\\\\"created_on\\\\\":\\\\\"1970-01-01T00:00:00.000+0000\\\\\",\\\\\"test\\\\\":\\\\\"__debezium_unavailable_value\\\\\",\\\\\"test3\\\\\":\\\\\"__debezium_unavailable_value\\\\\",\\\\\"t\\\\\":\\\\\"__debezium_unavailable_value\\\\\",\\\\\"pink\\\\\":null,\\\\\"pink2\\\\\":null,\\\\\"test10\\\\\":null,\\\\\"test11\\\\\":null,\\\\\"test12\\\\\":null},\\\\\"after\\\\\":{\\\\\"id\\\\\":\\\\\"3001\\\\\",\\\\\"username\\\\\":\\\\\"9999\\\\\",\\\\\"created_on\\\\\":\\\\\"2021-03-02T19:51:16.275+0000\\\\\",\\\\\"test\\\\\":\\\\\"1\\\\\",\\\\\"test3\\\\\":\\\\\"2\\\\\",\\\\\"t\\\\\":\\\\\"3\\\\\",\\\\\"pink\\\\\":null,\\\\\"pink2\\\\\":null,\\\\\"test10\\\\\":null,\\\\\"test11\\\\\":null,\\\\\"test12\\\\\":null},\\\\\"source\\\\\":{\\\\\"version\\\\\":\\\\\"0.10.0.Final\\\\\",\\\\\"connector\\\\\":\\\\\"postgresql\\\\\",\\\\\"name\\\\\":\\\\\"temporal\\\\\",\\\\\"ts_ms\\\\\":1614714723865,\\\\\"snapshot\\\\\":\\\\\"false\\\\\",\\\\\"db\\\\\":\\\\\"test\\\\\",\\\\\"schema\\\\\":\\\\\"public\\\\\",\\\\\"table\\\\\":\\\\\"jb\\\\\",\\\\\"txId\\\\\":30408691,\\\\\"lsn\\\\\":484676996640,\\\\\"xmin\\\\\":null},\\\\\"op\\\\\":\\\\\"u\\\\\",\\\\\"ts_ms\\\\\":1614714723865}\",\"topic\":\"temporal.public.jb\",\"partition\":\"9\",\"offset\":\"6\",\"timestamp\":\"2021-03-02T19:52:04.244+0000\",\"timestampType\":\"0\"}]\n",
    "\"\"\"\n",
    "\n",
    "JSON_SCHEMA = '{\"fields\":[{\"metadata\":{},\"name\":\"after\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"created_on\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"id\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"pink\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"pink2\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"t\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"test\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"test10\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"test11\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"test12\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"test3\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"username\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"before\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"created_on\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"id\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"pink\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"pink2\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"t\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"test\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"test10\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"test11\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"test12\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"test3\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"username\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"op\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"source\",\"nullable\":true,\"type\":{\"fields\":[{\"metadata\":{},\"name\":\"connector\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"db\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"lsn\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"name\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"schema\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"snapshot\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"table\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"ts_ms\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"txId\",\"nullable\":true,\"type\":\"long\"},{\"metadata\":{},\"name\":\"version\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"xmin\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}},{\"metadata\":{},\"name\":\"ts_ms\",\"nullable\":true,\"type\":\"long\"}],\"type\":\"struct\"}'\n",
    "\n",
    "\n",
    "class PipelineUnitTestCases(unittest.TestCase, TestPipelineJob):\n",
    "\n",
    "    folder = \"\"\n",
    "    sparkCls = \"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def toMap(path,folder):\n",
    "        data = {\"partition_column\": None, \"zorder_column\": \"id\", \"primary_key\": \"id\",\n",
    "                \"partition_column_derive_from_column\": None, \"table_name\": \"jb\", \"reference_index\": 0,\n",
    "                \"topic_pattern\": path, \"table_exists\": False, \"is_pattern\": False, \"topic_prefix\": \"temporal.public.\",\n",
    "                \"database\": \"database_test_%s\" % folder}\n",
    "        record = {data[\"table_name\"]: data}\n",
    "        return record\n",
    "\n",
    "    @staticmethod\n",
    "    def current_milli_time():\n",
    "        return round(time.time() * 1000)\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.folder = str(cls.current_milli_time())\n",
    "        print(cls.folder)\n",
    "        cls.sparkCls = SparkSession.builder.getOrCreate()\n",
    "        # cls.sparkCls = SparkSession.builder.master(\"local[1]\").getOrCreate()\n",
    "\n",
    "    def setUp(self):\n",
    "        self.spark = self.sparkCls\n",
    "        path = \"dbfs:/tmp/database_test_%s\" % self.folder\n",
    "        global_variables = Global_variables()\n",
    "        global_variables.sc = self.spark.sparkContext\n",
    "        global_variables.spark = self.spark\n",
    "        global_variables.schemaRegistryUrl = \"schemaRegistryAddress\"\n",
    "\n",
    "        global_variables.batchOutputPath = path\n",
    "        global_variables.kafka_grp = \"kafkaGrp\"\n",
    "        global_variables.OutputPathForMaterializedView = path\n",
    "        global_variables.bootstrapServers = \"bootstrapServers\"\n",
    "        global_variables.kafkaConsumerGroupForLag = \"kafkaConsumerGroupForLag\"\n",
    "        global_variables.startDateOfPipeline = \"1-03-2021\"\n",
    "        global_variables.timestampToStartStreaming = None\n",
    "        global_variables.test_data = None\n",
    "        global_variables.test_schema = None\n",
    "\n",
    "        if global_variables.startDateOfPipeline != \"\":\n",
    "            timestampToStartStreaming = datetime.datetime.strptime(global_variables.startDateOfPipeline, \"%d-%m-%Y\")\n",
    "            print(\"timestamp: \" + str(timestampToStartStreaming))\n",
    "            global_variables.timestampToStartStreaming = timestampToStartStreaming\n",
    "        self.test_config = {\n",
    "            \"output_format\": \"delta\",\n",
    "            \"global_variables\": global_variables,\n",
    "            \"mapOfTopicAndData\": self.toMap(path,self.folder),\n",
    "            \"output_path\": path\n",
    "        }\n",
    "        self.job = TestPipelineJob(spark=self.spark, init_conf=self.test_config)\n",
    "\n",
    "    def test_insert(self):\n",
    "        gv = self.test_config[\"global_variables\"]\n",
    "        mapOfTopicAndData = self.test_config[\"mapOfTopicAndData\"][\"jb\"]\n",
    "        gv.test_data = TESTDATA_INSERT\n",
    "        new_schema = StructType.fromJson(json.loads(JSON_SCHEMA))\n",
    "        gv.test_schema = new_schema\n",
    "        print(gv.test_data)\n",
    "        self.job.launch()\n",
    "\n",
    "        output_count = (\n",
    "            self.spark\n",
    "                .sql(\"select * from {}.jb\".format(mapOfTopicAndData['database']))\n",
    "                .count()\n",
    "        )\n",
    "        output_count2 = (\n",
    "            self.spark\n",
    "                .sql(\"select * from {}.jb where id='3001' and username='3001' \".format(mapOfTopicAndData['database']))\n",
    "                .count()\n",
    "        )\n",
    "\n",
    "        self.assertEqual(output_count, 2)\n",
    "        self.assertEqual(output_count2, 1)\n",
    "\n",
    "    def test_update(self):\n",
    "        gv = self.test_config[\"global_variables\"]\n",
    "        mapOfTopicAndData = self.test_config[\"mapOfTopicAndData\"][\"jb\"]\n",
    "        gv.test_data = TESTDATA_UPDATE\n",
    "        new_schema = StructType.fromJson(json.loads(JSON_SCHEMA))\n",
    "        gv.test_schema = new_schema\n",
    "        print(gv.test_data)\n",
    "        self.job.launch()\n",
    "\n",
    "        output_count = (\n",
    "            self.spark\n",
    "                .sql(\"select * from {}.jb\".format(mapOfTopicAndData['database']))\n",
    "                .count()\n",
    "        )\n",
    "        output_count2 = (\n",
    "            self.spark\n",
    "                .sql(\"select * from {}.jb where id='3001' and username='9999' \".format(mapOfTopicAndData['database']))\n",
    "                .count()\n",
    "        )\n",
    "\n",
    "        self.assertEqual(output_count, 1)\n",
    "        self.assertEqual(output_count2, 1)\n",
    "\n",
    "    def tearDown(self):\n",
    "        pass\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        dbutils = get_dbutils(cls.sparkCls)\n",
    "        cls.sparkCls.sql(\"drop database  {} cascade\".format(\"database_test_%s\" % cls.folder))\n",
    "        dbutils.fs.rm(\"/tmp/database_test_{0}database_test_{0}\".format(cls.folder), True)\n",
    "        dbutils.fs.rm(\"/tmp/database_test_%s\" % cls.folder, True)\n",
    "        print(\"completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba2b4dca-4286-42bf-b27d-ac4365607611",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = PipelineUnitTestCases()\n",
    "p.setUpClass()\n",
    "p.setUp()\n",
    "p.test_insert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "58c2091b-cd7e-4efa-8a6a-8eb064e9277c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# describe history dev_pipeline_test_optimize.user_summary_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2729672f-4302-414d-ac96-8c10e90bc758",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# alter table dev_pipeline_test_optimize.user_summary_old add columns (last_lat1 double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a7fc1f7a-1708-47d8-9552-e590ab9ac690",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df3.write.option(\"overwriteSchema\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0cf5eeef-59e8-46e6-aee3-c401afd3c52e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# alter table dev_pipeline_test_optimize.user_summary_old drop column last_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c4371f4c-b52f-4ad0-b58c-478310791041",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# update dev_pipeline_test_optimize.user_summary_old set last_lat1 = last_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f7c206ec-7203-4270-87e6-11e6570aedea",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# truncate table dev_pipeline_test_optimize.user_summary_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5c553d42-5848-4808-abe4-ad20dbb17696",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# select last_lat from dev_pipeline_test_optimize.user_summary_old order by created_at desc limit 1000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "320e3517-6cc3-44eb-81d3-ee2c3d1e9119",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Add new column with default\n",
    "# from pyspark.sql.functions import lit\n",
    "# from pyspark.sql.functions import col, struct, when\n",
    "# from pyspark.sql.types import StructType, StructField, IntegerType, LongType, StringType, FloatType, DecimalType,DoubleType\n",
    "\n",
    "# df = spark.sql(\"Select * from dev_pipeline_test_optimize.user_summary_old\")\n",
    "\n",
    "# df1 = df.withColumn(\"last_lat\", col(\"last_lat\").cast(DecimalType(10,0)))\n",
    "\n",
    "# df1.write.format(\"delta\").mode(\"OVERWRITE\").option(\"overwriteSchema\", \"true\").saveAsTable(\"dev_pipeline_test_optimize.user_summary_old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "62be2da7-e9a6-412a-b3b0-60f34ac7e02e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Add new column with default\n",
    "# from pyspark.sql.functions import lit\n",
    "# from pyspark.sql.functions import col, struct, when\n",
    "# from pyspark.sql.types import StructType, StructField, IntegerType, LongType, StringType, FloatType, DecimalType,DoubleType\n",
    "\n",
    "# df = spark.sql(\"Select * from dev_pipeline_test_optimize.user_summary_old\")\n",
    "\n",
    "# df1 = df.withColumn(\"last_lat\", col(\"last_lat\").cast(DoubleType()))\n",
    "\n",
    "# df1.write.format(\"delta\").mode(\"OVERWRITE\").option(\"overwriteSchema\", \"true\").saveAsTable(\"dev_pipeline_test_optimize.user_summary_old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f70ffb02-f212-4a28-b9ba-bfbba45090e8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# select * \n",
    "# from dev_pipeline_test_optimize.user_summary_old \n",
    "# where CAST(last_lat AS DECIMAL(10,0)) - last_lat = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "144d01dc-50ea-428d-b6e0-481013016289",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # Add new column with default\n",
    "# from pyspark.sql.functions import lit\n",
    "# from pyspark.sql.functions import col, struct, when\n",
    "# from pyspark.sql.types import StructType, StructField, IntegerType, LongType, StringType, FloatType, DecimalType,DoubleType\n",
    "\n",
    "# df = spark.sql(\"select * from dev_pipeline_test_optimize.user_summary_old where CAST(last_lat AS DECIMAL(10,0)) - last_lat = 0 limit 1\")\n",
    "\n",
    "# df3 = df.withColumn(\"last_lat\", col(\"last_lat\").cast(DoubleType()))\n",
    "\n",
    "# #df1.write.format(\"delta\").mode(\"OVERWRITE\").option(\"overwriteSchema\", \"true\").saveAsTable(\"dev_pipeline_test_optimize.user_summary_old\")\n",
    "\n",
    "# display(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b175fbd7-a2a8-410c-b9b1-136fa983b3d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df3.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dev_pipeline_test_optimize.user_summary_old1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4f4f1b5f-45db-44e6-844d-b53774cbfd06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "// %scala\n",
    "// def merge2(spark: SparkSession, paths: Seq[String]): DataFrame = {\n",
    "//     import spark.implicits._\n",
    "\n",
    "//     spark.sparkContext.union(paths.par.map {\n",
    "//       path =>\n",
    "//         spark.read.parquet(path).withColumn(\"myField\", $\"myField\".cast(DoubleType)).as[Double].rdd\n",
    "//     }.toList).toDF\n",
    "//   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e5e55f3f-a61e-431d-a52a-e429ad6315fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# MERGE INTO dev_pipeline_test_optimize.user_summary_old targetTable\n",
    "#               USING dev_pipeline_test_optimize.user_summary_old updatesTable\n",
    "#               ON updatesTable.city_id = targetTable.last_lat\n",
    "#               WHEN NOT MATCHED THEN INSERT *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "64012c16-a20a-4218-b3d0-7925a62dcf60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# desc dev_pipeline_test_optimize.user_summary_old1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "04381555-3a98-4e87-9446-e722edb6f24a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh\n",
    "\n",
    "curl http://172.31.32.174:8081/subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6162edac-b612-4cbe-95cf-a4061600d801",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh\n",
    "sudo apt-get install kafkacat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c743e94d-a0eb-414b-96ce-fb4dce204d5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sh\n",
    "kafkacat -b 172.31.47.188:9092 -t test-topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4ff28932-5e88-4d28-8244-90955f1e2046",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.types import StructType,StructField, StringType, IntegerType,ArrayType,MapType\n",
    "# from pyspark.sql.functions import col,struct,when\n",
    "\n",
    "# spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "#                     .appName('SparkByExamples.com') \\\n",
    "#                     .getOrCreate()\n",
    "\n",
    "# data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "#     (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "#     (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "#     (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "#     (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "#   ]\n",
    "\n",
    "# schema = StructType([ \n",
    "#     StructField(\"firstname\",StringType(),True), \n",
    "#     StructField(\"middlename\",StringType(),True), \n",
    "#     StructField(\"lastname\",StringType(),True), \n",
    "#     StructField(\"id\", StringType(), True), \n",
    "#     StructField(\"gender\", StringType(), True), \n",
    "#     StructField(\"salary\", IntegerType(), True) \n",
    "#   ])\n",
    " \n",
    "# df = spark.createDataFrame(data=data,schema=schema)\n",
    "# df.printSchema()\n",
    "# df.show(truncate=False)\n",
    "\n",
    "# structureData = [\n",
    "#     ((\"James\",\"\",\"Smith\"),\"36636\",\"M\",3100),\n",
    "#     ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",4300),\n",
    "#     ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",1400),\n",
    "#     ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",5500),\n",
    "#     ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",-1)\n",
    "#   ]\n",
    "# structureSchema = StructType([\n",
    "#         StructField('name', StructType([\n",
    "#              StructField('firstname', StringType(), True),\n",
    "#              StructField('middlename', StringType(), True),\n",
    "#              StructField('lastname', StringType(), True)\n",
    "#              ])),\n",
    "#          StructField('id', StringType(), True),\n",
    "#          StructField('gender', StringType(), True),\n",
    "#          StructField('salary', IntegerType(), True)\n",
    "#          ])\n",
    "\n",
    "# df2 = spark.createDataFrame(data=structureData,schema=structureSchema)\n",
    "# df2.printSchema()\n",
    "# df2.show(truncate=False)\n",
    "\n",
    "\n",
    "# updatedDF = df2.withColumn(\"OtherInfo\", \n",
    "#     struct(col(\"id\").alias(\"identifier\"),\n",
    "#     col(\"gender\").alias(\"gender\"),\n",
    "#     col(\"salary\").alias(\"salary\"),\n",
    "#     when(col(\"salary\").cast(IntegerType()) < 2000,\"Low\")\n",
    "#       .when(col(\"salary\").cast(IntegerType()) < 4000,\"Medium\")\n",
    "#       .otherwise(\"High\").alias(\"Salary_Grade\")\n",
    "#   )).drop(\"id\",\"gender\",\"salary\")\n",
    "\n",
    "# updatedDF.printSchema()\n",
    "# updatedDF.show(truncate=False)\n",
    "\n",
    "\n",
    "# \"\"\" Array & Map\"\"\"\n",
    "\n",
    "\n",
    "# arrayStructureSchema = StructType([\n",
    "#     StructField('name', StructType([\n",
    "#        StructField('firstname', StringType(), True),\n",
    "#        StructField('middlename', StringType(), True),\n",
    "#        StructField('lastname', StringType(), True)\n",
    "#        ])),\n",
    "#        StructField('hobbies', ArrayType(StringType()), True),\n",
    "#        StructField('properties', MapType(StringType(),StringType()), True)\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c25a664-cfbd-4d8e-9184-9eda3065ebda",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# updatedDF.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"testStruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "85742e20-f79a-4976-9724-282f46827d4d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %sql\n",
    "# desc table testStruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a2846cf9-08c3-468a-a164-24d52a27ff5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DatabricksPipeline_clean_jar",
   "notebookOrigID": 2319991568151890,
   "widgets": {
    "EMAIL_SERVICE_API_KEY": {
     "currentValue": "randomkey",
     "nuid": "6c3ad5c5-645b-42b0-be18-401cf09fd153",
     "widgetInfo": {
      "defaultValue": "randomkey",
      "label": null,
      "name": "EMAIL_SERVICE_API_KEY",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "EMAIL_SERVICE_URL": {
     "currentValue": "https://emailser.com",
     "nuid": "1d95c7b2-3055-45ed-9e6a-4bd13be529d5",
     "widgetInfo": {
      "defaultValue": "https://emailser.com",
      "label": null,
      "name": "EMAIL_SERVICE_URL",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "OutputPathForMaterializedView": {
     "currentValue": "/mnt/fareye-datalake-ca-test/processed527/",
     "nuid": "e9fb28cf-92ce-41e3-ad52-1bdbc4d11308",
     "widgetInfo": {
      "defaultValue": "/mnt/fareye-datalake-ca/processed/",
      "label": null,
      "name": "OutputPathForMaterializedView",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "SUBJECT_EMAIL": {
     "currentValue": "PIPELINE ALERT",
     "nuid": "4b6f42f3-fe85-4023-b9a4-f4409aa3875e",
     "widgetInfo": {
      "defaultValue": "PIPELINE ALERT",
      "label": null,
      "name": "SUBJECT_EMAIL",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "TO_EMAIL": {
     "currentValue": "data@getfareye.com",
     "nuid": "56b89662-19d2-4cb4-967e-19cf2d85e36a",
     "widgetInfo": {
      "defaultValue": "data@getfareye.com",
      "label": null,
      "name": "TO_EMAIL",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "batchOutputPath": {
     "currentValue": "/mnt/fareye-datalake-ca-test/raw517/",
     "nuid": "04142cee-f1fa-43b7-9bd4-c6b0a4bdab0b",
     "widgetInfo": {
      "defaultValue": "/mnt/fareye-datalake-ca/raw/",
      "label": null,
      "name": "batchOutputPath",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "bootstrapServers": {
     "currentValue": "172.31.47.188:9092,172.31.32.174:9092",
     "nuid": "db5a59c1-ee83-4ef3-8fb3-d4ced91951ec",
     "widgetInfo": {
      "defaultValue": "kafka-1.internal.ca:9092,kafka-2.internal.ca:9092,kafka-3.internal.ca:909",
      "label": null,
      "name": "bootstrapServers",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "database": {
     "currentValue": "'dev_pipeline_test_optimize'",
     "nuid": "86e2d258-2c81-45c7-99a1-202d08e4594e",
     "widgetInfo": {
      "defaultValue": "'aws_oregon_ca_prod'",
      "label": null,
      "name": "database",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "environment": {
     "currentValue": "qadev",
     "nuid": "b822770c-2a57-40d9-b0ac-831eace609ed",
     "widgetInfo": {
      "defaultValue": "prod",
      "label": null,
      "name": "environment",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "kafka-grp": {
     "currentValue": "datalake2",
     "nuid": "974ece40-93d6-4feb-ac1a-3fd106533b0a",
     "widgetInfo": {
      "defaultValue": "datalake",
      "label": null,
      "name": "kafka-grp",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "kafkaConsumerGroupForLag": {
     "currentValue": "datalake-qa-91",
     "nuid": "85ef1e14-9453-4248-a043-184ab9d922ee",
     "widgetInfo": {
      "defaultValue": "databricks-datalake",
      "label": null,
      "name": "kafkaConsumerGroupForLag",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "schemaRegistryUrl": {
     "currentValue": "http://172.31.32.174:8081",
     "nuid": "6719bb6d-cf89-4c50-b9ec-8170630de0c8",
     "widgetInfo": {
      "defaultValue": "http://schema-registry.internal.ca:8081",
      "label": null,
      "name": "schemaRegistryUrl",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "startDateOfPipeline": {
     "currentValue": "05-11-2021",
     "nuid": "476c0e42-6614-46d6-b199-5a63fbf4fe43",
     "widgetInfo": {
      "defaultValue": "1-03-2021",
      "label": null,
      "name": "startDateOfPipeline",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
