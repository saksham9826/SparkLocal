{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baff3b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/usr/local/lib/python3.9/dist-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-sql_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      "org.apache.spark#spark-avro_2.12 added as a dependency\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.spark#spark-streaming-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.spark#spark-streaming_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-71aa47a0-faf5-4e71-9c36-c51be87ea1d3;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.974 in central\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
      "\tfound com.github.luben#zstd-jni;1.4.9-1 in central\n",
      "\tfound org.lz4#lz4-java;1.7.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
      "\tfound org.apache.spark#spark-avro_2.12;3.0.0 in central\n",
      "\tfound io.delta#delta-core_2.12;0.8.0 in central\n",
      "\tfound org.antlr#antlr4;4.7 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.7 in central\n",
      "\tfound org.antlr#antlr-runtime;3.5.2 in central\n",
      "\tfound org.antlr#ST4;4.0.8 in central\n",
      "\tfound org.abego.treelayout#org.abego.treelayout.core;1.0.3 in central\n",
      "\tfound org.glassfish#javax.json;1.0.4 in central\n",
      "\tfound com.ibm.icu#icu4j;58.2 in central\n",
      "\tfound org.apache.spark#spark-streaming-kafka-0-10_2.12;3.0.0 in central\n",
      ":: resolution report :: resolve 804ms :: artifacts dl 14ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.974 from central in [default]\n",
      "\tcom.github.luben#zstd-jni;1.4.9-1 from central in [default]\n",
      "\tcom.ibm.icu#icu4j;58.2 from central in [default]\n",
      "\tio.delta#delta-core_2.12;0.8.0 from central in [default]\n",
      "\torg.abego.treelayout#org.abego.treelayout.core;1.0.3 from central in [default]\n",
      "\torg.antlr#ST4;4.0.8 from central in [default]\n",
      "\torg.antlr#antlr-runtime;3.5.2 from central in [default]\n",
      "\torg.antlr#antlr4;4.7 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.7 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
      "\torg.apache.spark#spark-avro_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-streaming-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.0.0 from central in [default]\n",
      "\torg.glassfish#javax.json;1.0.4 from central in [default]\n",
      "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;2.4.1 by [org.apache.kafka#kafka-clients;2.8.0] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   21  |   0   |   0   |   1   ||   20  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-71aa47a0-faf5-4e71-9c36-c51be87ea1d3\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 20 already retrieved (0kB/13ms)\n",
      "22/03/09 04:31:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/09 04:31:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.\\\n",
    "#         builder.\\\n",
    "#         appName(\"pyspark-notebook\").\\\n",
    "#         master(\"spark://spark-master:7077\").\\\n",
    "#         config(\"spark.executor.memory\", \"512m\").\\\n",
    "#         getOrCreate()\n",
    "\n",
    "\n",
    "# org.apache.spark:spark-avro:3.0.0\n",
    "\n",
    "# SparkConf conf = new SparkConf()\n",
    "#             .setAppName(\"Test spark\")\n",
    "#             .setMaster(\"spark://ip of your master node:port of your master node\")\n",
    "#             .set(\"spark.blockManager.port\", \"10025\")\n",
    "#             .set(\"spark.driver.blockManager.port\", \"10026\")\n",
    "#             .set(\"spark.driver.port\", \"10027\") //make all communication ports static (not necessary if you disabled firewalls, or if your nodes located in local network, otherwise you must open this ports in firewall settings)\n",
    "#             .set(\"spark.cores.max\", \"12\") \n",
    "#             .set(\"spark.executor.memory\", \"2g\")\n",
    "#             .set(\"spark.driver.host\", \"ip of your driver (PC)\");\n",
    "        \n",
    "    \n",
    "import os\n",
    "\n",
    "# setup arguments\n",
    "os.environ['kafka1'] = '34.220.52.64'\n",
    "os.environ['kafka2'] = '52.35.125.165'\n",
    "os.environ['schema-registry'] = '52.35.125.165'\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = \"/opt/workspace/env3/bin/python3.9\"\n",
    "\n",
    "spark = SparkSession.builder.appName('pyspark-notebook')\\\n",
    "    .master(\"spark://spark-master:7077\")\\\n",
    "    .config(\"spark.jars\", \"scalaSpark-assembly-1.0.jar\") \\\n",
    "    .config('spark.jars.packages', 'com.amazonaws:aws-java-sdk-bundle:1.11.974,org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-sql_2.12:3.0.0,org.apache.kafka:kafka-clients:2.8.0,org.apache.spark:spark-avro_2.12:3.0.0,io.delta:delta-core_2.12:0.8.0,org.apache.spark:spark-streaming-kafka-0-10_2.12:3.0.0,org.apache.spark:spark-streaming_2.12:3.0.0')\\\n",
    "    .config(\"spark.executor.memory\", \"512m\")\\\n",
    "    .config(\"spark.cores.max\", \"12\")\\\n",
    "    .config(\"spark.driver.port\", \"4040\")\\\n",
    "    .config(\"spark.archives\",\"venv1.zip\")\\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c93ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datalake-qa-91"
     ]
    }
   ],
   "source": [
    "# df4 = spark.sparkContext._jvm.com.example.Hello.add\n",
    "\n",
    "# print(spark._jsparkSession)\n",
    "df5 = spark.sparkContext._jvm.com.example.Hello.add(spark._jsparkSession, \"34.220.52.64:9092,52.35.125.165:9092\", \"datalake-qa-91\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "295a0905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datalake-qa-91"
     ]
    }
   ],
   "source": [
    "df5 = spark.sparkContext._jvm.com.example.Hello.add(spark._jsparkSession, \"34.220.52.64:9092,52.35.125.165:9092\", \"datalake-qa-91\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c44a0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets==6.0.0\n",
      "  Downloading ipywidgets-6.0.0-py2.py3-none-any.whl (46 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 KB\u001b[0m \u001b[31m383.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting widgetsnbextension~=2.0.0\n",
      "  Downloading widgetsnbextension-2.0.1-py2.py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets==6.0.0) (8.1.0)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets==6.0.0) (5.1.3)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets==6.0.0) (5.1.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.9/dist-packages (from ipywidgets==6.0.0) (6.9.1)\n",
      "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets==6.0.0) (1.5.1)\n",
      "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets==6.0.0) (0.1.3)\n",
      "Requirement already satisfied: tornado<7.0,>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets==6.0.0) (6.1)\n",
      "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets==6.0.0) (1.5.4)\n",
      "Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel>=4.5.1->ipywidgets==6.0.0) (7.1.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets==6.0.0) (0.7.5)\n",
      "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets==6.0.0) (0.18.1)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets==6.0.0) (5.1.1)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets==6.0.0) (2.11.2)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/lib/python3/dist-packages (from ipython>=4.0.0->ipywidgets==6.0.0) (52.0.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets==6.0.0) (0.2.0)\n",
      "Requirement already satisfied: stack-data in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets==6.0.0) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets==6.0.0) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=4.0.0->ipywidgets==6.0.0) (3.0.28)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.9/dist-packages (from nbformat>=4.2.0->ipywidgets==6.0.0) (4.9.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.9/dist-packages (from nbformat>=4.2.0->ipywidgets==6.0.0) (4.4.0)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.9/dist-packages (from nbformat>=4.2.0->ipywidgets==6.0.0) (0.2.0)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.9/dist-packages (from widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (6.4.8)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets==6.0.0) (0.8.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==6.0.0) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets==6.0.0) (0.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.9/dist-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets==6.0.0) (2.8.2)\n",
      "Requirement already satisfied: entrypoints in /usr/local/lib/python3.9/dist-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets==6.0.0) (0.4)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.9/dist-packages (from jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets==6.0.0) (22.3.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (0.13.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (1.8.0)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (6.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (3.0.3)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (21.3.0)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (0.13.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets==6.0.0) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets==6.0.0) (0.2.5)\n",
      "Requirement already satisfied: asttokens in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=4.0.0->ipywidgets==6.0.0) (2.0.5)\n",
      "Requirement already satisfied: executing in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=4.0.0->ipywidgets==6.0.0) (0.8.3)\n",
      "Requirement already satisfied: pure-eval in /usr/local/lib/python3.9/dist-packages (from stack-data->ipython>=4.0.0->ipywidgets==6.0.0) (0.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.1->jupyter-client<8.0->ipykernel>=4.5.1->ipywidgets==6.0.0) (1.16.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (21.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (2.1.0)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (0.8.4)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (4.1.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (1.5.0)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (0.5.11)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (0.1.2)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (0.6.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (0.7.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (1.15.0)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (0.5.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (21.3)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (2.21)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging->bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=2.0.0->ipywidgets==6.0.0) (3.0.7)\n",
      "Installing collected packages: widgetsnbextension, ipywidgets\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 3.5.2\n",
      "    Uninstalling widgetsnbextension-3.5.2:\n",
      "      Successfully uninstalled widgetsnbextension-3.5.2\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 7.6.5\n",
      "    Uninstalling ipywidgets-7.6.5:\n",
      "      Successfully uninstalled ipywidgets-7.6.5\n",
      "Successfully installed ipywidgets-6.0.0 widgetsnbextension-2.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets==6.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ef8096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ead76b63b84b7e8527101c3374c93a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525d7db3740441a5a959e59d62193ddb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widget\n",
    "l = widget.Label('consumerGroup')\n",
    "display(l)\n",
    "consumerGroup = widget.Text()\n",
    "display(consumerGroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9ad2ff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consumerGroup.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a486b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/09 04:31:18 WARN KafkaSourceProvider: maxOffsetsPerTrigger option ignored in batch queries\n"
     ]
    }
   ],
   "source": [
    "kafka = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"34.220.52.64:9092,52.35.125.165:9092\") \\\n",
    "            .option(\"subscribe\", \"qatest12nonfes.public.users\") \\\n",
    "            .option(\"startingOffsets\", \"earliest\") \\\n",
    "            .option(\"maxOffsetsPerTrigger\", 1) \\\n",
    "            .option(\"failOnDataLoss\", False) \\\n",
    "            .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "92db64b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka = spark.read \\\n",
    "#             .format(\"kafka\") \\\n",
    "#             .option(\"kafka.bootstrap.servers\", \"kafka1:9092,kafka2:9092,kafka6:9092,kafka7:9092,kafka3:9092,kafka4:9092,kafka5:9092\") \\\n",
    "#             .option(\"subscribe\", \"staging.public.runsheet_old\") \\\n",
    "#             .option(\"startingOffsets\", \"\"\"{\"staging.public.runsheet_old\":{\"0\": 515796,\"1\": 592117,\"2\": 554967,\"3\": 599241,\"4\": 593548,\"5\": 489667,\"6\": 606915,\"7\": 537525,\"8\": 591024,\"9\": 496304}}\"\"\") \\\n",
    "#             .option(\"maxOffsetsPerTrigger\", 1) \\\n",
    "#             .option(\"failOnDataLoss\", False) \\\n",
    "#             .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38fb66f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123ec0fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "kafka.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df66e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-schema-registry-client in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
      "Requirement already satisfied: aiofiles>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from python-schema-registry-client) (0.8.0)\n",
      "Requirement already satisfied: httpx>=0.19.0 in /usr/local/lib/python3.9/dist-packages (from python-schema-registry-client) (0.22.0)\n",
      "Requirement already satisfied: fastavro>=1.4.4 in /usr/local/lib/python3.9/dist-packages (from python-schema-registry-client) (1.4.9)\n",
      "Requirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from python-schema-registry-client) (4.4.0)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.9/dist-packages (from httpx>=0.19.0->python-schema-registry-client) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.9/dist-packages (from httpx>=0.19.0->python-schema-registry-client) (2.0.12)\n",
      "Requirement already satisfied: httpcore<0.15.0,>=0.14.5 in /usr/local/lib/python3.9/dist-packages (from httpx>=0.19.0->python-schema-registry-client) (0.14.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx>=0.19.0->python-schema-registry-client) (1.2.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx>=0.19.0->python-schema-registry-client) (2021.10.8)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.2.0->python-schema-registry-client) (21.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.2.0->python-schema-registry-client) (0.18.1)\n",
      "Requirement already satisfied: anyio==3.* in /usr/local/lib/python3.9/dist-packages (from httpcore<0.15.0,>=0.14.5->httpx>=0.19.0->python-schema-registry-client) (3.5.0)\n",
      "Requirement already satisfied: h11<0.13,>=0.11 in /usr/local/lib/python3.9/dist-packages (from httpcore<0.15.0,>=0.14.5->httpx>=0.19.0->python-schema-registry-client) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.9/dist-packages (from anyio==3.*->httpcore<0.15.0,>=0.14.5->httpx>=0.19.0->python-schema-registry-client) (3.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-schema-registry-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5082f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.avro.functions import from_avro\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "from schema_registry.client import SchemaRegistryClient, schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55736e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def getJsonSchema(schemaId, schemasById, registrySchemaUrl):\n",
    "        jsonSchema = schemasById.get(schemaId)\n",
    "        if jsonSchema is None:\n",
    "            src = SchemaRegistryClient(registrySchemaUrl)\n",
    "            schema = src.get_by_id(schemaId)\n",
    "            jsonSchema = json.dumps(schema.flat_schema)\n",
    "            schemasById[schemaId] = jsonSchema\n",
    "        return jsonSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d903f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    " \n",
    "# initialize list of lists\n",
    "data = [['tom', 10], ['nick', 15], ['juli', 14]]\n",
    " \n",
    "# Create the pandas DataFrame\n",
    "df = pd.DataFrame(data, columns = ['Name', 'Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30a13718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pandas\n",
      "Version: 1.4.1\n",
      "Summary: Powerful data structures for data analysis, time series, and statistics\n",
      "Home-page: https://pandas.pydata.org\n",
      "Author: The Pandas Development Team\n",
      "Author-email: pandas-dev@python.org\n",
      "License: BSD-3-Clause\n",
      "Location: /usr/local/lib/python3.9/dist-packages\n",
      "Requires: numpy, python-dateutil, pytz\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d185379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    \n",
    "def avroJsonSchema(schemaId, payload):\n",
    "#         import subprocess\n",
    "#         install(\"schema_registry\")\n",
    "#         install(\"python-schema-registry-client\")\n",
    "#         install(\"pandas\")\n",
    "        from schema_registry.client import SchemaRegistryClient, schema\n",
    "        import json\n",
    "        \n",
    "        src = SchemaRegistryClient(\"http://52.35.125.165:8081\")\n",
    "        schema = src.get_by_id(schemaId)\n",
    "        jsonSchema = json.dumps(schema.flat_schema)\n",
    "        return json.dumps(from_avro(payload, jsonSchema))\n",
    "#         return jsonSchema\n",
    "    \n",
    "# schema = T.StructType([T.StructField(\"companyId\",T.StringType(),False)])\n",
    "\n",
    "getJsonSchema_udf = udf(avroJsonSchema, StringType())\n",
    "\n",
    "def getJsonSchemaLatest(registrySchemaUrl, topicName):\n",
    "        src = SchemaRegistryClient(registrySchemaUrl)\n",
    "#         vl = get_versions(topicName)\n",
    "#         print(\"versions are\")\n",
    "#         print(vl[0])\n",
    "#         print(vl[-1])\n",
    "        \n",
    "#         schemaId = vl[-1]\n",
    "        schema = src.get_schema(subject=topicName).schema\n",
    "                      \n",
    "#         schema = src.get_by_id(schemaId)\n",
    "        jsonSchema = json.dumps(schema.flat_schema)\n",
    "        return jsonSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9af3dc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97a3791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkFiles\n",
    "spark.sparkContext.addPyFile(\"/usr/local/lib/python3.9/dist-packages/pandas-1.4.1.dist-info/WHEEL\")\n",
    "\n",
    "sys.path.append('/tmp/modules/lib')\n",
    "\n",
    "sys.path.append(SparkFiles.getRootDirectory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e3b5c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "kafka = kafka.filter(\"value is NOT NULL\")\n",
    "\n",
    "sd = datetime.datetime.strptime(\"03-03-2022\", \"%d-%m-%Y\")\n",
    "\n",
    "kafka = kafka.filter(kafka['timestamp'] >= sd)\n",
    "    \n",
    "byteArrayToLong = fn.udf(lambda x: int.from_bytes(x, byteorder='big', signed=False), LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93031b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "059de7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = kafka.withColumn('valuenew', kafka['value'])\n",
    "df = df.withColumn(\"schemaId\", byteArrayToLong(fn.substring(\"valuenew\", 2, 4))) \\\n",
    "                    .withColumn(\"payload\", fn.expr(\"substring(valuenew, 6, length(valuenew)-5)\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52f7ab50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"newCol\",getJsonSchema_udf(df.schemaId, df.payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "195926a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+---------+------+--------------------+-------------+--------------------+--------+\n",
      "|                 key|               value|               topic|partition|offset|           timestamp|timestampType|            valuenew|schemaId|\n",
      "+--------------------+--------------------+--------------------+---------+------+--------------------+-------------+--------------------+--------+\n",
      "|[00 00 00 07 34 E...|[, [53936, bha07_...|qatest12nonfes.pu...|        8| 44745|2022-03-03 10:25:...|            0|[00 00 00 0A 8C 0...|    2700|\n",
      "+--------------------+--------------------+--------------------+---------+------+--------------------+-------------+--------------------+--------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8893340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      " |-- valuenew: binary (nullable = true)\n",
      " |-- schemaId: long (nullable = true)\n",
      " |-- payload: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1c72e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.withColumn(\"value\", from_avro(\"payload\", \"newCol\"))\n",
    "\n",
    "dfSchema = getJsonSchemaLatest(\"http://52.35.125.165:8081\", \"qatest12nonfes.public.users-value\")\n",
    "df = df.withColumn(\"payload\", fn.expr(\"substring(valuenew, 6, length(valuenew)-5)\"))\n",
    "df = df.withColumn('value', from_avro(\"payload\", dfSchema))\n",
    "df = df.drop(\"payload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "10101ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"connect.name\": \"qatest12nonfes.public.users.Envelope\", \"type\": \"record\", \"name\": \"qatest12nonfes.public.users.Envelope\", \"fields\": [{\"default\": null, \"name\": \"before\", \"type\": [\"null\", {\"connect.name\": \"qatest12nonfes.public.users.Value\", \"type\": \"record\", \"name\": \"qatest12nonfes.public.users.Value\", \"fields\": [{\"name\": \"id\", \"type\": \"int\"}, {\"default\": null, \"name\": \"login\", \"type\": [\"null\", \"string\"]}, {\"default\": null, \"name\": \"password\", \"type\": [\"null\", \"string\"]}, {\"default\": null, \"name\": \"first_name\", \"type\": [\"null\", \"string\"]}, {\"default\": null, \"name\": \"last_name\", \"type\": [\"null\", \"string\"]}, {\"default\": null, \"name\": \"email\", \"type\": [\"null\", \"string\"]}, {\"default\": null, \"name\": \"activated\", \"type\": [\"null\", \"boolean\"]}, {\"default\": null, \"name\": \"lang_key\", \"type\": [\"null\", \"string\"]}, {\"default\": null, \"name\": \"activation_key\", \"type\": [\"null\", \"string\"]}, {\"name\": \"created_by\", \"type\": \"string\"}, {\"name\": \"created_date\", \"type\": {\"connect.version\": 1, \"connect.name\": \"org.apache.kafka.connect.data.Timestamp\", \"logicalType\": \"timestamp-millis\", \"type\": \"long\"}}, {\"default\": null, \"name\": \"last_modified_by\", \"type\": [\"null\", \"string\"]}, {\"default\": null, \"name\": \"last_modified_date\", \"type\": [\"null\", {\"connect.version\": 1, \"connect.name\": \"org.apache.kafka.connect.data.Timestamp\", \"logicalType\": \"timestamp-millis\", \"type\": \"long\"}]}, {\"default\": null, \"name\": \"emp_code\", \"type\": [\"null\", \"string\"]}, {\"default\": null, \"name\": \"mobile\", \"type\": [\"null\", \"string\"]}, {\"default\": null, \"name\": \"company_id\", \"type\": [\"null\", \"long\"]}, {\"default\": null, \"name\": \"user_type_id\", \"type\": [\"null\", \"long\"]}, {\"default\": null, \"name\": \"salt\", \"type\": [\"null\", \"string\"]}, {\"default\": null, \"name\": \"city_id\", \"type\": [\"null\", \"long\"]}, {\"default\": null, \"name\": \"hub_id\", \"type\": [\"null\", \"long\"]}, {\"default\": null, \"name\": \"last_login_time\", \"type\": [\"null\", {\"connect.version\": 1, \"connect.name\": \"org.apache.kafka.connect.data.Timestamp\", \"logicalType\": \"timestamp-millis\", \"type\": \"long\"}]}, {\"default\": null, \"name\": \"merchants\", \"type\": [\"null\", \"string\"]}, {\"default\": null, \"name\": \"is_auto_push_mail_activated\", \"type\": [\"null\", \"boolean\"]}, {\"default\": null, \"name\": \"wrong_attempts\", \"type\": [\"null\", \"int\"]}, {\"default\": null, \"name\": \"locked\", \"type\": [\"null\", \"boolean\"]}, {\"default\": null, \"name\": \"locked_date_time\", \"type\": [\"null\", {\"connect.version\": 1, \"connect.name\": \"org.apache.kafka.connect.data.Timestamp\", \"logicalType\": \"timestamp-millis\", \"type\": \"long\"}]}, {\"default\": null, \"name\": \"is_logged_in\", \"type\": [\"null\", \"boolean\"]}, {\"default\": null, \"name\": \"service_provider_code\", \"type\": [\"null\", \"string\"]}, {\"default\": null, \"name\": \"profile_image\", \"type\": [\"null\", \"string\"]}, {\"default\": null, \"name\": \"icdr\", \"type\": [\"null\", \"boolean\"]}, {\"default\": null, \"name\": \"routing_json\", \"type\": [\"null\", \"string\"]}, {\"default\": null, \"name\": \"auth_user_id\", \"type\": [\"null\", \"int\"]}, {\"default\": null, \"name\": \"black_listed\", \"type\": [\"null\", \"boolean\"]}, {\"default\": null, \"name\": \"release_time\", \"type\": [\"null\", {\"connect.version\": 1, \"connect.name\": \"org.apache.kafka.connect.data.Timestamp\", \"logicalType\": \"timestamp-millis\", \"type\": \"long\"}]}, {\"default\": null, \"name\": \"user_id\", \"type\": [\"null\", \"string\"]}]}]}, {\"default\": null, \"name\": \"after\", \"type\": [\"null\", \"qatest12nonfes.public.users.Value\"]}, {\"name\": \"source\", \"type\": {\"connect.name\": \"io.debezium.connector.postgresql.Source\", \"type\": \"record\", \"name\": \"io.debezium.connector.postgresql.Source\", \"fields\": [{\"name\": \"version\", \"type\": \"string\"}, {\"name\": \"connector\", \"type\": \"string\"}, {\"name\": \"name\", \"type\": \"string\"}, {\"name\": \"ts_ms\", \"type\": \"long\"}, {\"default\": \"false\", \"name\": \"snapshot\", \"type\": [{\"connect.version\": 1, \"connect.parameters\": {\"allowed\": \"true,last,false\"}, \"connect.default\": \"false\", \"connect.name\": \"io.debezium.data.Enum\", \"type\": \"string\"}, \"null\"]}, {\"name\": \"db\", \"type\": \"string\"}, {\"default\": null, \"name\": \"sequence\", \"type\": [\"null\", \"string\"]}, {\"name\": \"schema\", \"type\": \"string\"}, {\"name\": \"table\", \"type\": \"string\"}, {\"default\": null, \"name\": \"txId\", \"type\": [\"null\", \"long\"]}, {\"default\": null, \"name\": \"lsn\", \"type\": [\"null\", \"long\"]}, {\"default\": null, \"name\": \"xmin\", \"type\": [\"null\", \"long\"]}]}}, {\"name\": \"op\", \"type\": \"string\"}, {\"default\": null, \"name\": \"ts_ms\", \"type\": [\"null\", \"long\"]}, {\"default\": null, \"name\": \"transaction\", \"type\": [\"null\", {\"type\": \"record\", \"name\": \"io.confluent.connect.avro.ConnectDefault\", \"fields\": [{\"name\": \"id\", \"type\": \"string\"}, {\"name\": \"total_order\", \"type\": \"long\"}, {\"name\": \"data_collection_order\", \"type\": \"long\"}]}]}]}'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec995f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "schemasByIdDict = {}\n",
    "\n",
    "dfAllSchemas = df.select(\"schemaId\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74833d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:58)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(schemaId=2700)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/08 05:25:49 ERROR TaskSchedulerImpl: Lost executor 1 on 172.22.0.4: worker lost\n",
      "22/03/08 05:25:49 ERROR TaskSchedulerImpl: Lost executor 0 on 172.22.0.5: worker lost\n",
      "22/03/08 07:17:00 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "22/03/08 07:17:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:716)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:152)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:258)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:168)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:203)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/03/08 07:17:00 ERROR TaskSchedulerImpl: Lost executor 3 on 172.22.0.4: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n",
      "22/03/08 07:17:00 ERROR TaskSchedulerImpl: Lost executor 2 on 172.22.0.5: Remote RPC client disassociated. Likely due to containers exceeding thresholds, or network issues. Check driver logs for WARN messages.\n"
     ]
    }
   ],
   "source": [
    "schemaRowList = dfAllSchemas.collect()\n",
    "print(schemaRowList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "09258a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(schemaId=3737), Row(schemaId=3461)]\n"
     ]
    }
   ],
   "source": [
    "print(schemaRowList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af8f7bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------------------+--------+--------------------+--------------------+\n",
      "|                 key|               topic|partition|offset|           timestamp|timestampType|            valuenew|schemaId|             payload|               value|\n",
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------------------+--------+--------------------+--------------------+\n",
      "|[00 00 00 07 32 B...|qatest12nonfes.pu...|        1|195410|2022-03-03 10:26:...|            0|[00 00 00 0E 99 0...|    3737|[00 02 BC A3 2C B...|[, [362718, 53657...|\n",
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------------------+--------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dfBySchemaId.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1da7bd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------------------+--------+--------------------+--------------------+\n",
      "|                 key|               topic|partition|offset|           timestamp|timestampType|            valuenew|schemaId|             payload|               value|\n",
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------------------+--------+--------------------+--------------------+\n",
      "|[00 00 00 07 32 B...|qatest12nonfes.pu...|        1|195410|2022-03-03 10:26:...|            0|[00 00 00 0E 99 0...|    3737|[00 02 BC A3 2C B...|[, [362718, 53657...|\n",
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------------------+--------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# getJsonSchema(3461, schemasByIdDict,\"http://52.35.125.165:8081\")\n",
    "dfBySchemaId.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc640dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonSchema = getJsonSchema(3737, schemasByIdDict,\"http://52.35.125.165:8081\")\n",
    "currentValueSchemaId = 3737\n",
    "currentValueSchema = jsonSchema\n",
    "dfBySchemaId = df.where(df.schemaId == currentValueSchemaId)\n",
    "dfBySchemaId = dfBySchemaId.drop(\"value\")\n",
    "\n",
    "\n",
    "dfBySchemaId = dfBySchemaId.withColumn(\"value\", from_avro(\"payload\", currentValueSchema))\n",
    "# dfBySchemaId = dfBySchemaId.drop(\"payload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4af3b9e7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'schemaRowList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m schemaRow \u001b[38;5;129;01min\u001b[39;00m \u001b[43mschemaRowList\u001b[49m:\n\u001b[1;32m      2\u001b[0m     jsonSchema \u001b[38;5;241m=\u001b[39m getJsonSchema(schemaRow\u001b[38;5;241m.\u001b[39mschemaId, schemasByIdDict,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://52.35.125.165:8081\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m     currentValueSchemaId \u001b[38;5;241m=\u001b[39m schemaRow\u001b[38;5;241m.\u001b[39mschemaId\n",
      "\u001b[0;31mNameError\u001b[0m: name 'schemaRowList' is not defined"
     ]
    }
   ],
   "source": [
    "for schemaRow in schemaRowList:\n",
    "    jsonSchema = getJsonSchema(schemaRow.schemaId, schemasByIdDict,\"http://52.35.125.165:8081\")\n",
    "    currentValueSchemaId = schemaRow.schemaId\n",
    "    currentValueSchema = jsonSchema\n",
    "    dfBySchemaId = df.where(df.schemaId == currentValueSchemaId)\n",
    "    dfBySchemaId = dfBySchemaId.drop(\"value\")\n",
    "    dfBySchemaId = dfBySchemaId.withColumn(\"value\", from_avro(\"payload\", currentValueSchema))\n",
    "    dfBySchemaId = dfBySchemaId.drop(\"payload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6f8c56d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[key: binary, topic: string, partition: int, offset: bigint, timestamp: timestamp, timestampType: int, valuenew: binary, schemaId: bigint, value: struct<before:struct<id:bigint,user_id:int,hub_id:int,city_id:int,company_id:int,date:timestamp,app_version:string,gps_kms:double,odometer_kms:double,halt_duration:double,travel_duration:double,avg_speed:double,max_speed:double,official_sms_sent_count:int,personal_sms_sent_count:int,official_call_incoming_count:int,official_call_outgoing_count:int,official_call_incoming_duration:int,official_call_outgoing_duration:int,personal_call_incoming_count:int,personal_call_outgoing_count:int,personal_call_incoming_duration:int,personal_call_outgoing_duration:int,cug_call_incoming_count:int,cug_call_outgoing_count:int,cug_call_incoming_duration:int,cug_call_outgoing_duration:int,last_location_datetime:timestamp,last_lat:double,last_lng:double,last_speed:double,last_battery:int,pending_count:int,deliver_count:int,fail_count:int,cash_collected:double,cash_payment:double,cash_collected_by_card:double,created_at:timestamp,last_cash_collected:double,last_order_number:string,last_order_time:string,last_logout_time:timestamp,first_lat:double,first_long:double,active_time_in_millis:bigint,imei_number:string,next_job_transaction_id:bigint,active_mdm_version:string,trip_status:string,box_temperature:string,sensor_id:string,last_login_time:timestamp>,after:struct<id:bigint,user_id:int,hub_id:int,city_id:int,company_id:int,date:timestamp,app_version:string,gps_kms:double,odometer_kms:double,halt_duration:double,travel_duration:double,avg_speed:double,max_speed:double,official_sms_sent_count:int,personal_sms_sent_count:int,official_call_incoming_count:int,official_call_outgoing_count:int,official_call_incoming_duration:int,official_call_outgoing_duration:int,personal_call_incoming_count:int,personal_call_outgoing_count:int,personal_call_incoming_duration:int,personal_call_outgoing_duration:int,cug_call_incoming_count:int,cug_call_outgoing_count:int,cug_call_incoming_duration:int,cug_call_outgoing_duration:int,last_location_datetime:timestamp,last_lat:double,last_lng:double,last_speed:double,last_battery:int,pending_count:int,deliver_count:int,fail_count:int,cash_collected:double,cash_payment:double,cash_collected_by_card:double,created_at:timestamp,last_cash_collected:double,last_order_number:string,last_order_time:string,last_logout_time:timestamp,first_lat:double,first_long:double,active_time_in_millis:bigint,imei_number:string,next_job_transaction_id:bigint,active_mdm_version:string,trip_status:string,box_temperature:string,sensor_id:string,last_login_time:timestamp>,source:struct<version:string,connector:string,name:string,ts_ms:bigint,snapshot:string,db:string,sequence:string,schema:string,table:string,txId:bigint,lsn:bigint,xmin:bigint>,op:string,ts_ms:bigint,transaction:struct<id:string,total_order:bigint,data_collection_order:bigint>>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dfBySchemaId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b9836bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-schema-registry-client in /usr/local/lib/python3.9/dist-packages (2.2.2)\n",
      "Requirement already satisfied: httpx>=0.19.0 in /usr/local/lib/python3.9/dist-packages (from python-schema-registry-client) (0.22.0)\n",
      "Requirement already satisfied: aiofiles>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from python-schema-registry-client) (0.8.0)\n",
      "Requirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from python-schema-registry-client) (4.4.0)\n",
      "Requirement already satisfied: fastavro>=1.4.4 in /usr/local/lib/python3.9/dist-packages (from python-schema-registry-client) (1.4.9)\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /usr/local/lib/python3.9/dist-packages (from httpx>=0.19.0->python-schema-registry-client) (1.5.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from httpx>=0.19.0->python-schema-registry-client) (2021.10.8)\n",
      "Requirement already satisfied: httpcore<0.15.0,>=0.14.5 in /usr/local/lib/python3.9/dist-packages (from httpx>=0.19.0->python-schema-registry-client) (0.14.7)\n",
      "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.9/dist-packages (from httpx>=0.19.0->python-schema-registry-client) (2.0.12)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.9/dist-packages (from httpx>=0.19.0->python-schema-registry-client) (1.2.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.2.0->python-schema-registry-client) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.2.0->python-schema-registry-client) (21.4.0)\n",
      "Requirement already satisfied: anyio==3.* in /usr/local/lib/python3.9/dist-packages (from httpcore<0.15.0,>=0.14.5->httpx>=0.19.0->python-schema-registry-client) (3.5.0)\n",
      "Requirement already satisfied: h11<0.13,>=0.11 in /usr/local/lib/python3.9/dist-packages (from httpcore<0.15.0,>=0.14.5->httpx>=0.19.0->python-schema-registry-client) (0.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.9/dist-packages (from anyio==3.*->httpcore<0.15.0,>=0.14.5->httpx>=0.19.0->python-schema-registry-client) (3.3)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install python-schema-registry-client\n",
    "\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import to_date\n",
    "import base64\n",
    "import gzip\n",
    "import _datetime\n",
    "from schema_registry.client import SchemaRegistryClient\n",
    "import json\n",
    "import sys\n",
    "import datetime\n",
    "from pyspark.sql.functions import max as sparkMax\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# import org.apache.spark.sql.streaming.ProcessingTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48c131b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'updatesTable.id = targetTable.id'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_partition_query(partition_column: str, primary_key: str):\n",
    "\n",
    "        if not primary_key or not partition_column:\n",
    "            return \"updatesTable.id = targetTable.id\"\n",
    "        \n",
    "#         query = \"updatesTable.{0} = targetTable.{0}\".format(primary_key)\n",
    "        \n",
    "        query = \"\"\n",
    "        \n",
    "        primary_keyA = []\n",
    "    \n",
    "        pkey_array = primary_key.split(\",\")\n",
    "        for partitions in pkey_array:\n",
    "            partitions = partitions.strip()\n",
    "            if query == \"\":\n",
    "              query += \"updatesTable.{0} = targetTable.{0}\".format(partitions)\n",
    "            else:\n",
    "              query += \" and updatesTable.{0} = targetTable.{0}\".format(partitions)\n",
    "              \n",
    "            primary_keyA.append(partitions)\n",
    "        \n",
    "#         primary_key = primary_key.strip()\n",
    "        partitions_array = partition_column.split(\",\")\n",
    "        for partitions in partitions_array:\n",
    "            partitions = partitions.strip()\n",
    "            if partitions not in primary_keyA:\n",
    "                query += \" and updatesTable.{0} = targetTable.{0}\".format(partitions)\n",
    "        return query\n",
    "\n",
    "    \n",
    "get_partition_query(\"\", \"company_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e412bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, StringType, FloatType\n",
    "from delta.tables import *\n",
    "from pyspark.sql import DataFrame\n",
    "  \n",
    "def getTableColumn(my_schema):\n",
    "    tableList = []\n",
    "    col = []\n",
    "    inputTable = \"updatesTable.\"\n",
    "    targetTable = \"targetTable.\"\n",
    "\n",
    "    ignoreColumns = [\"table_key_ts\", \"table_delete\", \"table_key\"]\n",
    "\n",
    "    for field in my_schema.fields:\n",
    "        if field.name == \"after\":\n",
    "            for f3 in field.dataType:\n",
    "                te = f3.name + \" \" + f3.dataType.typeName()\n",
    "                if f3.name not in ignoreColumns:\n",
    "                    tableList.append(te)\n",
    "                    col.append(f3.name)\n",
    "        else:\n",
    "            te = field.name + \" \" + field.dataType.typeName()\n",
    "            if field.name not in ignoreColumns:\n",
    "                tableList.append(te)\n",
    "                col.append(field.name)\n",
    "    res = \"\"\n",
    "    insertResInp = \"\"\n",
    "    insertResOut = \"\"\n",
    "    merge = \"\"\n",
    "    for i in range(len(tableList)):\n",
    "        if i == (len(tableList) - 1):\n",
    "            res += tableList[i]\n",
    "            insertResInp += col[i]\n",
    "            insertResOut += inputTable + col[i]\n",
    "            merge += targetTable + col[i] + \" = \" + inputTable + col[i]\n",
    "        else:\n",
    "            res += tableList[i] + \",\"\n",
    "            insertResInp += col[i] + \",\"\n",
    "            insertResOut += inputTable + col[i] + \",\"\n",
    "            merge += targetTable + col[i] + \" = \" + inputTable + col[i] + \",\"\n",
    "    ans = [res, insertResInp, insertResOut, merge]\n",
    "    \n",
    "    \n",
    "    print(ans)\n",
    "    return ans\n",
    "\n",
    "def groupk(partition):\n",
    "      from itertools import groupby\n",
    "      \n",
    "      partCount = 1\n",
    "      l = []\n",
    "      for key, group in groupby(sorted(partition, key = lambda x: x[\"table_key\"]), lambda x: x[\"table_key\"]):\n",
    "        a = []\n",
    "        k = None\n",
    "        temp = -1\n",
    "\n",
    "        for thing in group:\n",
    "          if temp < int(thing[\"offset\"]):\n",
    "            temp = thing[\"offset\"]\n",
    "            k = thing\n",
    "\n",
    "#         yield k\n",
    "        l.append(k)\n",
    "        partCount += 1\n",
    "    \n",
    "      return l\n",
    "    \n",
    "def upsertToDelta(microBatchOutputDF):\n",
    "        df5 = spark.sparkContext._jvm.com.example.Hello.add(spark._jsparkSession, \"34.220.52.64:9092,52.35.125.165:9092\", \"datalake-qa-91\")\n",
    "        microBatchOutputDF = microBatchOutputDF.drop(\"valuenew\")\n",
    "        \n",
    "        microBatchOutputDF = microBatchOutputDF.withColumn('valueConsumerKafka', microBatchOutputDF['value'])\n",
    "        microBatchOutputDF = microBatchOutputDF.drop(\"value\")\n",
    "        \n",
    "        microBatchOutputDF.show(1)\n",
    "        \n",
    "        \n",
    "        microBatchOutputDF = microBatchOutputDF.withColumn(\"table_delete\", F.when(\n",
    "            col(\"valueConsumerKafka.before\").isNotNull and col(\"valueConsumerKafka.op\").isNotNull and col(\"valueConsumerKafka.op\").contains(\"d\"),\n",
    "            True).otherwise(False))\n",
    "        \n",
    "        \n",
    "        if True:\n",
    "          microBatchOutputDF = microBatchOutputDF.withColumn(\"table_key\", F.when(col(\"table_delete\") == True,\n",
    "                                                                                 microBatchOutputDF[\n",
    "                                                                                     \"valueConsumerKafka.before.{}\".format(\n",
    "                                                                                         \"id\")]).otherwise(\n",
    "              microBatchOutputDF[\"valueConsumerKafka.after.{}\".format(\"id\")]))\n",
    "\n",
    "        \n",
    "          \n",
    "        \n",
    "        print(\"starting point\")\n",
    "\n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "        print(\"total partitions are\")\n",
    "        print(microBatchOutputDF.rdd.getNumPartitions())\n",
    "        \n",
    "\n",
    "        dfrdd = microBatchOutputDF.rdd.mapPartitions(groupk)\n",
    "        dfByTopic = spark.createDataFrame(dfrdd, schema = microBatchOutputDF.schema)\n",
    "        \n",
    "        print(dfByTopic.rdd.getNumPartitions())\n",
    "        \n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "#         display(dfByTopic)\n",
    "\n",
    "        dfByTopicDeleted = dfByTopic.filter(F.col(\"table_delete\") == True)\n",
    "        dfByTopicDeleted = dfByTopicDeleted.select(\"valueConsumerKafka.before.*\", \"table_delete\", \"table_key\", \"offset\")\n",
    "\n",
    "        dfByTopicDeleted = dfByTopicDeleted.drop(\"valueConsumerKafka\", \"kafka-key\", \"schemaId\")\n",
    "        dfByTopicDeleted = dfByTopicDeleted.drop(\"offset\")\n",
    "\n",
    "        dfByTopicInsert = dfByTopic.filter(F.col(\"table_delete\") == False)\n",
    "        dfByTopicInsert = dfByTopicInsert.select(\"valueConsumerKafka.after.*\", \"table_delete\", \"table_key\", \"offset\")\n",
    "        # display(df)\n",
    "        dfByTopicInsert = dfByTopicInsert.drop(\"valueConsumerKafka\", \"kafka-key\", \"schemaId\")\n",
    "        dfByTopicInsert = dfByTopicInsert.drop(\"offset\")\n",
    "        \n",
    "        if True:\n",
    "            partition_column_derive_from_column_split = \"created_at_part,created_at\".split(\",\")\n",
    "            new_partition_key = partition_column_derive_from_column_split[0]\n",
    "            new_partition_value = partition_column_derive_from_column_split[1]\n",
    "            \n",
    "        print(new_partition_value)\n",
    "        \n",
    "        if True:\n",
    "            dfByTopicDeleted = dfByTopicDeleted.withColumn(new_partition_key,to_date(col(new_partition_value)))\n",
    "            \n",
    "            dfByTopicInsert = dfByTopicInsert.withColumn(new_partition_key,to_date(col(new_partition_value)))\n",
    "            \n",
    "#         if True:\n",
    "#             my_schema = dfByTopicInsert.schema\n",
    "#             res = getTableColumn(my_schema)\n",
    "# #             dfByTopic._jdf.sparkSession().sql(\"use {}\".format(self.database))\n",
    "#             if True:\n",
    "#                 print(\"table created\")\n",
    "#                 dfByTopic._jdf.sparkSession().sql(\n",
    "#                     \"create table if not exists {} ( {} ) using delta PARTITIONED BY ({}) location '{}' \".format(\"user_summary_old\",\n",
    "#                                                                                                                  res[0],\n",
    "#                                                                                                                  \"company_id,created_at_part\",\n",
    "#                                                                                                                  \"user_summary_old\"))\n",
    "\n",
    "#             table_exists = True\n",
    "\n",
    "        dfByTopicInsert = dfByTopicInsert.drop(\"table_delete\")\n",
    "        dfByTopicInsert = dfByTopicInsert.drop(\"table_key\")\n",
    "        \n",
    "        FullTableName = \"user_summary_old\"\n",
    "\n",
    "        print(FullTableName)\n",
    "        \n",
    "#         deltaTable=DeltaTable.forName(spark,FullTableName)\n",
    "\n",
    "        if True:\n",
    "#             return dfByTopicInsert\n",
    "            dfByTopicInsert.write.format(\"delta\").mode(\"overwrite\").save(\"user_summary_old\")\n",
    "#             query = get_partition_query(\"company_id,created_at_part\", \"id\")\n",
    "            \n",
    "            print(\"checkpoint 1\")\n",
    "            \n",
    "#             deltaTable.alias(\"targetTable\").merge(\n",
    "#                 datau.alias(\"updatesTable\"),\n",
    "#                 \"{0}\".format(\"updatesTable.id = targetTable.id\")) \\\n",
    "#               .whenMatchedUpdateAll() \\\n",
    "#               .whenNotMatchedInsertAll().execute()\n",
    "\n",
    "#             deltaTable.alias(\"targetTable\").merge(\n",
    "#                 dfByTopicDeleted.alias(\"updatesTable\"),\n",
    "#                 \"{0}\".format(query)) \\\n",
    "#               .whenMatchedDelete() \\\n",
    "#               .execute()\n",
    "\n",
    "\n",
    "#             deltaTable.alias(\"targetTable\").merge(\n",
    "#                 dfByTopicInsert.alias(\"updatesTable\"),\n",
    "#                 \"{0}\".format(query)) \\\n",
    "#               .whenMatchedUpdateAll() \\\n",
    "#               .whenNotMatchedInsertAll().execute()\n",
    "        \n",
    "        print(\"after merge\")\n",
    "\n",
    "        #logger.debug(\"after merge\")\n",
    "        \n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "        #logger.debug(str(datetime.datetime.now()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5a333f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datalake-qa-91"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------+--------------------+--------------------+\n",
      "|                 key|               topic|partition|offset|           timestamp|timestampType|schemaId|             payload|  valueConsumerKafka|\n",
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------+--------------------+--------------------+\n",
      "|[00 00 00 07 32 B...|qatest12nonfes.pu...|        1|195410|2022-03-03 10:26:...|            0|    3737|[00 02 BC A3 2C B...|[, [362718, 53657...|\n",
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "starting point\n",
      "2022-03-07 13:53:46.302413\n",
      "total partitions are\n",
      "10\n",
      "10\n",
      "2022-03-07 13:53:51.075628\n",
      "created_at\n",
      "user_summary_old\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:====================================================>    (46 + 2) / 50]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint 1\n",
      "after merge\n",
      "2022-03-07 13:55:26.733959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# dfBySchemaId.printSchema()\n",
    "dfa = upsertToDelta(dfBySchemaId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc17e745",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query started: 61ee83fb-6c47-4e7d-bd63-0c0c2ac44e7f\n",
      "Query started: 61ee83fb-6c47-4e7d-bd63-0c0c2ac44e7f\n",
      "Query started: 61ee83fb-6c47-4e7d-bd63-0c0c2ac44e7f\n",
      "Query started: 61ee83fb-6c47-4e7d-bd63-0c0c2ac44e7f\n",
      "Query started: 61ee83fb-6c47-4e7d-bd63-0c0c2ac44e7f\n",
      "datalake-qa-91"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------+--------------------+--------------------+\n",
      "|                 key|               topic|partition|offset|           timestamp|timestampType|schemaId|             payload|  valueConsumerKafka|\n",
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------+--------------------+--------------------+\n",
      "|[00 00 00 07 32 8...|qatest12nonfes.pu...|        0|181748|2022-03-08 07:49:...|            0|    3737|[00 02 82 CD 2C F...|[, [365377, 3834,...|\n",
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------+--------------------+--------------------+\n",
      "\n",
      "starting point\n",
      "2022-03-09 04:34:49.924840\n",
      "total partitions are\n",
      "1\n",
      "1\n",
      "2022-03-09 04:34:50.139965\n",
      "created_at\n",
      "user_summary_old\n",
      "checkpoint 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge\n",
      "2022-03-09 04:36:05.667785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/09 04:36:05 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 89192 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: {qatest12nonfes.public.user_summary_old-9=OffsetAndMetadata{offset=185963, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-7=OffsetAndMetadata{offset=191258, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-8=OffsetAndMetadata{offset=189877, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-5=OffsetAndMetadata{offset=171069, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-6=OffsetAndMetadata{offset=194807, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-3=OffsetAndMetadata{offset=201131, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-4=OffsetAndMetadata{offset=169495, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-1=OffsetAndMetadata{offset=199521, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-2=OffsetAndMetadata{offset=174686, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-0=OffsetAndMetadata{offset=181749, leaderEpoch=null, metadata=''}}\n",
      "query inside Jar\n",
      "datalake-qa-91Query made progress: {qatest12nonfes.public.user_summary_old-9=OffsetAndMetadata{offset=185963, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-7=OffsetAndMetadata{offset=191258, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-8=OffsetAndMetadata{offset=189877, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-5=OffsetAndMetadata{offset=171069, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-6=OffsetAndMetadata{offset=194807, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-3=OffsetAndMetadata{offset=201131, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-4=OffsetAndMetadata{offset=169495, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-1=OffsetAndMetadata{offset=199521, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-2=OffsetAndMetadata{offset=174686, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-0=OffsetAndMetadata{offset=181749, leaderEpoch=null, metadata=''}}\n",
      "query inside Jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: {qatest12nonfes.public.user_summary_old-9=OffsetAndMetadata{offset=185963, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-7=OffsetAndMetadata{offset=191258, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-8=OffsetAndMetadata{offset=189877, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-5=OffsetAndMetadata{offset=171069, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-6=OffsetAndMetadata{offset=194807, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-3=OffsetAndMetadata{offset=201131, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-4=OffsetAndMetadata{offset=169495, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-1=OffsetAndMetadata{offset=199521, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-2=OffsetAndMetadata{offset=174686, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-0=OffsetAndMetadata{offset=181749, leaderEpoch=null, metadata=''}}\n",
      "query inside Jar\n",
      "Query made progress: {qatest12nonfes.public.user_summary_old-9=OffsetAndMetadata{offset=185963, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-7=OffsetAndMetadata{offset=191258, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-8=OffsetAndMetadata{offset=189877, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-5=OffsetAndMetadata{offset=171069, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-6=OffsetAndMetadata{offset=194807, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-3=OffsetAndMetadata{offset=201131, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-4=OffsetAndMetadata{offset=169495, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-1=OffsetAndMetadata{offset=199521, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-2=OffsetAndMetadata{offset=174686, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-0=OffsetAndMetadata{offset=181749, leaderEpoch=null, metadata=''}}\n",
      "query inside Jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------+--------------------+--------------------+\n",
      "|                 key|               topic|partition|offset|           timestamp|timestampType|schemaId|             payload|  valueConsumerKafka|\n",
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------+--------------------+--------------------+\n",
      "|[00 00 00 07 32 9...|qatest12nonfes.pu...|        6|194807|2022-03-08 07:49:...|            0|    3737|[00 02 98 D2 2C C...|[, [365708, 44516...|\n",
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "starting point\n",
      "2022-03-09 04:36:18.419205\n",
      "total partitions are\n",
      "10\n",
      "10\n",
      "2022-03-09 04:36:18.659327\n",
      "created_at\n",
      "user_summary_old\n",
      "checkpoint 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:>                                                        (0 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: {qatest12nonfes.public.user_summary_old-9=OffsetAndMetadata{offset=185963, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-7=OffsetAndMetadata{offset=191258, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-8=OffsetAndMetadata{offset=189877, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-5=OffsetAndMetadata{offset=171069, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-6=OffsetAndMetadata{offset=194807, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-3=OffsetAndMetadata{offset=201131, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-4=OffsetAndMetadata{offset=169495, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-1=OffsetAndMetadata{offset=199521, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-2=OffsetAndMetadata{offset=174686, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-0=OffsetAndMetadata{offset=181749, leaderEpoch=null, metadata=''}}\n",
      "query inside Jar\n",
      "Query made progress: {qatest12nonfes.public.user_summary_old-9=OffsetAndMetadata{offset=185963, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-7=OffsetAndMetadata{offset=191258, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-8=OffsetAndMetadata{offset=189877, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-5=OffsetAndMetadata{offset=171069, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-6=OffsetAndMetadata{offset=194807, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-3=OffsetAndMetadata{offset=201131, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-4=OffsetAndMetadata{offset=169495, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-1=OffsetAndMetadata{offset=199521, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-2=OffsetAndMetadata{offset=174686, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-0=OffsetAndMetadata{offset=181749, leaderEpoch=null, metadata=''}}\n",
      "query inside Jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/09 04:38:55 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000 milliseconds, but spent 169494 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after merge\n",
      "2022-03-09 04:38:55.273789\n",
      "datalake-qa-91Query made progress: {qatest12nonfes.public.user_summary_old-9=OffsetAndMetadata{offset=186724, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-7=OffsetAndMetadata{offset=191876, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-8=OffsetAndMetadata{offset=191131, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-5=OffsetAndMetadata{offset=171581, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-6=OffsetAndMetadata{offset=195894, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-3=OffsetAndMetadata{offset=201396, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-4=OffsetAndMetadata{offset=170038, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-1=OffsetAndMetadata{offset=199790, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-2=OffsetAndMetadata{offset=175532, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-0=OffsetAndMetadata{offset=186736, leaderEpoch=null, metadata=''}}\n",
      "query inside Jar\n",
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------+--------------------+--------------------+\n",
      "|                 key|               topic|partition|offset|           timestamp|timestampType|schemaId|             payload|  valueConsumerKafka|\n",
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------+--------------------+--------------------+\n",
      "|[00 00 00 07 32 F...|qatest12nonfes.pu...|        1|199790|2022-03-09 04:37:...|            0|    3737|[00 02 F2 DC 2C E...|[, [366393, 54642...|\n",
      "+--------------------+--------------------+---------+------+--------------------+-------------+--------+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "starting point\n",
      "2022-03-09 04:38:57.467026\n",
      "total partitions are\n",
      "4\n",
      "4\n",
      "2022-03-09 04:38:57.709219\n",
      "created_at\n",
      "Query made progress: {qatest12nonfes.public.user_summary_old-9=OffsetAndMetadata{offset=186724, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-7=OffsetAndMetadata{offset=191876, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-8=OffsetAndMetadata{offset=191131, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-5=OffsetAndMetadata{offset=171581, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-6=OffsetAndMetadata{offset=195894, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-3=OffsetAndMetadata{offset=201396, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-4=OffsetAndMetadata{offset=170038, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-1=OffsetAndMetadata{offset=199790, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-2=OffsetAndMetadata{offset=175532, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-0=OffsetAndMetadata{offset=186736, leaderEpoch=null, metadata=''}}\n",
      "query inside Jar\n",
      "user_summary_old\n",
      "checkpoint 1\n",
      "Query made progress: {qatest12nonfes.public.user_summary_old-9=OffsetAndMetadata{offset=186724, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-7=OffsetAndMetadata{offset=191876, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-8=OffsetAndMetadata{offset=191131, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-5=OffsetAndMetadata{offset=171581, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-6=OffsetAndMetadata{offset=195894, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-3=OffsetAndMetadata{offset=201396, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-4=OffsetAndMetadata{offset=170038, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-1=OffsetAndMetadata{offset=199790, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-2=OffsetAndMetadata{offset=175532, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-0=OffsetAndMetadata{offset=186736, leaderEpoch=null, metadata=''}}\n",
      "query inside Jar\n",
      "Query made progress: {qatest12nonfes.public.user_summary_old-9=OffsetAndMetadata{offset=186724, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-7=OffsetAndMetadata{offset=191876, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-8=OffsetAndMetadata{offset=191131, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-5=OffsetAndMetadata{offset=171581, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-6=OffsetAndMetadata{offset=195894, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-3=OffsetAndMetadata{offset=201396, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-4=OffsetAndMetadata{offset=170038, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-1=OffsetAndMetadata{offset=199790, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-2=OffsetAndMetadata{offset=175532, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-0=OffsetAndMetadata{offset=186736, leaderEpoch=null, metadata=''}}\n",
      "query inside Jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: {qatest12nonfes.public.user_summary_old-9=OffsetAndMetadata{offset=186724, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-7=OffsetAndMetadata{offset=191876, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-8=OffsetAndMetadata{offset=191131, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-5=OffsetAndMetadata{offset=171581, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-6=OffsetAndMetadata{offset=195894, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-3=OffsetAndMetadata{offset=201396, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-4=OffsetAndMetadata{offset=170038, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-1=OffsetAndMetadata{offset=199790, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-2=OffsetAndMetadata{offset=175532, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-0=OffsetAndMetadata{offset=186736, leaderEpoch=null, metadata=''}}\n",
      "query inside Jar\n",
      "Query made progress: {qatest12nonfes.public.user_summary_old-9=OffsetAndMetadata{offset=186724, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-7=OffsetAndMetadata{offset=191876, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-8=OffsetAndMetadata{offset=191131, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-5=OffsetAndMetadata{offset=171581, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-6=OffsetAndMetadata{offset=195894, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-3=OffsetAndMetadata{offset=201396, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-4=OffsetAndMetadata{offset=170038, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-1=OffsetAndMetadata{offset=199790, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-2=OffsetAndMetadata{offset=175532, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-0=OffsetAndMetadata{offset=186736, leaderEpoch=null, metadata=''}}\n",
      "query inside Jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/09 04:39:02 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "22/03/09 04:39:02 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:716)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:152)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:258)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:168)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:203)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "22/03/09 04:39:03 ERROR FileFormatWriter: Aborting job 6f4fffd8-ff4b-43f8-a39b-e319d380ad08.\n",
      "org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\n",
      "Exchange hashpartitioning(id#15934L, company_id#15938, created_at_part#15988, 200), true, [id=#2095]\n",
      "+- *(3) Project [id#15934L, user_id#15935, hub_id#15936, city_id#15937, company_id#15938, date#15939, app_version#15940, gps_kms#15941, odometer_kms#15942, halt_duration#15943, travel_duration#15944, avg_speed#15945, max_speed#15946, official_sms_sent_count#15947, personal_sms_sent_count#15948, official_call_incoming_count#15949, official_call_outgoing_count#15950, official_call_incoming_duration#15951, official_call_outgoing_duration#15952, personal_call_incoming_count#15953, personal_call_outgoing_count#15954, personal_call_incoming_duration#15955, personal_call_outgoing_duration#15956, cug_call_incoming_count#15957, ... 32 more fields]\n",
      "   +- *(3) ColumnarToRow\n",
      "      +- FileScan parquet [id#15934L,user_id#15935,hub_id#15936,city_id#15937,company_id#15938,date#15939,app_version#15940,gps_kms#15941,odometer_kms#15942,halt_duration#15943,travel_duration#15944,avg_speed#15945,max_speed#15946,official_sms_sent_count#15947,personal_sms_sent_count#15948,official_call_incoming_count#15949,official_call_outgoing_count#15950,official_call_incoming_duration#15951,official_call_outgoing_duration#15952,personal_call_incoming_count#15953,personal_call_outgoing_count#15954,personal_call_incoming_duration#15955,personal_call_outgoing_duration#15956,cug_call_incoming_count#15957,... 31 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: TahoeBatchFileIndex[file:/opt/workspace/user_summary_old], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint,user_id:int,hub_id:int,city_id:int,company_id:int,date:timestamp,app_version:str...\n",
      "\n",
      "\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:95)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:182)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:96)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.MapPartitionsExec.doExecute(objects.scala:192)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.SerializeFromObjectExec.inputRDDs(objects.scala:117)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.delta.constraints.DeltaInvariantCheckerExec.doExecute(DeltaInvariantCheckerExec.scala:87)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:172)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:130)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:115)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:81)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$writeAllChanges$1(MergeIntoCommand.scala:545)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordMergeOperation(MergeIntoCommand.scala:622)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.writeAllChanges(MergeIntoCommand.scala:453)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$4(MergeIntoCommand.scala:269)\n",
      "\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53)\n",
      "\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32)\n",
      "\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.withStatusCode(MergeIntoCommand.scala:197)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$2(MergeIntoCommand.scala:269)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$2$adapted(MergeIntoCommand.scala:250)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:188)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$1(MergeIntoCommand.scala:250)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordMergeOperation(MergeIntoCommand.scala:622)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.run(MergeIntoCommand.scala:249)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:239)\n",
      "\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:60)\n",
      "\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:48)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:123)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:228)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "java.lang.Thread.run(Thread.java:750)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:111)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1471)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:232)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:398)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:389)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:484)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:202)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:198)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:519)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:202)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:198)\n",
      "\tat org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:196)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:47)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:64)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:64)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:81)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n",
      "\t... 93 more\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query made progress: {qatest12nonfes.public.user_summary_old-9=OffsetAndMetadata{offset=186724, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-7=OffsetAndMetadata{offset=191876, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-8=OffsetAndMetadata{offset=191131, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-5=OffsetAndMetadata{offset=171581, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-6=OffsetAndMetadata{offset=195894, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-3=OffsetAndMetadata{offset=201396, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-4=OffsetAndMetadata{offset=170038, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-1=OffsetAndMetadata{offset=199790, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-2=OffsetAndMetadata{offset=175532, leaderEpoch=null, metadata=''}, qatest12nonfes.public.user_summary_old-0=OffsetAndMetadata{offset=186736, leaderEpoch=null, metadata=''}}\n",
      "query inside Jar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/09 04:39:03 ERROR MicroBatchExecution: Query user_summary_old [id = 61ee83fb-6c47-4e7d-bd63-0c0c2ac44e7f, runId = fafce9b0-d550-41e3-9c33-e5a40665573d] terminated with error\n",
      "py4j.Py4JException: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\", line 2442, in _call_proxy\n",
      "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py\", line 210, in call\n",
      "    raise e\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py\", line 207, in call\n",
      "    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n",
      "  File \"/tmp/ipykernel_6709/2244588621.py\", line 195, in upsertToDelta2\n",
      "    deltaTable.alias(\"targetTable\").merge(\n",
      "  File \"/tmp/spark-b1c34e37-d345-4307-8d33-518a9101aad9/userFiles-81c5542b-cd15-496b-a369-3192d349d881/io.delta_delta-core_2.12-0.8.0.jar/delta/tables.py\", line 627, in execute\n",
      "    self._jbuilder.execute()\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\", line 1304, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py\", line 131, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/local/lib/python3.9/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o962.execute.\n",
      ": org.apache.spark.SparkException: Job aborted.\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:130)\n",
      "\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:115)\n",
      "\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:81)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$writeAllChanges$1(MergeIntoCommand.scala:545)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordMergeOperation(MergeIntoCommand.scala:622)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.writeAllChanges(MergeIntoCommand.scala:453)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$4(MergeIntoCommand.scala:269)\n",
      "\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53)\n",
      "\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32)\n",
      "\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.withStatusCode(MergeIntoCommand.scala:197)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$2(MergeIntoCommand.scala:269)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$2$adapted(MergeIntoCommand.scala:250)\n",
      "\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:188)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$1(MergeIntoCommand.scala:250)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordMergeOperation(MergeIntoCommand.scala:622)\n",
      "\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.run(MergeIntoCommand.scala:249)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:239)\n",
      "\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:60)\n",
      "\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:48)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:123)\n",
      "\tat io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:228)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\n",
      "Exchange hashpartitioning(id#15934L, company_id#15938, created_at_part#15988, 200), true, [id=#2095]\n",
      "+- *(3) Project [id#15934L, user_id#15935, hub_id#15936, city_id#15937, company_id#15938, date#15939, app_version#15940, gps_kms#15941, odometer_kms#15942, halt_duration#15943, travel_duration#15944, avg_speed#15945, max_speed#15946, official_sms_sent_count#15947, personal_sms_sent_count#15948, official_call_incoming_count#15949, official_call_outgoing_count#15950, official_call_incoming_duration#15951, official_call_outgoing_duration#15952, personal_call_incoming_count#15953, personal_call_outgoing_count#15954, personal_call_incoming_duration#15955, personal_call_outgoing_duration#15956, cug_call_incoming_count#15957, ... 32 more fields]\n",
      "   +- *(3) ColumnarToRow\n",
      "      +- FileScan parquet [id#15934L,user_id#15935,hub_id#15936,city_id#15937,company_id#15938,date#15939,app_version#15940,gps_kms#15941,odometer_kms#15942,halt_duration#15943,travel_duration#15944,avg_speed#15945,max_speed#15946,official_sms_sent_count#15947,personal_sms_sent_count#15948,official_call_incoming_count#15949,official_call_outgoing_count#15950,official_call_incoming_duration#15951,official_call_outgoing_duration#15952,personal_call_incoming_count#15953,personal_call_outgoing_count#15954,personal_call_incoming_duration#15955,personal_call_outgoing_duration#15956,cug_call_incoming_count#15957,... 31 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: TahoeBatchFileIndex[file:/opt/workspace/user_summary_old], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint,user_id:int,hub_id:int,city_id:int,company_id:int,date:timestamp,app_version:str...\n",
      "\n",
      "\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:95)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:182)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:96)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.MapPartitionsExec.doExecute(objects.scala:192)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n",
      "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n",
      "\tat org.apache.spark.sql.execution.SerializeFromObjectExec.inputRDDs(objects.scala:117)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.delta.constraints.DeltaInvariantCheckerExec.doExecute(DeltaInvariantCheckerExec.scala:87)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:172)\n",
      "\t... 40 more\n",
      "Caused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "This stopped SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "The currently active SparkContext was created at:\n",
      "\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "java.lang.Thread.run(Thread.java:750)\n",
      "         \n",
      "\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:111)\n",
      "\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1471)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:232)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:398)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:389)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:484)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:202)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:198)\n",
      "\tat org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:519)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:202)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:198)\n",
      "\tat org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:196)\n",
      "\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:47)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:64)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:64)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:83)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:81)\n",
      "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n",
      "\t... 93 more\n",
      "\n",
      "\n",
      "\tat py4j.Protocol.getReturnValue(Protocol.java:476)\n",
      "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:108)\n",
      "\tat com.sun.proxy.$Proxy17.call(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:36)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:573)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$15(MicroBatchExecution.scala:571)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:571)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:223)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:352)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:350)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:191)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:185)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:334)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\n",
      "22/03/09 04:39:03 WARN NettyRpcEnv: Ignored failure: java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@760b4de rejected from java.util.concurrent.ScheduledThreadPoolExecutor@581c59de[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 0]\n",
      "Exception in thread \"stream execution thread for user_summary_old [id = 61ee83fb-6c47-4e7d-bd63-0c0c2ac44e7f, runId = fafce9b0-d550-41e3-9c33-e5a40665573d]\" org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StateStoreCoordinatorRef.deactivateInstances(StateStoreCoordinator.scala:108)\n",
      "\tat org.apache.spark.sql.streaming.StreamingQueryManager.notifyQueryTermination(StreamingQueryManager.scala:439)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$2(StreamExecution.scala:382)\n",
      "\tat org.apache.spark.util.UninterruptibleThread.runUninterruptibly(UninterruptibleThread.scala:77)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:363)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:245)\n",
      "Caused by: org.apache.spark.rpc.RpcEnvStoppedException: RpcEnv already stopped.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:167)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:548)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:552)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)\n",
      "\t... 7 more\n"
     ]
    },
    {
     "ename": "StreamingQueryException",
     "evalue": "An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\", line 2442, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py\", line 210, in call\n    raise e\n  File \"/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py\", line 207, in call\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n  File \"/tmp/ipykernel_6709/2244588621.py\", line 195, in upsertToDelta2\n    deltaTable.alias(\"targetTable\").merge(\n  File \"/tmp/spark-b1c34e37-d345-4307-8d33-518a9101aad9/userFiles-81c5542b-cd15-496b-a369-3192d349d881/io.delta_delta-core_2.12-0.8.0.jar/delta/tables.py\", line 627, in execute\n    self._jbuilder.execute()\n  File \"/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py\", line 131, in deco\n    return f(*a, **kw)\n  File \"/usr/local/lib/python3.9/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o962.execute.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:130)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:115)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:81)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$writeAllChanges$1(MergeIntoCommand.scala:545)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordMergeOperation(MergeIntoCommand.scala:622)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.writeAllChanges(MergeIntoCommand.scala:453)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$4(MergeIntoCommand.scala:269)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.withStatusCode(MergeIntoCommand.scala:197)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$2(MergeIntoCommand.scala:269)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$2$adapted(MergeIntoCommand.scala:250)\n\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:188)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$1(MergeIntoCommand.scala:250)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordMergeOperation(MergeIntoCommand.scala:622)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.run(MergeIntoCommand.scala:249)\n\tat io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:239)\n\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:60)\n\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:48)\n\tat io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:123)\n\tat io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:228)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(id#15934L, company_id#15938, created_at_part#15988, 200), true, [id=#2095]\n+- *(3) Project [id#15934L, user_id#15935, hub_id#15936, city_id#15937, company_id#15938, date#15939, app_version#15940, gps_kms#15941, odometer_kms#15942, halt_duration#15943, travel_duration#15944, avg_speed#15945, max_speed#15946, official_sms_sent_count#15947, personal_sms_sent_count#15948, official_call_incoming_count#15949, official_call_outgoing_count#15950, official_call_incoming_duration#15951, official_call_outgoing_duration#15952, personal_call_incoming_count#15953, personal_call_outgoing_count#15954, personal_call_incoming_duration#15955, personal_call_outgoing_duration#15956, cug_call_incoming_count#15957, ... 32 more fields]\n   +- *(3) ColumnarToRow\n      +- FileScan parquet [id#15934L,user_id#15935,hub_id#15936,city_id#15937,company_id#15938,date#15939,app_version#15940,gps_kms#15941,odometer_kms#15942,halt_duration#15943,travel_duration#15944,avg_speed#15945,max_speed#15946,official_sms_sent_count#15947,personal_sms_sent_count#15948,official_call_incoming_count#15949,official_call_outgoing_count#15950,official_call_incoming_duration#15951,official_call_outgoing_duration#15952,personal_call_incoming_count#15953,personal_call_outgoing_count#15954,personal_call_incoming_duration#15955,personal_call_outgoing_duration#15956,cug_call_incoming_count#15957,... 31 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: TahoeBatchFileIndex[file:/opt/workspace/user_summary_old], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint,user_id:int,hub_id:int,city_id:int,company_id:int,date:timestamp,app_version:str...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:182)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.MapPartitionsExec.doExecute(objects.scala:192)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.SerializeFromObjectExec.inputRDDs(objects.scala:117)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.delta.constraints.DeltaInvariantCheckerExec.doExecute(DeltaInvariantCheckerExec.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:172)\n\t... 40 more\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:750)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:111)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1471)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:232)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:398)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:389)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:484)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:202)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:198)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:519)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:202)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:198)\n\tat org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:196)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:47)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:64)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:64)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:83)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:81)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:98)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 93 more\n\n\n=== Streaming Query ===\nIdentifier: user_summary_old [id = 61ee83fb-6c47-4e7d-bd63-0c0c2ac44e7f, runId = fafce9b0-d550-41e3-9c33-e5a40665573d]\nCurrent Committed Offsets: {KafkaV2[Subscribe[qatest12nonfes.public.user_summary_old]]: {\"qatest12nonfes.public.user_summary_old\":{\"8\":191131,\"2\":175532,\"5\":171581,\"4\":170038,\"7\":191876,\"1\":199790,\"9\":186724,\"3\":201396,\"6\":195894,\"0\":186736}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[qatest12nonfes.public.user_summary_old]]: {\"qatest12nonfes.public.user_summary_old\":{\"8\":191132,\"2\":175534,\"5\":171581,\"4\":170038,\"7\":191878,\"1\":199799,\"9\":186724,\"3\":201396,\"6\":195894,\"0\":186736}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nFilter isnotnull(value#3194)\n+- StreamingDataSourceV2Relation [key#3193, value#3194, topic#3195, partition#3196, offset#3197L, timestamp#3198, timestampType#3199], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@7e5a4053, KafkaV2[Subscribe[qatest12nonfes.public.user_summary_old]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStreamingQueryException\u001b[0m                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 236>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m kafka \u001b[38;5;241m=\u001b[39m kafka\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue is NOT NULL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    229\u001b[0m ssc \u001b[38;5;241m=\u001b[39m kafka \\\n\u001b[1;32m    230\u001b[0m             \u001b[38;5;241m.\u001b[39mwriteStream \\\n\u001b[1;32m    231\u001b[0m             \u001b[38;5;241m.\u001b[39mqueryName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_summary_old\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \\\n\u001b[1;32m    232\u001b[0m             \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointLocation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcp/raw/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m    233\u001b[0m             \u001b[38;5;241m.\u001b[39mtrigger(processingTime\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10 seconds\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m    234\u001b[0m             \u001b[38;5;241m.\u001b[39mforeachBatch(upsertToDelta2)\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m--> 236\u001b[0m \u001b[43mssc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/streaming.py:103\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py:137\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    133\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mStreamingQueryException\u001b[0m: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\n  File \"/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\", line 2442, in _call_proxy\n    return_value = getattr(self.pool[obj_id], method)(*params)\n  File \"/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py\", line 210, in call\n    raise e\n  File \"/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py\", line 207, in call\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\n  File \"/tmp/ipykernel_6709/2244588621.py\", line 195, in upsertToDelta2\n    deltaTable.alias(\"targetTable\").merge(\n  File \"/tmp/spark-b1c34e37-d345-4307-8d33-518a9101aad9/userFiles-81c5542b-cd15-496b-a369-3192d349d881/io.delta_delta-core_2.12-0.8.0.jar/delta/tables.py\", line 627, in execute\n    self._jbuilder.execute()\n  File \"/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\", line 1304, in __call__\n    return_value = get_return_value(\n  File \"/usr/local/lib/python3.9/dist-packages/pyspark/sql/utils.py\", line 131, in deco\n    return f(*a, **kw)\n  File \"/usr/local/lib/python3.9/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o962.execute.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:226)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$1(TransactionalWrite.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:100)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:160)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:87)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:763)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:130)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:115)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:81)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$writeAllChanges$1(MergeIntoCommand.scala:545)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordMergeOperation(MergeIntoCommand.scala:622)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.writeAllChanges(MergeIntoCommand.scala:453)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$4(MergeIntoCommand.scala:269)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.withStatusCode(MergeIntoCommand.scala:197)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$2(MergeIntoCommand.scala:269)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$2$adapted(MergeIntoCommand.scala:250)\n\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:188)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$run$1(MergeIntoCommand.scala:250)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordMergeOperation(MergeIntoCommand.scala:622)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.run(MergeIntoCommand.scala:249)\n\tat io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:239)\n\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:60)\n\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:48)\n\tat io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:123)\n\tat io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:228)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(id#15934L, company_id#15938, created_at_part#15988, 200), true, [id=#2095]\n+- *(3) Project [id#15934L, user_id#15935, hub_id#15936, city_id#15937, company_id#15938, date#15939, app_version#15940, gps_kms#15941, odometer_kms#15942, halt_duration#15943, travel_duration#15944, avg_speed#15945, max_speed#15946, official_sms_sent_count#15947, personal_sms_sent_count#15948, official_call_incoming_count#15949, official_call_outgoing_count#15950, official_call_incoming_duration#15951, official_call_outgoing_duration#15952, personal_call_incoming_count#15953, personal_call_outgoing_count#15954, personal_call_incoming_duration#15955, personal_call_outgoing_duration#15956, cug_call_incoming_count#15957, ... 32 more fields]\n   +- *(3) ColumnarToRow\n      +- FileScan parquet [id#15934L,user_id#15935,hub_id#15936,city_id#15937,company_id#15938,date#15939,app_version#15940,gps_kms#15941,odometer_kms#15942,halt_duration#15943,travel_duration#15944,avg_speed#15945,max_speed#15946,official_sms_sent_count#15947,personal_sms_sent_count#15948,official_call_incoming_count#15949,official_call_outgoing_count#15950,official_call_incoming_duration#15951,official_call_outgoing_duration#15952,personal_call_incoming_count#15953,personal_call_outgoing_count#15954,personal_call_incoming_duration#15955,personal_call_outgoing_duration#15956,cug_call_incoming_count#15957,... 31 more fields] Batched: true, DataFilters: [], Format: Parquet, Location: TahoeBatchFileIndex[file:/opt/workspace/user_summary_old], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<id:bigint,user_id:int,hub_id:int,city_id:int,company_id:int,date:timestamp,app_version:str...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:95)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.joins.SortMergeJoinExec.doExecute(SortMergeJoinExec.scala:182)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.DeserializeToObjectExec.doExecute(objects.scala:96)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.MapPartitionsExec.doExecute(objects.scala:192)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.SerializeFromObjectExec.inputRDDs(objects.scala:117)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.delta.constraints.DeltaInvariantCheckerExec.doExecute(DeltaInvariantCheckerExec.scala:87)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:172)\n\t... 40 more\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:750)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:750)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:111)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1471)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.buildReaderWithPartitionValues(ParquetFileFormat.scala:232)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:398)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:389)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecuteColumnar(DataSourceScanExec.scala:484)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:202)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:198)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteColumnar(WholeStageCodegenExec.scala:519)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeColumnar$1(SparkPlan.scala:202)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.executeColumnar(SparkPlan.scala:198)\n\tat org.apache.spark.sql.execution.ColumnarToRowExec.inputRDDs(Columnar.scala:196)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:47)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:720)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:175)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:213)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:210)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:171)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:64)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:64)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:83)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:81)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:98)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 93 more\n\n\n=== Streaming Query ===\nIdentifier: user_summary_old [id = 61ee83fb-6c47-4e7d-bd63-0c0c2ac44e7f, runId = fafce9b0-d550-41e3-9c33-e5a40665573d]\nCurrent Committed Offsets: {KafkaV2[Subscribe[qatest12nonfes.public.user_summary_old]]: {\"qatest12nonfes.public.user_summary_old\":{\"8\":191131,\"2\":175532,\"5\":171581,\"4\":170038,\"7\":191876,\"1\":199790,\"9\":186724,\"3\":201396,\"6\":195894,\"0\":186736}}}\nCurrent Available Offsets: {KafkaV2[Subscribe[qatest12nonfes.public.user_summary_old]]: {\"qatest12nonfes.public.user_summary_old\":{\"8\":191132,\"2\":175534,\"5\":171581,\"4\":170038,\"7\":191878,\"1\":199799,\"9\":186724,\"3\":201396,\"6\":195894,\"0\":186736}}}\n\nCurrent State: ACTIVE\nThread State: RUNNABLE\n\nLogical Plan:\nFilter isnotnull(value#3194)\n+- StreamingDataSourceV2Relation [key#3193, value#3194, topic#3195, partition#3196, offset#3197L, timestamp#3198, timestampType#3199], org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaScan@7e5a4053, KafkaV2[Subscribe[qatest12nonfes.public.user_summary_old]]\n"
     ]
    }
   ],
   "source": [
    "# kafka = spark \\\n",
    "#   .read \\\n",
    "#   .format(\"kafka\") \\\n",
    "#             .option(\"kafka.bootstrap.servers\", \"34.220.52.64:9092,52.35.125.165:9092\") \\\n",
    "#             .option(\"subscribe\", \"qatest12nonfes.public.user_summary_old\") \\\n",
    "#             .option(\"startingOffsets\", \"earliest\") \\\n",
    "#             .option(\"maxOffsetsPerTrigger\", 1) \\\n",
    "#             .option(\"failOnDataLoss\", False) \\\n",
    "#             .load()\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, StringType, FloatType\n",
    "from delta.tables import *\n",
    "from pyspark.sql import DataFrame\n",
    "  \n",
    "def getTableColumn(my_schema):\n",
    "    tableList = []\n",
    "    col = []\n",
    "    inputTable = \"updatesTable.\"\n",
    "    targetTable = \"targetTable.\"\n",
    "\n",
    "    ignoreColumns = [\"table_key_ts\", \"table_delete\", \"table_key\"]\n",
    "\n",
    "    for field in my_schema.fields:\n",
    "        if field.name == \"after\":\n",
    "            for f3 in field.dataType:\n",
    "                te = f3.name + \" \" + f3.dataType.typeName()\n",
    "                if f3.name not in ignoreColumns:\n",
    "                    tableList.append(te)\n",
    "                    col.append(f3.name)\n",
    "        else:\n",
    "            te = field.name + \" \" + field.dataType.typeName()\n",
    "            if field.name not in ignoreColumns:\n",
    "                tableList.append(te)\n",
    "                col.append(field.name)\n",
    "    res = \"\"\n",
    "    insertResInp = \"\"\n",
    "    insertResOut = \"\"\n",
    "    merge = \"\"\n",
    "    for i in range(len(tableList)):\n",
    "        if i == (len(tableList) - 1):\n",
    "            res += tableList[i]\n",
    "            insertResInp += col[i]\n",
    "            insertResOut += inputTable + col[i]\n",
    "            merge += targetTable + col[i] + \" = \" + inputTable + col[i]\n",
    "        else:\n",
    "            res += tableList[i] + \",\"\n",
    "            insertResInp += col[i] + \",\"\n",
    "            insertResOut += inputTable + col[i] + \",\"\n",
    "            merge += targetTable + col[i] + \" = \" + inputTable + col[i] + \",\"\n",
    "    ans = [res, insertResInp, insertResOut, merge]\n",
    "    \n",
    "    \n",
    "    print(ans)\n",
    "    return ans\n",
    "\n",
    "def groupk(partition):\n",
    "      from itertools import groupby\n",
    "      \n",
    "      partCount = 1\n",
    "      l = []\n",
    "      for key, group in groupby(sorted(partition, key = lambda x: x[\"table_key\"]), lambda x: x[\"table_key\"]):\n",
    "        a = []\n",
    "        k = None\n",
    "        temp = -1\n",
    "\n",
    "        for thing in group:\n",
    "          if temp < int(thing[\"offset\"]):\n",
    "            temp = thing[\"offset\"]\n",
    "            k = thing\n",
    "\n",
    "#         yield k\n",
    "        l.append(k)\n",
    "        partCount += 1\n",
    "    \n",
    "      return l\n",
    "    \n",
    "def upsertToDelta2(kafka, batchId):\n",
    "        byteArrayToLong = fn.udf(lambda x: int.from_bytes(x, byteorder='big', signed=False), LongType())\n",
    "        df = kafka.withColumn('valuenew', kafka['value'])\n",
    "        df = df.withColumn(\"schemaId\", byteArrayToLong(fn.substring(\"valuenew\", 2, 4))) \\\n",
    "                            .withColumn(\"payload\", fn.expr(\"substring(valuenew, 6, length(valuenew)-5)\"))\n",
    "\n",
    "\n",
    "        jsonSchema = getJsonSchema(3737, schemasByIdDict,\"http://52.35.125.165:8081\")\n",
    "        currentValueSchemaId = 3737\n",
    "        currentValueSchema = jsonSchema\n",
    "        dfBySchemaId = df.where(df.schemaId == currentValueSchemaId)\n",
    "        dfBySchemaId = dfBySchemaId.drop(\"value\")\n",
    "\n",
    "\n",
    "        dfBySchemaId = dfBySchemaId.withColumn(\"value\", from_avro(\"payload\", currentValueSchema))\n",
    "        \n",
    "        microBatchOutputDF = dfBySchemaId\n",
    "\n",
    "        df5 = spark.sparkContext._jvm.com.example.Hello.add(spark._jsparkSession, \"34.220.52.64:9092,52.35.125.165:9092\", \"datalake-qa-91\")\n",
    "        microBatchOutputDF = microBatchOutputDF.drop(\"valuenew\")\n",
    "        \n",
    "        microBatchOutputDF = microBatchOutputDF.withColumn('valueConsumerKafka', microBatchOutputDF['value'])\n",
    "        microBatchOutputDF = microBatchOutputDF.drop(\"value\")\n",
    "        \n",
    "        microBatchOutputDF.show(1)\n",
    "        \n",
    "        \n",
    "        microBatchOutputDF = microBatchOutputDF.withColumn(\"table_delete\", F.when(\n",
    "            col(\"valueConsumerKafka.before\").isNotNull and col(\"valueConsumerKafka.op\").isNotNull and col(\"valueConsumerKafka.op\").contains(\"d\"),\n",
    "            True).otherwise(False))\n",
    "        \n",
    "        \n",
    "        if True:\n",
    "          microBatchOutputDF = microBatchOutputDF.withColumn(\"table_key\", F.when(col(\"table_delete\") == True,\n",
    "                                                                                 microBatchOutputDF[\n",
    "                                                                                     \"valueConsumerKafka.before.{}\".format(\n",
    "                                                                                         \"id\")]).otherwise(\n",
    "              microBatchOutputDF[\"valueConsumerKafka.after.{}\".format(\"id\")]))\n",
    "\n",
    "        \n",
    "          \n",
    "        \n",
    "        print(\"starting point\")\n",
    "\n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "        print(\"total partitions are\")\n",
    "        print(microBatchOutputDF.rdd.getNumPartitions())\n",
    "        \n",
    "\n",
    "        dfrdd = microBatchOutputDF.rdd.mapPartitions(groupk)\n",
    "        dfByTopic = spark.createDataFrame(dfrdd, schema = microBatchOutputDF.schema)\n",
    "        \n",
    "        print(dfByTopic.rdd.getNumPartitions())\n",
    "        \n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "#         display(dfByTopic)\n",
    "\n",
    "        dfByTopicDeleted = dfByTopic.filter(F.col(\"table_delete\") == True)\n",
    "        dfByTopicDeleted = dfByTopicDeleted.select(\"valueConsumerKafka.before.*\", \"table_delete\", \"table_key\", \"offset\")\n",
    "\n",
    "        dfByTopicDeleted = dfByTopicDeleted.drop(\"valueConsumerKafka\", \"kafka-key\", \"schemaId\")\n",
    "        dfByTopicDeleted = dfByTopicDeleted.drop(\"offset\")\n",
    "\n",
    "        dfByTopicInsert = dfByTopic.filter(F.col(\"table_delete\") == False)\n",
    "        dfByTopicInsert = dfByTopicInsert.select(\"valueConsumerKafka.after.*\", \"table_delete\", \"table_key\", \"offset\")\n",
    "        # display(df)\n",
    "        dfByTopicInsert = dfByTopicInsert.drop(\"valueConsumerKafka\", \"kafka-key\", \"schemaId\")\n",
    "        dfByTopicInsert = dfByTopicInsert.drop(\"offset\")\n",
    "        \n",
    "        if True:\n",
    "            partition_column_derive_from_column_split = \"created_at_part,created_at\".split(\",\")\n",
    "            new_partition_key = partition_column_derive_from_column_split[0]\n",
    "            new_partition_value = partition_column_derive_from_column_split[1]\n",
    "            \n",
    "        print(new_partition_value)\n",
    "        \n",
    "        if True:\n",
    "            dfByTopicDeleted = dfByTopicDeleted.withColumn(new_partition_key,to_date(col(new_partition_value)))\n",
    "            \n",
    "            dfByTopicInsert = dfByTopicInsert.withColumn(new_partition_key,to_date(col(new_partition_value)))\n",
    "            \n",
    "#         if True:\n",
    "#             my_schema = dfByTopicInsert.schema\n",
    "#             res = getTableColumn(my_schema)\n",
    "# #             dfByTopic._jdf.sparkSession().sql(\"use {}\".format(self.database))\n",
    "#             if True:\n",
    "#                 print(\"table created\")\n",
    "#                 dfByTopic._jdf.sparkSession().sql(\n",
    "#                     \"create table if not exists {} ( {} ) using delta PARTITIONED BY ({}) location '{}' \".format(\"user_summary_old\",\n",
    "#                                                                                                                  res[0],\n",
    "#                                                                                                                  \"company_id,created_at_part\",\n",
    "#                                                                                                                  \"user_summary_old\"))\n",
    "\n",
    "#             table_exists = True\n",
    "\n",
    "        dfByTopicInsert = dfByTopicInsert.drop(\"table_delete\")\n",
    "        dfByTopicInsert = dfByTopicInsert.drop(\"table_key\")\n",
    "        \n",
    "        FullTableName = \"user_summary_old\"\n",
    "\n",
    "        print(FullTableName)\n",
    "        \n",
    "        deltaTable=DeltaTable.forName(spark,FullTableName)\n",
    "\n",
    "        if True:\n",
    "#             return dfByTopicInsert\n",
    "#             dfByTopicInsert.write.format(\"delta\").mode(\"overwrite\").save(\"user_summary_old\")\n",
    "            query = get_partition_query(\"company_id,created_at_part\", \"id\")\n",
    "            \n",
    "            print(\"checkpoint 1\")\n",
    "            \n",
    "#             deltaTable.alias(\"targetTable\").merge(\n",
    "#                 datau.alias(\"updatesTable\"),\n",
    "#                 \"{0}\".format(\"updatesTable.id = targetTable.id\")) \\\n",
    "#               .whenMatchedUpdateAll() \\\n",
    "#               .whenNotMatchedInsertAll().execute()\n",
    "\n",
    "            deltaTable.alias(\"targetTable\").merge(\n",
    "                dfByTopicDeleted.alias(\"updatesTable\"),\n",
    "                \"{0}\".format(query)) \\\n",
    "              .whenMatchedDelete() \\\n",
    "              .execute()\n",
    "\n",
    "\n",
    "            deltaTable.alias(\"targetTable\").merge(\n",
    "                dfByTopicInsert.alias(\"updatesTable\"),\n",
    "                \"{0}\".format(query)) \\\n",
    "              .whenMatchedUpdateAll() \\\n",
    "              .whenNotMatchedInsertAll().execute()\n",
    "        \n",
    "        print(\"after merge\")\n",
    "\n",
    "        #logger.debug(\"after merge\")\n",
    "        \n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "        #logger.debug(str(datetime.datetime.now()))\n",
    "\n",
    "        \n",
    "kafka = spark.readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"34.220.52.64:9092,52.35.125.165:9092\") \\\n",
    "            .option(\"subscribe\", \"qatest12nonfes.public.user_summary_old\") \\\n",
    "            .option(\"startingOffsets\", \"earliest\") \\\n",
    "            .option(\"maxOffsetsPerTrigger\", 1000000) \\\n",
    "            .option(\"failOnDataLoss\", False) \\\n",
    "            .load()\n",
    "\n",
    "\n",
    "kafka = kafka.filter(\"value is NOT NULL\")\n",
    "\n",
    "ssc = kafka \\\n",
    "            .writeStream \\\n",
    "            .queryName(\"{0}\".format(\"user_summary_old\")) \\\n",
    "            .option(\"checkpointLocation\", \"cp/raw/\") \\\n",
    "            .trigger(processingTime='10 seconds') \\\n",
    "            .foreachBatch(upsertToDelta2).start()\n",
    "\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "138c4dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, LongType, StringType, FloatType\n",
    "from delta.tables import *\n",
    "from pyspark.sql import DataFrame\n",
    "  \n",
    "def getTableColumn(my_schema):\n",
    "    tableList = []\n",
    "    col = []\n",
    "    inputTable = \"updatesTable.\"\n",
    "    targetTable = \"targetTable.\"\n",
    "\n",
    "    ignoreColumns = [\"table_key_ts\", \"table_delete\", \"table_key\"]\n",
    "\n",
    "    for field in my_schema.fields:\n",
    "        if field.name == \"after\":\n",
    "            for f3 in field.dataType:\n",
    "                te = f3.name + \" \" + f3.dataType.typeName()\n",
    "                if f3.name not in ignoreColumns:\n",
    "                    tableList.append(te)\n",
    "                    col.append(f3.name)\n",
    "        else:\n",
    "            te = field.name + \" \" + field.dataType.typeName()\n",
    "            if field.name not in ignoreColumns:\n",
    "                tableList.append(te)\n",
    "                col.append(field.name)\n",
    "    res = \"\"\n",
    "    insertResInp = \"\"\n",
    "    insertResOut = \"\"\n",
    "    merge = \"\"\n",
    "    for i in range(len(tableList)):\n",
    "        if i == (len(tableList) - 1):\n",
    "            res += tableList[i]\n",
    "            insertResInp += col[i]\n",
    "            insertResOut += inputTable + col[i]\n",
    "            merge += targetTable + col[i] + \" = \" + inputTable + col[i]\n",
    "        else:\n",
    "            res += tableList[i] + \",\"\n",
    "            insertResInp += col[i] + \",\"\n",
    "            insertResOut += inputTable + col[i] + \",\"\n",
    "            merge += targetTable + col[i] + \" = \" + inputTable + col[i] + \",\"\n",
    "    ans = [res, insertResInp, insertResOut, merge]\n",
    "    \n",
    "    \n",
    "    print(ans)\n",
    "    return ans\n",
    "\n",
    "\n",
    "class KafkaJob:\n",
    "\n",
    "    def __init__(self, record, global_variables):\n",
    "        self.test_data = global_variables.test_data\n",
    "        self.test_schema = global_variables.test_schema\n",
    "        self.sc = global_variables.sc\n",
    "        self.spark = global_variables.spark\n",
    "        self.schemaRegistryAddress = global_variables.schemaRegistryUrl\n",
    "        self.database = record[\"database\"]\n",
    "#         self.batchOutputPath = global_variables.batchOutputPath\n",
    "        \n",
    "        #change 1\n",
    "        self.batchOutputPath = record[\"batchOutputPath\"]\n",
    "        \n",
    "#         self.kafkaGrp = global_variables.kafka_grp\n",
    "        \n",
    "#         #change 2\n",
    "#         self.kafkaGrp = record[\"kafka_grp\"]\n",
    "        \n",
    "        \n",
    "#         self.OutputPathForMaterializedView = global_variables.OutputPathForMaterializedView\n",
    "        \n",
    "        # change 3\n",
    "        self.OutputPathForMaterializedView = record[\"OutputPathForMaterializedView\"]\n",
    "        \n",
    "        self.schemasByIdDict = {}\n",
    "        self.bootstrapServers = global_variables.bootstrapServers\n",
    "        \n",
    "        # change 4\n",
    "#         self.kafkaConsumerGroupForLag = record[\"kafkaConsumerGroupForLag\"]\n",
    "        \n",
    "        self.kafkaConsumerGroupForLag = global_variables.kafkaConsumerGroupForLag\n",
    "        \n",
    "        \n",
    "        self.startDateOfPipeline = global_variables.startDateOfPipeline\n",
    "        self.timestampToStartStreaming = global_variables.timestampToStartStreaming\n",
    "        self.table = record[\"table_name\"]\n",
    "        \n",
    "        self.secondary_key = record[\"secondary_key\"]\n",
    "        self.topic_prefix = record[\"topic_prefix\"]\n",
    "        self.partition_column = record[\"partition_column\"]\n",
    "        self.zorder_column = record[\"zorder_column\"]\n",
    "        self.partition_column_derive_from_column = record[\"partition_column_derive_from_column\"]\n",
    "        self.primary_key = record[\"primary_key\"]\n",
    "        self.reference_index = record[\"reference_index\"]\n",
    "        self.topic_pattern_to_subscribe = record[\"topic_pattern\"]\n",
    "        self.is_pattern = record[\"is_pattern\"]\n",
    "        self.batchOutputPathForTopic = \"{}{}/{}\".format(self.batchOutputPath, self.database, self.table)\n",
    "        self.checkpoint = self.batchOutputPathForTopic + \"_AvroCheckpoint\"\n",
    "        self.OutputPathForMaterializedViewPath = \"{}{}/{}\".format(self.OutputPathForMaterializedView, self.database, self.table)\n",
    "        self.table_exists = False\n",
    "        self.subriction_option = \"subscribePattern\"\n",
    "        self.current_primary_key = self.sc.broadcast(self.primary_key)\n",
    "        self.current_secondary_key = self.sc.broadcast(self.secondary_key)\n",
    "        \n",
    "\n",
    "        self.spark.sql(\"create database  IF NOT EXISTS  {} LOCATION  '{}'\".format(self.database, self.OutputPathForMaterializedView))\n",
    "        self.spark.sql(\"use {}\".format(self.database))\n",
    "        print(self.table)\n",
    "        self.new_partition_key = None\n",
    "        self.new_partition_value = None\n",
    "        self.byteArrayToLong = fn.udf(lambda x: int.from_bytes(x, byteorder='big', signed=False), LongType())\n",
    "        self.schemasByIdDict = {}\n",
    "#         val = sc._jvm.example13.Hello.add(self.bootstrapServers, self.kafkaConsumerGroupForLag)\n",
    "\n",
    "\n",
    "        scalaDfLag = self.sc._jvm.com.example.Hello.add(self.spark._jsparkSession, dbutils.widgets.get(\"bootstrapServers\"), dbutils.widgets.get(\"kafkaConsumerGroupForLag\"))\n",
    "\n",
    "        print(scalaDfLag)\n",
    "#         print(\"query\")\n",
    "#         print(val)\n",
    "        \n",
    "        if self.partition_column_derive_from_column:\n",
    "            partition_column_derive_from_column_split = self.partition_column_derive_from_column.split(\",\")\n",
    "            self.new_partition_key = self.sc.broadcast(partition_column_derive_from_column_split[0])\n",
    "            self.new_partition_value = self.sc.broadcast(partition_column_derive_from_column_split[1])\n",
    "\n",
    "        if not self.is_pattern:\n",
    "            self.subriction_option = \"subscribe\"\n",
    "            \n",
    "            \n",
    "        self.spark.sql(\"SET spark.databricks.delta.schema.autoMerge.enabled = true\")\n",
    "        \n",
    "        \n",
    "        # adding option of lowshuffle merge but will work with DBR9.0+ so uncomment below as needed\n",
    "#         self.spark.sql(\"SET spark.databricks.delta.merge.enableLowShuffle = true\")\n",
    "        \n",
    "        self.decode_process_data = fn.udf(KafkaJob.decode_process_data_udf, StringType())\n",
    "\n",
    "    @staticmethod\n",
    "    def decode_process_data_udf(coded_string):\n",
    "        result = \"\"\n",
    "        try:\n",
    "            temp = base64.b64decode(coded_string)\n",
    "            result = gzip.decompress(temp).decode('utf-8')\n",
    "        except:\n",
    "            result = coded_string\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def get_multiple_partition_query(partition_column: str, primary_key: str):\n",
    "\n",
    "        if not primary_key or not partition_column:\n",
    "            return \"updatesTable.id = targetTable.id\"\n",
    "        \n",
    "#         query = \"updatesTable.{0} = targetTable.{0}\".format(primary_key)\n",
    "        \n",
    "        query = \"\"\n",
    "        \n",
    "        primary_keyA = []\n",
    "    \n",
    "        pkey_array = primary_key.split(\",\")\n",
    "        for partitions in pkey_array:\n",
    "            partitions = partitions.strip()\n",
    "            if query == \"\":\n",
    "              query += \"updatesTable.{0} = targetTable.{0}\".format(partitions)\n",
    "            else:\n",
    "              query += \" and updatesTable.{0} = targetTable.{0}\".format(partitions)\n",
    "              \n",
    "            primary_keyA.append(partitions)\n",
    "        \n",
    "#         primary_key = primary_key.strip()\n",
    "        partitions_array = partition_column.split(\",\")\n",
    "        for partitions in partitions_array:\n",
    "            partitions = partitions.strip()\n",
    "            if partitions not in primary_keyA:\n",
    "                query += \" and updatesTable.{0} = targetTable.{0}\".format(partitions)\n",
    "        return query\n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_secondary_partition_query(partition_column: str, primary_key: str, secondary_key: str):\n",
    "        \n",
    "        query = \"updatesTable.{0} = targetTable.{0}\".format(primary_key)\n",
    "        \n",
    "        query += \" and updatesTable.{0} = targetTable.{0}\".format(secondary_key)\n",
    "        \n",
    "        primary_key = primary_key.strip()\n",
    "        partitions_array = partition_column.split(\",\")\n",
    "        for partitions in partitions_array:\n",
    "            partitions = partitions.strip()\n",
    "            if partitions != primary_key and partitions != secondary_key:\n",
    "                query += \" and updatesTable.{0} = targetTable.{0}\".format(partitions)\n",
    "        return query\n",
    "\n",
    "      \n",
    "      \n",
    "    @staticmethod\n",
    "    def get_partition_query(partition_column: str, primary_key: str):\n",
    "\n",
    "        if not primary_key or not partition_column:\n",
    "            return \"updatesTable.id = targetTable.id\"\n",
    "        query = \"updatesTable.{0} = targetTable.{0}\".format(primary_key)\n",
    "        primary_key = primary_key.strip()\n",
    "        partitions_array = partition_column.split(\",\")\n",
    "        for partitions in partitions_array:\n",
    "            partitions = partitions.strip()\n",
    "            if partitions != primary_key:\n",
    "                query += \" and updatesTable.{0} = targetTable.{0}\".format(partitions)\n",
    "        return query\n",
    "\n",
    "    def getJsonSchema(self, schemaId, schemasById, registrySchemaUrl):\n",
    "        jsonSchema = schemasById.get(schemaId)\n",
    "        if jsonSchema is None:\n",
    "            src = SchemaRegistryClient(registrySchemaUrl)\n",
    "            schema = src.get_by_id(schemaId)\n",
    "            jsonSchema = json.dumps(schema.flat_schema)\n",
    "            schemasById[schemaId] = jsonSchema\n",
    "        return jsonSchema\n",
    "\n",
    "    @staticmethod\n",
    "    def generator(partition):\n",
    "      \"\"\"\n",
    "      Function yielding some result created by some function applied to each row of a partition (in this case lower-casing a string)\n",
    "\n",
    "      @partition: iterator-object of partition\n",
    "      \"\"\"\n",
    "\n",
    "      partCount = 1\n",
    "      for row in partition:\n",
    "          yield [row[\"offset\"].lower(), row, partCount]\n",
    "          partCount+=1\n",
    "        \n",
    "    @staticmethod\n",
    "    def groupk(partition):\n",
    "      from itertools import groupby\n",
    "      \n",
    "      partCount = 1\n",
    "      l = []\n",
    "      for key, group in groupby(sorted(partition, key = lambda x: x[\"table_key\"]), lambda x: x[\"table_key\"]):\n",
    "        a = []\n",
    "        k = None\n",
    "        temp = -1\n",
    "\n",
    "        for thing in group:\n",
    "          if temp < int(thing[\"offset\"]):\n",
    "            temp = thing[\"offset\"]\n",
    "            k = thing\n",
    "\n",
    "#         yield k\n",
    "        l.append(k)\n",
    "        partCount += 1\n",
    "    \n",
    "      return l\n",
    "    \n",
    "    def upsertToDelta(self,microBatchOutputDF, batchId):\n",
    "        microBatchOutputDF = microBatchOutputDF.drop(\"valuenew\")\n",
    "        \n",
    "        microBatchOutputDF = microBatchOutputDF.withColumn('valueConsumerKafka', microBatchOutputDF['value'])\n",
    "        microBatchOutputDF = microBatchOutputDF.drop(\"value\")\n",
    "        \n",
    "        \n",
    "        microBatchOutputDF = microBatchOutputDF.withColumn(\"table_delete\", F.when(\n",
    "            col(\"valueConsumerKafka.before\").isNotNull and col(\"valueConsumerKafka.op\").isNotNull and col(\"valueConsumerKafka.op\").contains(\"d\"),\n",
    "            True).otherwise(False))\n",
    "        \n",
    "        \n",
    "        if self.secondary_key:\n",
    "          microBatchOutputDF = microBatchOutputDF.withColumn(\"table_key\", F.when(col(\"table_delete\")==True, \n",
    "                                                                                 F.concat(microBatchOutputDF[\n",
    "                                                                                     \"valueConsumerKafka.before.{}\".format(self.current_primary_key.value)],\n",
    "                                                                                          F.lit('_'), \n",
    "                                                                                          microBatchOutputDF[\"valueConsumerKafka.before.{}\".format(\n",
    "                                                                                         self.current_secondary_key.value)])).otherwise(F.concat(microBatchOutputDF[\n",
    "                                                                                     \"valueConsumerKafka.after.{}\".format(self.current_primary_key.value)],F.lit('_'), \n",
    "                                                                                         microBatchOutputDF[\"valueConsumerKafka.after.{}\".format(\n",
    "                                                                                         self.current_secondary_key.value)])))\n",
    "        else:\n",
    "          microBatchOutputDF = microBatchOutputDF.withColumn(\"table_key\", F.when(col(\"table_delete\") == True,\n",
    "                                                                                 microBatchOutputDF[\n",
    "                                                                                     \"valueConsumerKafka.before.{}\".format(\n",
    "                                                                                         self.current_primary_key.value)]).otherwise(\n",
    "              microBatchOutputDF[\"valueConsumerKafka.after.{}\".format(self.current_primary_key.value)]))\n",
    "\n",
    "        \n",
    "          \n",
    "        \n",
    "        print(\"starting point\")\n",
    "        #logger.debug(\"starting point\")\n",
    "\n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "        #logger.debug(str(datetime.datetime.now()))\n",
    "        \n",
    "#         microBatchOutputDF.show()\n",
    "        # display(microBatchOutputDF)\n",
    "        print(\"total partitions are\")\n",
    "        print(microBatchOutputDF.rdd.getNumPartitions())\n",
    "      \n",
    "      \n",
    "#         dfByTopic = microBatchOutputDF.withColumn(\"rn\", F.row_number().over(\n",
    "#             Window.partitionBy(\"table_key\").orderBy(F.col(\"offset\").desc())))\n",
    "#         dfByTopic = dfByTopic.filter(F.col(\"rn\") == 1).drop(\"rn\")\n",
    "      \n",
    "      \n",
    "#         dfByTopic2 = microBatchOutputDF.withColumn(\"max_offset\", F.max(\"offset\").over(\n",
    "#             Window.partitionBy(\"table_key\")))\n",
    "        \n",
    "#         dfByTopic = dfByTopic2.where(F.col(\"offset\") == F.col(\"max_offset\")).drop(\"max_offset\")\n",
    "    \n",
    "        \n",
    "#         # option 1 [still it requires group by but broadcasts it to multiple so maybe less shuffles]\n",
    "#         dfMax = microBatchOutputDF.groupBy(col(\"table_key\").alias(\"max_table_key_con\")).agg(sparkMax(col(\"offset\")).alias(\"max_offset_value_con\"))\n",
    "\n",
    "#         dfTopByJoin = microBatchOutputDF.join(broadcast(dfMax),\n",
    "#         (col(\"table_key\") == col(\"max_table_key_con\")) & (col(\"offset\") == col(\"max_offset_value_con\"))).drop(\"max_table_key_con\").drop(\"max_offset_value_con\")\n",
    "        \n",
    "# #         for same offset can it have same key value??\n",
    "#         dfByTopic = dfTopByJoin.dropDuplicates([\"table_key\",\"offset\"])\n",
    "  \n",
    "# #         dfByTopic = spark.createDataFrame(dfByTopic2.rdd, schema = microBatchOutputDF.schema)\n",
    "\n",
    "# #         # option 1\n",
    "        \n",
    "        \n",
    "#         #option 2 [reducebyKey is also wide operation but it have less shuffles as can combine first and then shuffles will be less]\n",
    "\n",
    "#         dfrdd = microBatchOutputDF.rdd.map(lambda x: (x.table_key, x)).reduceByKey(lambda x, y: x if (x.offset > y.offset) else y)\n",
    "\n",
    "#         a = microBatchOutputDF.schema\n",
    "  \n",
    "#         schema_str = StructType([\n",
    "#           StructField('table_key', IntegerType(), True),\n",
    "#           StructField('others', a)])\n",
    "    \n",
    "#         dfByT = spark.createDataFrame(dfrdd, schema = schema_str)\n",
    "      \n",
    "#         dfByTopic = dfByT.select(\"others.*\")\n",
    "  \n",
    "#         # option 2\n",
    "\n",
    "\n",
    "        dfrdd = microBatchOutputDF.rdd.mapPartitions(KafkaJob.groupk)\n",
    "        dfByTopic = spark.createDataFrame(dfrdd, schema = microBatchOutputDF.schema)\n",
    "    \n",
    "#         dfByTopic = microBatchOutputDF\n",
    "\n",
    "#         dfByTopic = microBatchOutputDF\n",
    "        print(dfByTopic.rdd.getNumPartitions())\n",
    "        #logger.debug(\"ending point\")\n",
    "        \n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "        #logger.debug(str(datetime.datetime.now()))\n",
    "        dfByTopic.show()\n",
    "        display(dfByTopic)\n",
    "\n",
    "        dfByTopicDeleted = dfByTopic.filter(F.col(\"table_delete\") == True)\n",
    "        dfByTopicDeleted = dfByTopicDeleted.select(\"valueConsumerKafka.before.*\", \"table_delete\", \"table_key\", \"offset\")\n",
    "\n",
    "        dfByTopicDeleted = dfByTopicDeleted.drop(\"valueConsumerKafka\", \"kafka-key\", \"schemaId\")\n",
    "        dfByTopicDeleted = dfByTopicDeleted.drop(\"offset\")\n",
    "\n",
    "        dfByTopicInsert = dfByTopic.filter(F.col(\"table_delete\") == False)\n",
    "        dfByTopicInsert = dfByTopicInsert.select(\"valueConsumerKafka.after.*\", \"table_delete\", \"table_key\", \"offset\")\n",
    "        # display(df)\n",
    "        dfByTopicInsert = dfByTopicInsert.drop(\"valueConsumerKafka\", \"kafka-key\", \"schemaId\")\n",
    "        dfByTopicInsert = dfByTopicInsert.drop(\"offset\")\n",
    "        if self.partition_column_derive_from_column:\n",
    "            dfByTopicDeleted = dfByTopicDeleted.withColumn(self.new_partition_key.value,\n",
    "                                                           to_date(col(self.new_partition_value.value)))\n",
    "            dfByTopicInsert = dfByTopicInsert.withColumn(self.new_partition_key.value,\n",
    "                                                         to_date(col(self.new_partition_value.value)))\n",
    "        if self.table == 'process':\n",
    "            dfByTopicInsert = dfByTopicInsert.withColumn(\"process_data\", self.decode_process_data(\"process_data\"))\n",
    "\n",
    "        if not self.table_exists:\n",
    "            my_schema = dfByTopicInsert.schema\n",
    "            res = getTableColumn(my_schema)\n",
    "            dfByTopic._jdf.sparkSession().sql(\"use {}\".format(self.database))\n",
    "            if self.partition_column:\n",
    "                dfByTopic._jdf.sparkSession().sql(\n",
    "                    \"create table if not exists {} ( {} ) using delta PARTITIONED BY ({}) location '{}' \".format(self.table,\n",
    "                                                                                                                 res[0],\n",
    "                                                                                                                 self.partition_column,\n",
    "                                                                                                                 self.OutputPathForMaterializedViewPath))\n",
    "            else:\n",
    "                dfByTopic._jdf.sparkSession().sql(\n",
    "                    \"create table if not exists {} ( {} ) using delta location '{}' \".format(self.table, res[0],\n",
    "                                                                                             self.OutputPathForMaterializedViewPath))\n",
    "\n",
    "            self.table_exists = True\n",
    "\n",
    "        dfByTopicInsert = dfByTopicInsert.drop(\"table_delete\")\n",
    "        dfByTopicInsert = dfByTopicInsert.drop(\"table_key\")\n",
    "\n",
    "#         PYSPARK MERGE\n",
    "#         deltaTable=DeltaTable.forName(spark,self.table)\n",
    "        FullTableName = self.database + \".\" + self.table\n",
    "\n",
    "        print(FullTableName)\n",
    "\n",
    "#         deltaTable = DeltaTable.forName(spark, FullTableName)\n",
    "        \n",
    "        deltaTable=DeltaTable.forName(spark,FullTableName)\n",
    "    \n",
    "#         jab\n",
    "        \n",
    "        if self.secondary_key:\n",
    "            if not self.partition_column:\n",
    "              query = KafkaJob.get_secondary_partition_query(\"\", self.primary_key, self.secondary_key)\n",
    "            else:\n",
    "              query = KafkaJob.get_secondary_partition_query(self.partition_column, self.primary_key, self.secondary_key)\n",
    "\n",
    "            deltaTable.alias(\"targetTable\").merge(\n",
    "                dfByTopicDeleted.alias(\"updatesTable\"),\n",
    "                \"{0}\".format(query)) \\\n",
    "              .whenMatchedDelete() \\\n",
    "              .execute()\n",
    "\n",
    "\n",
    "            deltaTable.alias(\"targetTable\").merge(\n",
    "                dfByTopicInsert.alias(\"updatesTable\"),\n",
    "                \"{0}\".format(query)) \\\n",
    "              .whenMatchedUpdateAll() \\\n",
    "              .whenNotMatchedInsertAll().execute()\n",
    "\n",
    "        elif self.partition_column and self.partition_column != self.primary_key:\n",
    "            query = KafkaJob.get_partition_query(self.partition_column, self.primary_key)\n",
    "\n",
    "            deltaTable.alias(\"targetTable\").merge(\n",
    "                dfByTopicDeleted.alias(\"updatesTable\"),\n",
    "                \"{0}\".format(query)) \\\n",
    "              .whenMatchedDelete() \\\n",
    "              .execute()\n",
    "\n",
    "\n",
    "            deltaTable.alias(\"targetTable\").merge(\n",
    "                dfByTopicInsert.alias(\"updatesTable\"),\n",
    "                \"{0}\".format(query)) \\\n",
    "              .whenMatchedUpdateAll() \\\n",
    "              .whenNotMatchedInsertAll().execute()\n",
    "\n",
    "        else:\n",
    "            \n",
    "            deltaTable.alias(\"targetTable\").merge(\n",
    "                dfByTopicDeleted.alias(\"updatesD\"),\n",
    "                \"updatesD.table_key = targetTable.{0}\".format(self.primary_key)) \\\n",
    "              .whenMatchedDelete() \\\n",
    "              .execute()\n",
    "\n",
    "            deltaTable.alias(\"targetTable\").merge(\n",
    "                dfByTopicInsert.alias(\"updatesI\"),\n",
    "                \"updatesI.{0} = targetTable.{0}\".format(self.primary_key)) \\\n",
    "              .whenMatchedUpdateAll() \\\n",
    "              .whenNotMatchedInsertAll().execute()\n",
    "\n",
    "#         PYSPARK MERGE\n",
    "\n",
    "        print(\"after merge\")\n",
    "\n",
    "        #logger.debug(\"after merge\")\n",
    "        \n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "        #logger.debug(str(datetime.datetime.now()))\n",
    "\n",
    "    def get_avro_data_by_schema_for_each(self,df, batchId):\n",
    "      \n",
    "        self.reference_index = batchId % 50\n",
    "        print(\"optimize check\")\n",
    "        \n",
    "        ldf = spark.sql(\"select (int(current_timestamp()) - int(last_updated))/60 from pipeline_common.vacuumStatus where table = '\" + self.database + \".\" + self.table + \"'\")\n",
    "        \n",
    "        if ldf.first()[0] > 5:\n",
    "          if batchId % 50 == int(self.reference_index) and batchId >= 50:\n",
    "              try:\n",
    "                  print(\"optimizing {}.{}\".format(self.database,self.table))\n",
    "                  if self.is_pattern:\n",
    "                    print(\"using partition last 3 month\")\n",
    "\n",
    "                    timeCol = self.partition_column_derive_from_column.split(\",\")[0] \n",
    "\n",
    "                    if self.zorder_column and batchId % 150 == int(self.reference_index):\n",
    "                        self.spark.sql(\"optimize {0}.{1} WHERE {3} >= current_timestamp() - INTERVAL 100 day ZORDER BY ({2})\".format(self.database, self.table, self.zorder_column, timeCol))\n",
    "                    else:\n",
    "                        self.spark.sql(\"optimize {0}.{1} WHERE {2} >= current_timestamp() - INTERVAL 100 day \".format(self.database, self.table, timeCol))\n",
    "                  else:\n",
    "                    if self.zorder_column and batchId % 150 == int(self.reference_index):\n",
    "                        self.spark.sql(\"optimize {0}.{1} ZORDER BY ({2})\".format(self.database, self.table, self.zorder_column))\n",
    "                    else:\n",
    "                        self.spark.sql(\"optimize {0}.{1}\".format(self.database, self.table))\n",
    "\n",
    "                  print(\"vacuum {}.{}\".format(self.database,self.table))\n",
    "                  self.spark.sql(\"vacuum {0}.{1}\".format(self.database, self.table))\n",
    "                  spark.sql(\"update pipeline_common.vacuumStatus set last_updated = current_timestamp() where table = '\" + self.database + \".\" + self.table + \"'\")\n",
    "        \n",
    "              except Exception as e:\n",
    "                  print(e)\n",
    "        month_year = \"\"\n",
    "        avro_topic = self.topic_pattern_to_subscribe + \"-value\"\n",
    "        if self.is_pattern:\n",
    "            month_year = \"_\" + str(_datetime.date.today().month) + \"_\" + str(_datetime.date.today().year)\n",
    "            avro_topic = self.topic_prefix + self.table + month_year + \"-value\"\n",
    "        print(\"avro_topic: \" + avro_topic)\n",
    "        avro_topic_br = self.sc.broadcast(avro_topic)\n",
    "        try:\n",
    "            print(\"running fast: {}\", avro_topic)\n",
    "            df = df.withColumn('valuenew', df['value'])\n",
    "            df = df.withColumn('value', from_avro(\"value\", avro_topic_br.value, self.schemaRegistryAddress))\n",
    "            \n",
    "            self.upsertToDelta(df, batchId)\n",
    "        except Exception as fastException:\n",
    "#           print(\"checking aa\")  \n",
    "            try:\n",
    "                mem_limit = 2 ** 20\n",
    "                print(\"running slow: {}\", avro_topic)\n",
    "                print(\"fastException try slow\", fastException)\n",
    "                if sys.getsizeof(self.schemasByIdDict) > mem_limit:\n",
    "                    self.schemasByIdDict.clear()\n",
    "                df = df \\\n",
    "                    .withColumn(\"schemaId\", self.byteArrayToLong(fn.substring(\"valuenew\", 2, 4))) \\\n",
    "                    .withColumn(\"payload\", fn.expr(\"substring(valuenew, 6, length(valuenew)-5)\"))\n",
    "                dfAllSchemas = df.select(\"schemaId\").distinct()\n",
    "                schemaRowList = dfAllSchemas.collect()\n",
    "                print(schemaRowList)\n",
    "                for schemaRow in schemaRowList:\n",
    "                    jsonSchema = self.getJsonSchema(schemaRow.schemaId, self.schemasByIdDict,\n",
    "                                                    'http://schema-registry:8081')\n",
    "                    currentValueSchemaId = self.sc.broadcast(schemaRow.schemaId)\n",
    "                    currentValueSchema = self.sc.broadcast(jsonSchema)\n",
    "                    dfBySchemaId = df.where(df.schemaId == currentValueSchemaId.value)\n",
    "                    dfBySchemaId = dfBySchemaId.drop(\"value\")\n",
    "                    dfBySchemaId = dfBySchemaId.withColumn(\"value\", from_avro(\"payload\", currentValueSchema.value))\n",
    "                    dfBySchemaId = dfBySchemaId.drop(\"payload\")\n",
    "                    self.upsertToDelta(dfBySchemaId, batchId)\n",
    "            except Exception as slowException:\n",
    "                print(\"slowException {}\".format(avro_topic), fastException)\n",
    "                raise slowException\n",
    "\n",
    "    def start_read_stream(self):\n",
    "      \n",
    "        print(\"starting stream\")\n",
    "        #logger.debug(\"starting stream\")\n",
    "\n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "        #logger.debug(str(datetime.datetime.now()))\n",
    "        \n",
    "        print(\"topic_pattern_to_subscribe: \" + self.topic_pattern_to_subscribe)\n",
    "        \n",
    "        \n",
    "#         GrpId = \"fareye_\" + self.kafkaGrp + \"_\" + self.topic_pattern_to_subscribe\n",
    "        kafka = self.spark.readStream \\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", self.bootstrapServers) \\\n",
    "            .option(self.subriction_option, self.topic_pattern_to_subscribe) \\\n",
    "            .option(\"startingOffsets\", \"earliest\") \\\n",
    "            .option(\"maxOffsetsPerTrigger\", 1000000) \\\n",
    "            .option(\"failOnDataLoss\", False) \\\n",
    "            .load()\n",
    "\n",
    "        if self.timestampToStartStreaming is not None:\n",
    "            kafka = kafka.filter(kafka['timestamp'] >= self.timestampToStartStreaming)\n",
    "\n",
    "        kafka = kafka.filter(\"value is NOT NULL\")\n",
    "        return kafka\n",
    "\n",
    "    def start_write_stream(self,kafka,method_to_run_on_for_each):\n",
    "      \n",
    "        print(\"starting write\")\n",
    "        #logger.debug(\"starting write\")\n",
    "\n",
    "        print(datetime.datetime.now())\n",
    "        \n",
    "        #logger.debug(str(datetime.datetime.now()))\n",
    "        \n",
    "        ssc = kafka \\\n",
    "            .writeStream \\\n",
    "            .queryName(\"{0}_{1}\".format(self.database,self.table)) \\\n",
    "            .option(\"checkpointLocation\", self.checkpoint) \\\n",
    "            .trigger(processingTime='60 seconds') \\\n",
    "            .foreachBatch(method_to_run_on_for_each).start()\n",
    "        ssc.awaitTermination()\n",
    "\n",
    "\n",
    "    def start_pipeline_with_avro(self):\n",
    "        kafka = self.start_read_stream()\n",
    "        self.start_write_stream(kafka,self.get_avro_data_by_schema_for_each)\n",
    "\n",
    "    def start_pipeline_without_avro(self):\n",
    "        kafka = self.start_read_stream()\n",
    "        self.start_write_stream(kafka,self.upsertToDelta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6a3bde3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kafka = kafka.filter(\"value is NOT NULL\").withColumn(\"value\",from_avro('value', \"qatest12nonfes.public.user_summary_old-value\", \"http://172.31.32.174:8081\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb9cf573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a0855105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "data = spark.range(1,5)\n",
    "data.write.format(\"delta\").mode(\"overwrite\").save(\"delta_sample1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cd80281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfa.show(1)\n",
    "# # dfa.write.format(\"delta\").mode(\"overwrite\").save(\"user_summary_old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19ab218e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE \" + \"user_summary_old\" + \" USING DELTA LOCATION '\" + \"user_summary_old\" + \"'\")\n",
    "\n",
    "# fByTopic._jdf.sparkSession().sql(\n",
    "#                     \"create table if not exists {} ( {} ) using delta PARTITIONED BY ({}) location '{}' \".format(\"user_summary_old\",\n",
    "#                                                                                                                  res[0],\n",
    "#                                                                                                                  \"company_id,created_at_part\",\n",
    "#                                                                                                                  \"user_summary_old\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8650009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable=DeltaTable.forName(spark,\"delta_sample1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40cc9815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(deltaTable)\n",
    "df1 = spark.sql(\"select * from user_summary_old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a012f591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id: bigint]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c0f962b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+-------+----------+-------------------+-----------+-------+------------+-------------+---------------+---------+---------+-----------------------+-----------------------+----------------------------+----------------------------+-------------------------------+-------------------------------+----------------------------+----------------------------+-------------------------------+-------------------------------+-----------------------+-----------------------+--------------------------+--------------------------+----------------------+--------+--------+----------+------------+-------------+-------------+----------+--------------+------------+----------------------+--------------------+-------------------+-----------------+---------------+----------------+---------+----------+---------------------+----------------+-----------------------+------------------+-----------+---------------+---------+---------------+----+---------------+\n",
      "|    id|user_id|hub_id|city_id|company_id|               date|app_version|gps_kms|odometer_kms|halt_duration|travel_duration|avg_speed|max_speed|official_sms_sent_count|personal_sms_sent_count|official_call_incoming_count|official_call_outgoing_count|official_call_incoming_duration|official_call_outgoing_duration|personal_call_incoming_count|personal_call_outgoing_count|personal_call_incoming_duration|personal_call_outgoing_duration|cug_call_incoming_count|cug_call_outgoing_count|cug_call_incoming_duration|cug_call_outgoing_duration|last_location_datetime|last_lat|last_lng|last_speed|last_battery|pending_count|deliver_count|fail_count|cash_collected|cash_payment|cash_collected_by_card|          created_at|last_cash_collected|last_order_number|last_order_time|last_logout_time|first_lat|first_long|active_time_in_millis|     imei_number|next_job_transaction_id|active_mdm_version|trip_status|box_temperature|sensor_id|last_login_time|make|created_at_part|\n",
      "+------+-------+------+-------+----------+-------------------+-----------+-------+------------+-------------+---------------+---------+---------+-----------------------+-----------------------+----------------------------+----------------------------+-------------------------------+-------------------------------+----------------------------+----------------------------+-------------------------------+-------------------------------+-----------------------+-----------------------+--------------------------+--------------------------+----------------------+--------+--------+----------+------------+-------------+-------------+----------+--------------+------------+----------------------+--------------------+-------------------+-----------------+---------------+----------------+---------+----------+---------------------+----------------+-----------------------+------------------+-----------+---------------+---------+---------------+----+---------------+\n",
      "|361358|  51170| 56301|   6523|      1829|2022-02-26 18:30:00|     31.1.0|    0.0|         0.0|          0.0|            0.0|      0.0|      0.0|                      0|                      3|                           0|                           0|                              0|                              0|                           0|                           0|                              0|                              0|                      0|                      0|                         0|                         0|   2022-03-07 07:47:30|     0.0|     0.0|      null|        null|            0|            0|         0|           0.0|         0.0|                   0.0|2022-02-27 09:29:...|               null|             null|           null|            null|     null|      null|            685096686|affff946439a908f|                   null|              null|       null|           null|     null|           null|null|     2022-02-27|\n",
      "+------+-------+------+-------+----------+-------------------+-----------+-------+------------+-------------+---------------+---------+---------+-----------------------+-----------------------+----------------------------+----------------------------+-------------------------------+-------------------------------+----------------------------+----------------------------+-------------------------------+-------------------------------+-----------------------+-----------------------+--------------------------+--------------------------+----------------------+--------+--------+----------+------------+-------------+-------------+----------+--------------+------------+----------------------+--------------------+-------------------+-----------------+---------------+----------------+---------+----------+---------------------+----------------+-----------------------+------------------+-----------+---------------+---------+---------------+----+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d57e1056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "datau = spark.range(3,8)\n",
    "\n",
    "deltaTable.alias(\"targetTable\").merge(\n",
    "                datau.alias(\"updatesTable\"),\n",
    "                \"{0}\".format(\"updatesTable.id = targetTable.id\")) \\\n",
    "              .whenMatchedUpdateAll() \\\n",
    "              .whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "faefa73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  6|\n",
      "|  5|\n",
      "|  7|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.sql(\"select * from delta_sample1\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2aad81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
